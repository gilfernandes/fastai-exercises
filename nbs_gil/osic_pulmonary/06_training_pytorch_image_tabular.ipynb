{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"osic-pulmonary-pytorch-resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/kaggle/osic_pulmonary')\n",
    "assert path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, submission_df = common.read_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = common.prepare_submission(submission_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[((submission_df['Patient'] == 'ID00419637202311204720264') & (submission_df['Weeks'] == 6))].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_percent_in_submission():\n",
    "    previous_match = None\n",
    "    for i, r in submission_df.iterrows():\n",
    "        in_training = train_df[(train_df['Patient'] == r['Patient']) & (train_df['Weeks'] == r['Weeks'])]\n",
    "        if(len(in_training) > 0):\n",
    "            previous_match = in_training['Percent'].item()\n",
    "            submission_df.iloc[i, submission_df.columns.get_loc('Percent')] = previous_match\n",
    "        elif previous_match is not None:\n",
    "            submission_df.iloc[i, submission_df.columns.get_loc('Percent')] = previous_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_percent_in_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[test_df['Patient'] == 'ID00419637202311204720264']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df['Patient'] == 'ID00419637202311204720264']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[submission_df['Patient'] == 'ID00419637202311204720264'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['WHERE'] = 'train'\n",
    "test_df['WHERE'] = 'val'\n",
    "submission_df['WHERE'] = 'test'\n",
    "data = train_df.append([test_df, submission_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['min_week'] = data['Weeks']\n",
    "data.loc[data.WHERE=='test','min_week'] = np.nan\n",
    "data['min_week'] = data.groupby('Patient')['min_week'].transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = data.loc[data.Weeks == data.min_week]\n",
    "base = base[['Patient','FVC']].copy()\n",
    "base.columns = ['Patient','min_FVC']\n",
    "base['nb'] = 1\n",
    "base['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\n",
    "base = base[base.nb==1]\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(base, on='Patient', how='left')\n",
    "data['base_week'] = data['Weeks'] - data['min_week']\n",
    "data['base_week'] = data['base_week']\n",
    "del base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Patient'] == 'ID00421637202311550012437']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = ['Sex','SmokingStatus'] #,'Age'\n",
    "FE = []\n",
    "for col in COLS:\n",
    "    for mod in data[col].unique():\n",
    "        FE.append(mod)\n",
    "        data[mod] = (data[col] == mod).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Patient'] == 'ID00421637202311550012437']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df:pd.DataFrame, cont_names, target_names):\n",
    "    \"Compute the means and stds of `self.cont_names` columns to normalize them.\"\n",
    "    means, stds = {},{}\n",
    "    for n, t in zip(cont_names, target_names):\n",
    "        means[n], stds[n] = df[n].mean(), df[n].std()\n",
    "        df[t] = (df[n]-means[n]) / (1e-7 + stds[n])\n",
    "\n",
    "normalize(data, ['Age','min_FVC','base_week','Percent'], ['age','BASE','week','percent'])\n",
    "FE += ['age','percent','week','BASE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data.loc[data.WHERE=='train']\n",
    "test_df = data.loc[data.WHERE=='val']\n",
    "submission_df = data.loc[data.WHERE=='test']\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sort_values(['Patient', 'Weeks']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df[FE]\n",
    "X.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['FVC']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=2020):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_IMAGES = 10\n",
    "wandb.config.num_images = NUMBER_IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "\n",
    "def read_png(patient, ds_type='train', limit=1):\n",
    "    current_folder = path/f\"{ds_type}/{patient}\"\n",
    "    os.chdir(current_folder)\n",
    "    img_files = glob.glob(\"*.png\")\n",
    "    pil_images = []\n",
    "    for i in range(limit):\n",
    "        if len(img_files) > i:\n",
    "            img_file = img_files[i]\n",
    "        else:\n",
    "            raise Exception(f\"Not enough images in folder {current_folder}. Only {len(img_files)} found and at least {i + 1} expected.\")\n",
    "        full_path = path/f\"{ds_type}/{patient}/{img_file}\"\n",
    "        image_tensor = transforms.functional.to_tensor(Image.open(full_path))\n",
    "        image_tensor = image_tensor[0].unsqueeze(0)\n",
    "        pil_images.append(image_tensor)\n",
    "    return torch.cat(pil_images)\n",
    "\n",
    "read_png('ID00015637202177877247924', limit = 1).shape, read_png('ID00015637202177877247924', limit = NUMBER_IMAGES).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /kaggle/osic_pulmonary/train/ID00248637202266698862378/*.png | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([read_png(patient) for patient in ['ID00419637202311204720264', 'ID00421637202311550012437']]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x, y, patients, image_path='train'):\n",
    "        self.x, self.y = torch.tensor(x.values, dtype=torch.float32), torch.tensor(y.values, dtype=torch.float32)\n",
    "        self.patients = patients.values\n",
    "        self.image_path = image_path\n",
    "        assert(len(self.x) == len(self.y))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        patient = self.patients[i]\n",
    "        return self.x[i], self.y[i], read_png(patient, ds_type=self.image_path, limit = NUMBER_IMAGES)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'x: {self.x.shape} y: {self.y.shape}, patients: {len(self.patients)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(X, y, patients, image_path='train', batch_size=64, num_workers=10, shuffle=True):\n",
    "    ds = TabularImageDataset(X, y, patients, image_path)\n",
    "    return DataLoader(ds, batch_size, shuffle=shuffle, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dl = create_dl(X, y, train_df['Patient'], image_path='train')\n",
    "x_sample, y_sample, image_sample = next(iter(sample_dl))\n",
    "x_sample.shape, y_sample.shape, image_sample.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def move_to_dev(x, y, img=None):\n",
    "    x = x.to(device)\n",
    "    y  = y.to(device)\n",
    "    if img is not None:\n",
    "        img = img.to(device)\n",
    "    else:\n",
    "        img = None\n",
    "    return x, y, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1, C2 = torch.tensor(70, dtype=torch.float32), torch.tensor(1000, dtype=torch.float32)\n",
    "C1, C2, _ = move_to_dev(C1, C2)\n",
    "q = torch.tensor([0.2, 0.50, 0.8]).float().to(device)\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    sigma = y_pred[:, 2] - y_pred[:, 0]\n",
    "    fvc_pred = y_pred[:, 1]\n",
    "    \n",
    "    #sigma_clip = sigma + C1\n",
    "    sigma_clip = torch.max(sigma, C1)\n",
    "    delta = torch.abs(y_true[:, 0] - fvc_pred)\n",
    "    delta = torch.min(delta, C2)\n",
    "    sq2 = torch.sqrt(torch.tensor(2.))\n",
    "    metric = (delta / sigma_clip)*sq2 + torch.log(sigma_clip* sq2)\n",
    "    return torch.mean(metric)\n",
    "\n",
    "def qloss(y_true, y_pred):\n",
    "    # Pinball loss for multiple quantiles\n",
    "    e = y_true - y_pred\n",
    "    v = torch.max(q*e, (q-1)*e)\n",
    "    return torch.mean(v)\n",
    "\n",
    "def mloss(_lambda):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = y_true.unsqueeze(1)\n",
    "        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(ni, nf, ks=3, stride=2, add_relu=True, padding=None):\n",
    "    if padding is None:\n",
    "        padding = ks//2\n",
    "    if add_relu:\n",
    "        return nn.Sequential(nn.Conv2d(ni, nf, ks, padding=padding, stride=stride), nn.ReLU(), nn.BatchNorm2d(nf))\n",
    "    return nn.Sequential(nn.Conv2d(ni, nf, ks, padding=padding, stride=stride), nn.BatchNorm2d(nf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.expansion = 4\n",
    "        self.conv1 = conv2d(in_channels, out_channels, ks=1, stride=1)\n",
    "        self.conv2 = conv2d(out_channels, out_channels, ks=3, stride=stride)\n",
    "        self.conv3 = conv2d(out_channels, out_channels * self.expansion, ks=1, stride=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "            \n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OsicModel(torch.nn.Module):\n",
    "    def __init__(self, ni, nh1, nh2, num_resnet_classes=[3, 4, 6, 3]):\n",
    "        super(OsicModel, self).__init__()\n",
    "        # Tab processing\n",
    "        self.l1 = nn.Linear(ni, nh1)\n",
    "        self.l1_bn = nn.BatchNorm1d(nh1, momentum=0.1)\n",
    "        self.l2 = nn.Linear(nh1, nh2)\n",
    "        \n",
    "        # Image processing\n",
    "        self.conv1 = conv2d(image_sample.shape[1], 128, ks=7, stride=2)\n",
    "        self.maxpool1 = nn.MaxPool2d((3, 3), stride=2)\n",
    "        # resnet layers\n",
    "        self.in_channels = 64\n",
    "        self.resnet1 = self.make_layer(num_resnet_classes[0], 128, stride=1)\n",
    "        self.resnet2 = self.make_layer(num_resnet_classes[1], 128, stride=2)\n",
    "        self.resnet3 = self.make_layer(num_resnet_classes[2], 256, stride=2)\n",
    "        self.resnet4 = self.make_layer(num_resnet_classes[3], 512, stride=2)\n",
    "        self.pool2d = nn.AdaptiveAvgPool2d(1)\n",
    "        self.nh3 = 50\n",
    "        self.ln1 = nn.Linear(2048 * 1 * 1, self.nh3)\n",
    "        self.batchnorm = nn.BatchNorm1d(self.nh3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout2d(0.2)\n",
    "        \n",
    "        # Final layer\n",
    "        self.p1 = nn.Linear(nh2 + self.nh3, 3)\n",
    "        self.p2 = nn.Linear(nh2 + self.nh3, 3)\n",
    "        \n",
    "        \n",
    "    def make_layer(self, num_residual_blocks, out_channels, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "        \n",
    "        if stride != 1 or self.in_channels != out_channels * 4:\n",
    "            identity_downsample = conv2d(self.in_channels, out_channels*4, ks=1, stride=stride, padding=0)\n",
    "            \n",
    "        layers.append(ResnetBlock(self.in_channels, out_channels, identity_downsample, stride))\n",
    "        self.in_channels = out_channels * 4\n",
    "        \n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(ResnetBlock(self.in_channels, out_channels))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x, img):\n",
    "        \n",
    "        x = self.relu(self.l1(x))\n",
    "        x = self.l1_bn(x)\n",
    "        x = self.relu(self.l2(x))\n",
    "        \n",
    "        img = self.conv1(img)\n",
    "        img = self.maxpool1(img)\n",
    "        img = self.resnet1(img)\n",
    "        img = self.resnet2(img)\n",
    "        img = self.resnet3(img)\n",
    "        img = self.resnet4(img)\n",
    "        img = self.pool2d(img)\n",
    "        img = img.reshape(img.shape[0], -1)\n",
    "        img = self.ln1(img)\n",
    "        img = self.batchnorm(img)\n",
    "        img = self.relu(img)\n",
    "        img = self.dropout(img)\n",
    "        \n",
    "        x = torch.cat((img, x), dim=1)\n",
    "        \n",
    "        p1 = self.p1(x)\n",
    "        p2 = self.relu(self.p2(x))\n",
    "        preds = p1 + torch.cumsum(p2, axis=1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(nh1=100, nh2=100):\n",
    "    model = OsicModel(X.shape[1], nh1, nh2, num_resnet_classes=[3, 4, 6, 3])\n",
    "    model = model.to(device)\n",
    "    wandb.config.hidden_layer_1 = nh1\n",
    "    wandb.config.hidden_layer_2 = nh2\n",
    "    wandb.config.hidden_layer_3 = model.nh3\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=mloss(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test model\n",
    "y_sample, x_sample, image_sample = move_to_dev(y_sample, x_sample, image_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = sample_model.conv1(image_sample)\n",
    "# img = sample_model.maxpool1(img)\n",
    "# img = sample_model.resnet1(img)\n",
    "# img = sample_model.resnet2(img)\n",
    "# img = sample_model.resnet3(img)\n",
    "# img = sample_model.resnet4(img)\n",
    "# img = sample_model.pool2d(img)\n",
    "# img = img.reshape(img.shape[0], -1)\n",
    "# img = sample_model.ln1(img)\n",
    "# img = sample_model.relu(img)\n",
    "# img = sample_model.batchnorm(img)\n",
    "# img = sample_model.dropout(img)\n",
    "# img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = sample_model(x_sample, image_sample)\n",
    "# criterion(y_sample, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=50\n",
    "LR = 4e-3\n",
    "wandb.config.epochs = EPOCHS\n",
    "wandb.config.lr = LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(valid_dl, model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        total_eval_score = 0\n",
    "        for x, y, image_tensor in valid_dl:\n",
    "            x, y, image_tensor = move_to_dev(x, y, image_tensor)\n",
    "            output = model(x, image_tensor)\n",
    "            loss = criterion(y, output)\n",
    "            total_eval_loss += loss.item()\n",
    "            total_eval_score += score(y.unsqueeze(1), output)\n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(valid_dl)\n",
    "        avg_val_score = total_eval_score / len(valid_dl) * -1\n",
    "        return {\n",
    "            'avg_val_loss': avg_val_loss,\n",
    "            'avg_val_score': avg_val_score\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_log(epoch, avg_train_loss, avg_val_loss, avg_val_score):\n",
    "    wandb.log({'epoch': epoch, 'avg_train_loss': avg_train_loss, 'avg_val_loss': avg_val_loss, 'avg_val_score': avg_val_score})\n",
    "\n",
    "\n",
    "def train_loop(epochs, train_dl, valid_dl, model, lr = 1e-3, print_score=False, model_name='test', use_wandb=False):\n",
    "    steps = len(train_dl) * epochs\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_dl), epochs=epochs)\n",
    "    avg_train_losses = []\n",
    "    avg_val_losses = []\n",
    "    avg_val_scores = []\n",
    "    scale_scores = []\n",
    "    lr = []\n",
    "    best_avg_val_score = -1000\n",
    "    scaler = torch.cuda.amp.GradScaler() # mixed precision support\n",
    "    for epoch in tqdm(range(epochs), total=epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        for i, (x, y, image_tensor) in enumerate(train_dl):\n",
    "            x, y, image_tensor = move_to_dev(x, y, image_tensor)\n",
    "            model.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(x, image_tensor)\n",
    "                loss = criterion(y, output)\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Backward Pass and Optimization\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            lr.append(get_lr(optimizer))\n",
    "            scale_scores.append(scaler.get_scale())\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dl)\n",
    "        avg_train_losses.append(avg_train_loss)\n",
    "        eval_res = eval_loop(valid_dl, model)\n",
    "        avg_val_loss = eval_res['avg_val_loss']\n",
    "        avg_val_score = eval_res['avg_val_score']\n",
    "        avg_val_losses.append(avg_val_loss)\n",
    "        avg_val_scores.append(avg_val_score.item())\n",
    "        if use_wandb:\n",
    "            wandb_log(epoch, avg_train_loss, avg_val_loss, avg_val_score)\n",
    "        if best_avg_val_score < avg_val_score:\n",
    "            best_avg_val_score = avg_val_score\n",
    "            # save best model\n",
    "            if os.path.isdir(path/'model') == False:\n",
    "                os.makedirs(path/'model')\n",
    "            torch.save(model.state_dict(), path/f'model/best_model_{model_name}.pt')\n",
    "        if print_score:\n",
    "            print(f'{epoch}: avg_val_score: {avg_val_score}')\n",
    "    return pd.DataFrame({'avg_train_losses': avg_train_losses, 'avg_val_losses': avg_val_losses, 'avg_val_scores': avg_val_scores}), pd.DataFrame({'lr': lr}), pd.DataFrame({'scale_scores': scale_scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df, lr_df, scale_df = train_loop(EPOCHS, sample_dl, sample_dl, sample_model, lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sample_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df[['avg_train_losses', 'avg_val_losses']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df[['avg_val_scores']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df[['avg_val_scores']].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFOLD = 5\n",
    "kf = KFold(n_splits=NFOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor(df):\n",
    "    return torch.tensor(df.values, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_patients = submission_df['Patient']\n",
    "submission_df['dummy_FVC'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_dl = create_dl(submission_df[FE], pd.Series(np.zeros(submission_df[FE].shape[0])), submission_patients, image_path='test', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample, y_sample, image_sample = next(iter(submission_dl))\n",
    "x_sample.shape, y_sample.shape, image_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = np.zeros((submission_df.shape[0], 3))\n",
    "pred = np.zeros((train_df.shape[0], 3))\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dl, model):\n",
    "    prediction = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for x, y, image_tensor in dl:\n",
    "            model = model.cpu()\n",
    "            prediction.append(model(x.detach().cpu(), image_tensor.detach().cpu()))\n",
    "    return torch.cat(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "res_dfs = []\n",
    "for cnt, (tr_idx, val_idx) in tqdm(enumerate(kf.split(X)), total=NFOLD):\n",
    "    X_train, y_train = X.loc[tr_idx], y[tr_idx]\n",
    "    X_valid, y_valid = X.loc[val_idx], y[val_idx]\n",
    "    print(f\"FOLD {cnt}\", X_train.shape, y_train.shape, X_valid.shape, y_valid.shape)\n",
    "    model = create_model()\n",
    "    train_dl = create_dl(X_train, y_train, train_df.loc[tr_idx]['Patient'], image_path='train')\n",
    "    valid_dl = create_dl(X_valid, y_valid, train_df.loc[val_idx]['Patient'], image_path='train')\n",
    "    res_df, _, _ = train_loop(EPOCHS, train_dl, valid_dl, model, print_score=True, lr = LR, model_name=str(cnt), use_wandb=True)\n",
    "    res_dfs.append(res_df)\n",
    "    pred[val_idx] = predict(valid_dl, model)\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean validation score last:\", np.mean([res_dfs[i]['avg_val_scores'][len(res_dfs[0]) - 1] for i in range(NFOLD)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best validation score:\", np.mean([res_dfs[i]['avg_val_scores'].max() for i in range(NFOLD)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "def plot_results(cols=['avg_train_losses', 'avg_val_losses']):\n",
    "    nrows = len(res_dfs) // 2 + 1\n",
    "    ncols = 2\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(20, 10))\n",
    "    figure(num=None, figsize=(10, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    for r in range(nrows):\n",
    "        for c in range(ncols):\n",
    "            index = r * 2 + c\n",
    "            if index < len(res_dfs):\n",
    "                res_dfs[r * 2 + c][cols].plot(ax=axes[r,c])\n",
    "                \n",
    "plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(['avg_val_scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model(i):\n",
    "    model_path = path/f'model/best_model_{i}.pt'\n",
    "    model = create_model()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NFOLD):\n",
    "    model = load_best_model(i)\n",
    "    pe += predict(submission_dl, model).numpy()\n",
    "pe = pe / NFOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_opt = mean_absolute_error(y, pred[:, 1])\n",
    "unc = pred[:,2] - pred[:, 0]\n",
    "sigma_mean = np.mean(unc)\n",
    "sigma_opt, sigma_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['FVC1'] = pe[:,1]\n",
    "submission_df['Confidence1'] = pe[:, 2] - pe[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = submission_df[['Patient_Week','FVC','Confidence','FVC1','Confidence1']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.loc[~subm.FVC1.isnull()].shape, subm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.loc[~subm.FVC1.isnull(),'FVC'] = subm.loc[~subm.FVC1.isnull(),'FVC1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sigma_mean<70:\n",
    "    subm['Confidence'] = sigma_opt\n",
    "else:\n",
    "    subm.loc[~subm.FVC1.isnull(),'Confidence'] = subm.loc[~subm.FVC1.isnull(),'Confidence1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_existing(df):\n",
    "    for i in range(len(df)):\n",
    "        patient_week_filter = subm['Patient_Week']==df.Patient[i]+'_'+str(df.Weeks[i])\n",
    "        subm.loc[patient_week_filter, 'FVC'] = df.FVC[i]\n",
    "        subm.loc[patient_week_filter, 'Confidence'] = 0.1\n",
    "\n",
    "train_df = pd.read_csv(path/'train.csv', dtype = common.TRAIN_TYPES)\n",
    "test_df = pd.read_csv(path/'test.csv', dtype = common.TRAIN_TYPES)\n",
    "replace_with_existing(train_df)\n",
    "replace_with_existing(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm[subm['Patient_Week'].str.find('ID00419637202311204720264') > -1].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_final_df = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Patient'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_final_df[submission_final_df['Patient_Week'].str.find('ID00419637202311204720264') == 0]['FVC'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_final_df[submission_final_df['Patient_Week'].str.find('ID00421637202311550012437') == 0]['FVC'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_final_df[submission_final_df['Patient_Week'].str.find('ID00423637202312137826377') == 0]['FVC'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_final_df[submission_final_df['Patient_Week'].str.find('ID00422637202311677017371') == 0]['FVC'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
