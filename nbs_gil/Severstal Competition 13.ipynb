{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from fastai.vision import *\n",
    "from fastai import *\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = pd.read_csv('/root/.fastai/data/severstal/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId_ClassId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002cc93b.jpg_1</td>\n",
       "      <td>29102 12 29346 24 29602 24 29858 24 30114 24 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002cc93b.jpg_2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002cc93b.jpg_3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002cc93b.jpg_4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00031f466.jpg_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId_ClassId                                      EncodedPixels\n",
       "0  0002cc93b.jpg_1  29102 12 29346 24 29602 24 29858 24 30114 24 3...\n",
       "1  0002cc93b.jpg_2                                                NaN\n",
       "2  0002cc93b.jpg_3                                                NaN\n",
       "3  0002cc93b.jpg_4                                                NaN\n",
       "4  00031f466.jpg_1                                                NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/root/.fastai/data/severstal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/root/.fastai/data/severstal/train_images.zip'),\n",
       " PosixPath('/root/.fastai/data/severstal/sample_submission.csv'),\n",
       " PosixPath('/root/.fastai/data/severstal/test_images.zip'),\n",
       " PosixPath('/root/.fastai/data/severstal/train.csv'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images'),\n",
       " PosixPath('/root/.fastai/data/severstal/test_images')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/root/.fastai/data/severstal/train_images/5e581254c.jpg'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images/fd2f7b4f4.jpg'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images/82f4c0b69.jpg')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = get_image_files(path/'train_images')\n",
    "train_images[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check maximum size of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_img_max_size(folder):\n",
    "    max_height = 0\n",
    "    max_width = 0\n",
    "    for train_image in train_images:\n",
    "        img = open_image(train_image)\n",
    "        if max_height < img.shape[1]:\n",
    "            max_height = img.shape[1]\n",
    "        if max_width < img.shape[2]:\n",
    "            max_width = img.shape[2]\n",
    "    return max_height, max_width\n",
    "\n",
    "def show_image(images, index):\n",
    "    img_f = images[index]\n",
    "    print(type(img_f))\n",
    "    img = open_image(img_f)\n",
    "    print(img)\n",
    "    img.show(figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_path = Path('/kaggle/mask')\n",
    "if not os.path.exists(mask_path):\n",
    "    os.makedirs(str(mask_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_encoded_to_array(encoded_pixels):\n",
    "    pos_array = []\n",
    "    len_array = []\n",
    "    splits = encoded_pixels.split()\n",
    "    pos_array = [int(n) - 1 for i, n in enumerate(splits) if i % 2 == 0]\n",
    "    len_array = [int(n) for i, n in enumerate(splits) if i % 2 == 1]\n",
    "    return pos_array, len_array\n",
    "        \n",
    "def convert_to_pair(pos_array, rows):\n",
    "    return [(p % rows, p // rows) for p in pos_array]\n",
    "\n",
    "def create_positions(single_pos, size):\n",
    "    return [i for i in range(single_pos, single_pos + size)]\n",
    "\n",
    "def create_positions_pairs(single_pos, size, row_size):\n",
    "    return convert_to_pair(create_positions(single_pos, size), row_size)\n",
    "\n",
    "def convert_to_mask(encoded_pixels, row_size, col_size, category):\n",
    "    pos_array, len_array = convert_encoded_to_array(encoded_pixels)\n",
    "    mask = np.zeros([row_size, col_size])\n",
    "    for(p, l) in zip(pos_array, len_array):\n",
    "        for row, col in create_positions_pairs(p, l, row_size):\n",
    "            mask[row][col] = category\n",
    "    return mask\n",
    "\n",
    "def save_to_image(masked, image_name):\n",
    "    im = PIL.Image.fromarray(masked)\n",
    "    im = im.convert(\"L\")\n",
    "    image_name = re.sub(r'(.+)\\.jpg', r'\\1', image_name) + \".png\"\n",
    "    real_path = mask_path/image_name\n",
    "    im.save(real_path)\n",
    "    return real_path\n",
    "\n",
    "def open_single_image(path):\n",
    "    img = open_image(path)\n",
    "    img.show(figsize=(20,20))\n",
    "    \n",
    "def get_y_fn(x):\n",
    "    return mask_path/(x.stem + '.png')\n",
    "\n",
    "def group_by(train_images, train_pd):\n",
    "    tran_dict = {image.name:[] for image in train_images}\n",
    "    pattern = re.compile('(.+)_(\\d+)')\n",
    "    for index, image_path in train_pd.iterrows():\n",
    "        m = pattern.match(image_path['ImageId_ClassId'])\n",
    "        file_name = m.group(1)\n",
    "        category = m.group(2)\n",
    "        tran_dict[file_name].append((int(category), image_path['EncodedPixels']))\n",
    "    return tran_dict\n",
    "\n",
    "def display_image_with_mask(img_name):\n",
    "    full_image = path/'train_images'/img_name\n",
    "    print(full_image)\n",
    "    open_single_image(full_image)\n",
    "    mask_image = get_y_fn(full_image)\n",
    "    mask = open_mask(mask_image)\n",
    "    print(full_image)\n",
    "    mask.show(figsize=(20, 20), alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_categories_mask = group_by(train_images, train_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create mask files and save these to kaggle/mask/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 256\n",
    "image_width = 1600\n",
    "if not os.path.exists(mask_path/'0002cc93b.png'):\n",
    "    for image_name, cat_list in grouped_categories_mask.items():\n",
    "        masked = np.zeros([image_height, image_width])\n",
    "        for cat_mask in cat_list:\n",
    "            encoded_pixels = cat_mask[1]\n",
    "            if pd.notna(cat_mask[1]):\n",
    "                masked += convert_to_mask(encoded_pixels, image_height, image_width, cat_mask[0])\n",
    "        if np.amax(masked) > 4:\n",
    "            print(f'Check {image_name} for max category {np.amax(masked)}')\n",
    "        save_to_image(masked, image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limited_dihedral_affine(k:partial(uniform_int,0,3)):\n",
    "    \"Randomly flip `x` image based on `k`.\"\n",
    "    x = -1 if k&1 else 1\n",
    "    y = -1 if k&2 else 1\n",
    "    if k&4: return [[0, x, 0.],\n",
    "                    [y, 0, 0],\n",
    "                    [0, 0, 1.]]\n",
    "    return [[x, 0, 0.],\n",
    "            [0, y, 0],\n",
    "            [0, 0, 1.]]\n",
    "\n",
    "dihedral_affine = TfmAffine(limited_dihedral_affine)\n",
    "\n",
    "def get_extra_transforms(max_rotate:float=3., max_zoom:float=1.1,\n",
    "                   max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75,\n",
    "                   p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None)->Collection[Transform]:\n",
    "    \"Utility func to easily create a list of flip, rotate, `zoom`, warp, lighting transforms.\"\n",
    "    p_lightings = [p_lighting, p_lighting + 0.2, p_lighting + 0.4, p_lighting + 0.6, p_lighting + 0.7]\n",
    "    max_lightings = [max_lighting, max_lighting + 0.2, max_lighting + 0.4, max_lighting + 0.6, max_lighting + 0.7]\n",
    "    res = [rand_crop(), dihedral_affine(), \n",
    "           symmetric_warp(magnitude=(-max_warp,max_warp), p=p_affine),\n",
    "           rotate(degrees=(-max_rotate,max_rotate), p=p_affine),\n",
    "           rand_zoom(scale=(1., max_zoom), p=p_affine)]\n",
    "    res.extend([brightness(change=(0.5*(1-mp[0]), 0.5*(1+mp[0])), p=mp[1]) for mp in zip(max_lightings, p_lightings)])\n",
    "    res.extend([contrast(scale=(1-mp[0], 1/(1-mp[0])), p=mp[1]) for mp in zip(max_lightings, p_lightings)])\n",
    "    #       train                   , valid\n",
    "    return (res, [crop_pad()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = (path/'train_images').ls()\n",
    "src_size = np.array(open_image(str(train_images[0])).shape[1:])\n",
    "valid_pct = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = array(['0', '1', '2', '3', '4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = (SegmentationItemList.from_folder(path/'train_images')\n",
    "       .split_by_rand_pct(valid_pct=valid_pct)\n",
    "       .label_from_func(get_y_fn, classes=codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "size = src_size//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (src.transform(get_extra_transforms(), size=size, tfm_y=True)\n",
    "        .add_test(ImageList.from_folder(path/'test_images'), tfms=None, tfm_y=False)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create learner and training\n",
    "Starting with low resolution training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "name2id = {v:k for k,v in enumerate(codes)}\n",
    "void_code = name2id['0']\n",
    "\n",
    "def acc_camvid(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    mask = target != void_code\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    comparison = argmax[mask]==target[mask]\n",
    "    return torch.tensor(0.) if comparison.numel() == 0 else comparison.float().mean()\n",
    "\n",
    "def acc_camvid_with_zero_check(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        if(torch.sum(argmax[b]).item() == 0.0 and torch.sum(target[b]).item() == 0.0):\n",
    "            total[b] = 1\n",
    "        else:\n",
    "            mask = target[b] != void_code\n",
    "            comparison = argmax[b][mask]==target[b][mask]\n",
    "            total[b] = torch.tensor(0.) if comparison.numel() == 0 else comparison.float().mean()\n",
    "    return total.mean()\n",
    "\n",
    "\n",
    "def calc_dice_coefficients(argmax, target, cats):\n",
    "    def calc_dice_coefficient(seg, gt, cat: int):\n",
    "        mask_seg = seg == cat\n",
    "        mask_gt = gt == cat\n",
    "        sum_seg = torch.sum(mask_seg.float())\n",
    "        sum_gt = torch.sum(mask_gt.float())\n",
    "        if sum_seg + sum_gt == 0:\n",
    "            return torch.tensor(1.0)\n",
    "        return (torch.sum((seg[gt == cat] / cat).float()) * 2.0) / (sum_seg + sum_gt)\n",
    "\n",
    "    total_avg = torch.empty([len(cats)])\n",
    "    for i, c in enumerate(cats):\n",
    "        total_avg[i] = calc_dice_coefficient(argmax, target, c)\n",
    "    return total_avg.mean()\n",
    "\n",
    "\n",
    "def dice_coefficient(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    cats = [1, 2, 3, 4]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        total[b] = calc_dice_coefficients(argmax[b], target[b], cats)\n",
    "    return total.mean()\n",
    "\n",
    "def calc_dice_coefficients_2(argmax, target, cats):\n",
    "    def calc_dice_coefficient(seg, gt, cat: int):\n",
    "        mask_seg = seg == cat\n",
    "        mask_gt = gt == cat\n",
    "        sum_seg = torch.sum(mask_seg.float())\n",
    "        sum_gt = torch.sum(mask_gt.float())\n",
    "        return (torch.sum((seg[gt == cat] / cat).float())), (sum_seg + sum_gt)\n",
    "\n",
    "    total_avg = torch.empty([len(cats), 2])\n",
    "    for i, c in enumerate(cats):\n",
    "        total_avg[i][0], total_avg[i][1] = calc_dice_coefficient(argmax, target, c)\n",
    "    total_sum = total_avg.sum(axis=0)\n",
    "    if (total_sum[1] == 0.0):\n",
    "        return torch.tensor(1.0)\n",
    "    return total_sum[0] * 2.0 / total_sum[1]\n",
    "\n",
    "\n",
    "def dice_coefficient_2(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    cats = [1, 2, 3, 4]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        total[b] = calc_dice_coefficients_2(argmax[b], target[b], cats)\n",
    "    return total.mean()\n",
    "\n",
    "\n",
    "def accuracy_simple(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    return (input.argmax(dim=1)==target).float().mean()\n",
    "\n",
    "\n",
    "def dice_coeff(pred, target):\n",
    "    smooth = 1.\n",
    "    num = pred.size(0)\n",
    "    m1 = pred.view(num, -1)  # Flatten\n",
    "    m2 = target.view(num, -1)  # Flatten\n",
    "    intersection = (m1 * m2).sum()\n",
    "    return (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import callbacks\n",
    "\n",
    "def train_learner(learn, slice_lr, epochs=10, pct_start=0.8, best_model_name='best_model', \n",
    "                  patience_early_stop=4, patience_reduce_lr = 3):\n",
    "    learn.fit_one_cycle(epochs, slice_lr, pct_start=pct_start, \n",
    "                    callbacks=[callbacks.SaveModelCallback(learn, monitor='dice_coefficient',mode='max', name=best_model_name),\n",
    "                              callbacks.EarlyStoppingCallback(learn=learn, monitor='dice_coefficient', patience=patience_early_stop),\n",
    "                              callbacks.ReduceLROnPlateauCallback(learn=learn, monitor='dice_coefficient', patience=patience_reduce_lr),\n",
    "                              callbacks.TerminateOnNaNCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=accuracy_simple, acc_camvid_with_zero_check, dice_coefficient, dice_coefficient_2\n",
    "wd=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedCrossEntropyFlat"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd, bottle=True)\n",
    "learn.loss_func = CrossEntropyFlat(axis=1, weight=torch.tensor([2.0, .5, .5, .5, .5]).cuda())\n",
    "learn.loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model_dir = Path('/kaggle/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn = to_fp16(learn, loss_scale=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_find(learn, num_it=400)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      30.00% [3/10 20:29<47:48]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_simple</th>\n",
       "      <th>acc_camvid_with_zero_check</th>\n",
       "      <th>dice_coefficient</th>\n",
       "      <th>dice_coefficient_2</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.297326</td>\n",
       "      <td>0.287130</td>\n",
       "      <td>0.970575</td>\n",
       "      <td>0.492834</td>\n",
       "      <td>0.865048</td>\n",
       "      <td>0.492834</td>\n",
       "      <td>06:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.298064</td>\n",
       "      <td>0.287130</td>\n",
       "      <td>0.970575</td>\n",
       "      <td>0.492834</td>\n",
       "      <td>0.865048</td>\n",
       "      <td>0.492834</td>\n",
       "      <td>06:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.292913</td>\n",
       "      <td>0.287130</td>\n",
       "      <td>0.970575</td>\n",
       "      <td>0.492834</td>\n",
       "      <td>0.865048</td>\n",
       "      <td>0.492834</td>\n",
       "      <td>06:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='2828', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with dice_coefficient value: 0.8650477528572083.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e3b0494ec619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train_learner(learn, slice(lr), epochs=10, pct_start=0.8, best_model_name='bestmodel-frozen-1', \n\u001b[0;32m----> 2\u001b[0;31m               patience_early_stop=4, patience_reduce_lr = 3)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-f266707c06c5>\u001b[0m in \u001b[0;36mtrain_learner\u001b[0;34m(learn, slice_lr, epochs, pct_start, best_model_name, patience_early_stop, patience_reduce_lr)\u001b[0m\n\u001b[1;32m      7\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStoppingCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dice_coefficient'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatience_early_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateauCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dice_coefficient'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatience_reduce_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                               callbacks.TerminateOnNaNCallback()])\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     21\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb_fns_registered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_update\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_interrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_t\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcur_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_update\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_iter_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36mupdate_bar\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your generator is empty.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'100% [0/0]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{100 * val/self.total:.2f}% [{val}/{self.total} {elapsed_t}<{remaining_t}{end}]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36mon_update\u001b[0;34m(self, val, text, interrupted)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml_progress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterrupted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mNBMasterBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMasterBar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mto_show\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_show\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, obj, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0madditional\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mupdate_display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \"\"\"\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mupdate_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mupdate_display\u001b[0;34m(obj, display_id, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \"\"\"\n\u001b[1;32m    335\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'update'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_display_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0;31m# object handled itself, don't proceed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</opt/conda/envs/deeplearning/lib/python3.7/site-packages/decorator.py:decorator-gen-11>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    914\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/traitlets/traitlets.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/traitlets/traitlets.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trait_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;31m# Check for a dynamic initializer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_learner(learn, slice(lr), epochs=10, pct_start=0.8, best_model_name='bestmodel-frozen-1', \n",
    "              patience_early_stop=4, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('bestmodel-frozen-1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(file='/kaggle/model/export-1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = slice(lr/100,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_learner(learn, lrs, epochs=6, pct_start=0.8, best_model_name='bestmodel-unfrozen-1', \n",
    "              patience_early_stop=4, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('bestmodel-unfrozen-1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(file='/kaggle/model/export-2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_large_learner(bs=4, transform_func=get_extra_transforms, model_to_load='bestmodel-unfrozen-1'):\n",
    "    src = (SegmentationItemList.from_folder(path/'train_images')\n",
    "       .split_by_rand_pct(valid_pct=valid_pct)\n",
    "       .label_from_func(get_y_fn, classes=codes))\n",
    "    data = (src.transform(transform_func(), size=src_size, tfm_y=True)\n",
    "        .add_test(ImageList.from_folder(path/'test_images'), tfms=None, tfm_y=False)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))\n",
    "    learn = unet_learner(data, models.resnet50, metrics=metrics, wd=wd, bottle=True)\n",
    "    learn.model_dir = Path('/kaggle/model')\n",
    "    learn.loss_func = CrossEntropyFlat(axis=1, weight=torch.tensor([2.0, .5, .5, .5, .5]).cuda())\n",
    "    learn = to_fp16(learn, loss_scale=4.0)\n",
    "    learn.load(model_to_load)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_large_learner(bs=bs, transform_func=get_extra_transforms, model_to_load='bestmodel-unfrozen-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn, num_it=400)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_learner(learn, slice(lr), epochs=8, pct_start=0.8, best_model_name='bestmodel-frozen-3', \n",
    "              patience_early_stop=4, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-3');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('bestmodel-frozen-3');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(file='/kaggle/model/export-3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_large_learner(bs=bs, transform_func=get_extra_transforms, model_to_load='bestmodel-frozen-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = slice(lr/1000,lr/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_learner(learn, lrs, epochs=10, pct_start=0.8, best_model_name='bestmodel-4', \n",
    "              patience_early_stop=5, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-4');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('bestmodel-4');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(file='/kaggle/model/export-4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!cp /kaggle/model/export.pkl /opt/fastai/fastai-exercises/nbs_gil\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'export-4.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn=None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = (path/'test_images').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_learn = load_learner('/kaggle/model/', file='export-2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_learn = to_fp16(inference_learn, loss_scale=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_path):\n",
    "    pred_class, pred_idx, outputs = inference_learn.predict(open_image(str(img_path)))\n",
    "    return pred_class, pred_idx, outputs\n",
    "\n",
    "def encode_classes(pred_class_data):\n",
    "    pixels = np.concatenate([[0], torch.transpose(pred_class_data.squeeze(), 0, 1).flatten(), [0]])\n",
    "    classes_dict = {1: [], 2: [], 3: [], 4: []}\n",
    "    count = 0\n",
    "    previous = pixels[0]\n",
    "    for i, val in enumerate(pixels):\n",
    "        if val != previous:\n",
    "            if previous in classes_dict:\n",
    "                classes_dict[previous].append((i - count, count))\n",
    "            count = 0\n",
    "        previous = val\n",
    "        count += 1\n",
    "    return classes_dict\n",
    "\n",
    "\n",
    "def convert_classes_to_text(classes_dict, clazz):\n",
    "    return ' '.join([f'{v[0]} {v[1]}' for v in classes_dict[clazz]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_to_predict = train_images[16].name\n",
    "display_image_with_mask(image_to_predict)\n",
    "pred_class, pred_idx, outputs = predict(path/f'train_images/{image_to_predict}')\n",
    "pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(pred_class.data.squeeze(), 0, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking encoding methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_all = encode_classes(pred_class.data)\n",
    "print(convert_classes_to_text(encoded_all, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = train_images[16]\n",
    "print(get_y_fn(image_name))\n",
    "img = open_mask(get_y_fn(image_name))\n",
    "img_data = img.data\n",
    "print(convert_classes_to_text(encode_classes(img_data), 3))\n",
    "img_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through the test images and create submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "defect_classes = [1, 2, 3, 4]\n",
    "with open('submission.csv', 'w') as submission_file:\n",
    "    submission_file.write('ImageId_ClassId,EncodedPixels\\n')\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        pred_class, pred_idx, outputs = predict(test_image)\n",
    "        encoded_all = encode_classes(pred_class.data)\n",
    "        for defect_class in defect_classes:\n",
    "            submission_file.write(f'{test_image.name}_{defect_class},{convert_classes_to_text(encoded_all, defect_class)}\\n')\n",
    "        if i % 5 == 0:\n",
    "            print(f'Processed {i} images\\r', end='')\n",
    "            \n",
    "print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative prediction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,y = learn.get_preds(ds_type=DatasetType.Test, with_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class_data = preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((path/'test_images').ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test_ds.x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
