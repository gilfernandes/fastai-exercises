
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/07_batchnorm.ipynb

from exp.nb_06 import *

def init_cnn_(module, fun):
    if isinstance(module, nn.Conv2d):
        fun(module.weight, a=.1)
        if getattr(module, 'bias', None) is not None:
            module.bias.data.zero_()
    for l in module.children():
        init_cnn_(l, fun)

def init_cnn(model, uniform=False):
    f = init.kaiming_uniform_ if uniform else init.kaiming_normal_
    init_cnn_(model, f)

def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, uniform=False, **kwargs):
    model = get_cnn_model(data, nfs, layer, **kwargs)
    init_cnn(model, uniform)
    return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)

def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs):
    # No bias needed using batch norm
    layers = [nn.Conv2d(ni, nf, kernel_size=ks, padding=ks//2, stride=stride, bias = not bn), GeneralRelu(**kwargs)]
    if bn:
        layers.append(nn.BatchNorm2d(nf, momentum=0.1))
    return nn.Sequential(*layers)