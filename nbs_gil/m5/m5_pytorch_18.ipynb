{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  datetime import datetime, timedelta\n",
    "import gc\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scripts import m5_common\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/kaggle/m5_forecasting/')\n",
    "assert(path.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 4, 25, 0, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 28 \n",
    "tr_last = 1913\n",
    "fday = datetime(2016, 4, 25) \n",
    "fday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to MLflow server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow server --backend-store-uri mlruns/ --default-artifact-root mlruns/ --host 0.0.0.0 --port 5000\n",
    "# server in /opt/mlflow_server/start.sh\n",
    "remote_server_uri = \"http://localhost:5000\" # set to your server URI\n",
    "mlflow.set_tracking_uri(remote_server_uri)  # or set the MLFLOW_TRACKING_URI in the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment('M5_Pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.42 s, sys: 160 ms, total: 1.58 s\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prices, cal = m5_common.prepare_tables(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_name_1_map, event_type_1_map = m5_common.replace_cal_cols(cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "uint8_types= ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'month', 'wday', 'weekday', \n",
    "              'snap_CA', 'snap_TX', 'snap_WI']\n",
    "m5_common.convert_uint8(cal, uint8_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m5_common.add_days_before(cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_DAY = 1 # If you want to load all the data set it to '1' -->  Great  memory overflow  risk !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42 s, sys: 5.8 s, total: 47.8 s\n",
      "Wall time: 47.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = m5_common.create_dt(cal, prices, is_train=True, first_day= FIRST_DAY, tr_last=tr_last, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_cats(dt):\n",
    "    m5_common.replace_cat(dt, 'wday')\n",
    "    m5_common.replace_cat(dt, 'month')\n",
    "    m5_common.replace_cat(dt, 'year')\n",
    "    m5_common.replace_cat(dt, 'mday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6}\n",
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11}\n",
      "{2011: 0, 2012: 1, 2013: 2, 2014: 3, 2015: 4, 2016: 5}\n",
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30}\n"
     ]
    }
   ],
   "source": [
    "replace_cats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 46027957 entries, 4081 to 46025082\n",
      "Data columns (total 27 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   id                object        \n",
      " 1   item_id           int16         \n",
      " 2   dept_id           int16         \n",
      " 3   store_id          int16         \n",
      " 4   cat_id            int16         \n",
      " 5   state_id          int16         \n",
      " 6   d                 object        \n",
      " 7   sales             float16       \n",
      " 8   date              datetime64[ns]\n",
      " 9   wm_yr_wk          int16         \n",
      " 10  weekday           uint8         \n",
      " 11  wday              uint8         \n",
      " 12  month             uint8         \n",
      " 13  year              uint8         \n",
      " 14  event_name_1      uint8         \n",
      " 15  event_type_1      uint8         \n",
      " 16  event_name_2      uint8         \n",
      " 17  event_type_2      uint8         \n",
      " 18  snap_CA           uint8         \n",
      " 19  snap_TX           uint8         \n",
      " 20  snap_WI           uint8         \n",
      " 21  before_christmas  uint16        \n",
      " 22  sell_price        float16       \n",
      " 23  Dayofyear         uint16        \n",
      " 24  week              uint8         \n",
      " 25  mday              uint8         \n",
      " 26  lag_price_1       float16       \n",
      "dtypes: datetime64[ns](1), float16(3), int16(6), object(2), uint16(2), uint8(13)\n",
      "memory usage: 2.9+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_cols(dt, cols):\n",
    "    dt[f\"mean_{'_'.join(cols)}\"] = dt[cols].mean(axis=1)\n",
    "\n",
    "def create_fea(dt, dropna=True):\n",
    "    \n",
    "    wins = [7, 28]\n",
    "    lags = [7, 28]\n",
    "    \n",
    "    grouped_sales = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"]\n",
    "    \n",
    "    for win in wins:\n",
    "        mean_col = f'mean_{win}'\n",
    "        emean_col = f'e{mean_col}' # exponential mean average\n",
    "        esmean_col = f'es{mean_col}'\n",
    "        dt[emean_col] = grouped_sales.transform(lambda x : x.ewm(span=win, adjust=False).mean())\n",
    "        dt[esmean_col] = grouped_sales.transform(lambda x : x.ewm(alpha=1/win, adjust=False).mean())\n",
    "        for lag in lags:\n",
    "            dt[f'emean_{win}_{lag}'] = dt[[\"id\", emean_col]].groupby(\"id\").shift(lag)\n",
    "            dt[f'esmean_{win}_{lag}'] = dt[[\"id\", esmean_col]].groupby(\"id\").shift(lag)\n",
    "        del dt[emean_col]\n",
    "        del dt[esmean_col]\n",
    "            \n",
    "    ra = [1, 2, 3, 4, 5, 6, 7]\n",
    "    for simple_lag in ra:\n",
    "        dt[f'lag_{simple_lag}'] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(simple_lag)\n",
    "        \n",
    "#     mean_cols(dt, ['lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7'])\n",
    "    \n",
    "    if dropna == True:\n",
    "        dt.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 52s, sys: 10.4 s, total: 2min 2s\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "create_fea(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 45174237 entries, 342559 to 46025082\n",
      "Data columns (total 42 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   id                object        \n",
      " 1   item_id           int16         \n",
      " 2   dept_id           int16         \n",
      " 3   store_id          int16         \n",
      " 4   cat_id            int16         \n",
      " 5   state_id          int16         \n",
      " 6   d                 object        \n",
      " 7   sales             float16       \n",
      " 8   date              datetime64[ns]\n",
      " 9   wm_yr_wk          int16         \n",
      " 10  weekday           uint8         \n",
      " 11  wday              uint8         \n",
      " 12  month             uint8         \n",
      " 13  year              uint8         \n",
      " 14  event_name_1      uint8         \n",
      " 15  event_type_1      uint8         \n",
      " 16  event_name_2      uint8         \n",
      " 17  event_type_2      uint8         \n",
      " 18  snap_CA           uint8         \n",
      " 19  snap_TX           uint8         \n",
      " 20  snap_WI           uint8         \n",
      " 21  before_christmas  uint16        \n",
      " 22  sell_price        float16       \n",
      " 23  Dayofyear         uint16        \n",
      " 24  week              uint8         \n",
      " 25  mday              uint8         \n",
      " 26  lag_price_1       float16       \n",
      " 27  emean_7_7         float16       \n",
      " 28  esmean_7_7        float16       \n",
      " 29  emean_7_28        float16       \n",
      " 30  esmean_7_28       float16       \n",
      " 31  emean_28_7        float16       \n",
      " 32  esmean_28_7       float16       \n",
      " 33  emean_28_28       float16       \n",
      " 34  esmean_28_28      float16       \n",
      " 35  lag_1             float16       \n",
      " 36  lag_2             float16       \n",
      " 37  lag_3             float16       \n",
      " 38  lag_4             float16       \n",
      " 39  lag_5             float16       \n",
      " 40  lag_6             float16       \n",
      " 41  lag_7             float16       \n",
      "dtypes: datetime64[ns](1), float16(18), int16(6), object(2), uint16(2), uint8(13)\n",
      "memory usage: 4.1+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45174237, 42)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>...</th>\n",
       "      <th>esmean_28_7</th>\n",
       "      <th>emean_28_28</th>\n",
       "      <th>esmean_28_28</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>lag_4</th>\n",
       "      <th>lag_5</th>\n",
       "      <th>lag_6</th>\n",
       "      <th>lag_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>342559</th>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "      <td>1536</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>d_29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-02-26</td>\n",
       "      <td>11105</td>\n",
       "      <td>...</td>\n",
       "      <td>2.021484</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342560</th>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "      <td>1536</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>d_30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-02-27</td>\n",
       "      <td>11105</td>\n",
       "      <td>...</td>\n",
       "      <td>2.021484</td>\n",
       "      <td>2.792969</td>\n",
       "      <td>2.892578</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342561</th>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "      <td>1536</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>d_31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-02-28</td>\n",
       "      <td>11105</td>\n",
       "      <td>...</td>\n",
       "      <td>1.949219</td>\n",
       "      <td>2.599609</td>\n",
       "      <td>2.789062</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342562</th>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "      <td>1536</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>d_32</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-03-01</td>\n",
       "      <td>11105</td>\n",
       "      <td>...</td>\n",
       "      <td>1.951172</td>\n",
       "      <td>2.490234</td>\n",
       "      <td>2.726562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342563</th>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "      <td>1536</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>d_33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2011-03-02</td>\n",
       "      <td>11105</td>\n",
       "      <td>...</td>\n",
       "      <td>1.953125</td>\n",
       "      <td>2.593750</td>\n",
       "      <td>2.771484</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45956915</th>\n",
       "      <td>HOUSEHOLD_2_516_WI_3_evaluation</td>\n",
       "      <td>2047</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>d_1909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-04-20</td>\n",
       "      <td>11612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038788</td>\n",
       "      <td>0.079956</td>\n",
       "      <td>0.083252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45956916</th>\n",
       "      <td>HOUSEHOLD_2_516_WI_3_evaluation</td>\n",
       "      <td>2047</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>d_1910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-04-21</td>\n",
       "      <td>11612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037384</td>\n",
       "      <td>0.074463</td>\n",
       "      <td>0.080261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45956917</th>\n",
       "      <td>HOUSEHOLD_2_516_WI_3_evaluation</td>\n",
       "      <td>2047</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>d_1911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>11612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036041</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.077393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46025081</th>\n",
       "      <td>HOUSEHOLD_2_516_WI_3_evaluation</td>\n",
       "      <td>2047</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>d_1912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>11613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034760</td>\n",
       "      <td>0.064514</td>\n",
       "      <td>0.074646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46025082</th>\n",
       "      <td>HOUSEHOLD_2_516_WI_3_evaluation</td>\n",
       "      <td>2047</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>d_1913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>11613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033539</td>\n",
       "      <td>0.060089</td>\n",
       "      <td>0.071960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45174237 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id  item_id  dept_id  store_id  cat_id  \\\n",
       "342559        FOODS_1_001_CA_1_evaluation     1536        4         0       2   \n",
       "342560        FOODS_1_001_CA_1_evaluation     1536        4         0       2   \n",
       "342561        FOODS_1_001_CA_1_evaluation     1536        4         0       2   \n",
       "342562        FOODS_1_001_CA_1_evaluation     1536        4         0       2   \n",
       "342563        FOODS_1_001_CA_1_evaluation     1536        4         0       2   \n",
       "...                                   ...      ...      ...       ...     ...   \n",
       "45956915  HOUSEHOLD_2_516_WI_3_evaluation     2047        3         9       1   \n",
       "45956916  HOUSEHOLD_2_516_WI_3_evaluation     2047        3         9       1   \n",
       "45956917  HOUSEHOLD_2_516_WI_3_evaluation     2047        3         9       1   \n",
       "46025081  HOUSEHOLD_2_516_WI_3_evaluation     2047        3         9       1   \n",
       "46025082  HOUSEHOLD_2_516_WI_3_evaluation     2047        3         9       1   \n",
       "\n",
       "          state_id       d  sales       date  wm_yr_wk  ...  esmean_28_7  \\\n",
       "342559           0    d_29    2.0 2011-02-26     11105  ...     2.021484   \n",
       "342560           0    d_30    2.0 2011-02-27     11105  ...     2.021484   \n",
       "342561           0    d_31    0.0 2011-02-28     11105  ...     1.949219   \n",
       "342562           0    d_32    2.0 2011-03-01     11105  ...     1.951172   \n",
       "342563           0    d_33    1.0 2011-03-02     11105  ...     1.953125   \n",
       "...            ...     ...    ...        ...       ...  ...          ...   \n",
       "45956915         2  d_1909    0.0 2016-04-20     11612  ...     0.038788   \n",
       "45956916         2  d_1910    0.0 2016-04-21     11612  ...     0.037384   \n",
       "45956917         2  d_1911    0.0 2016-04-22     11612  ...     0.036041   \n",
       "46025081         2  d_1912    0.0 2016-04-23     11613  ...     0.034760   \n",
       "46025082         2  d_1913    0.0 2016-04-24     11613  ...     0.033539   \n",
       "\n",
       "          emean_28_28  esmean_28_28  lag_1  lag_2  lag_3  lag_4  lag_5  lag_6  \\\n",
       "342559       3.000000      3.000000    4.0    2.0    2.0    2.0    0.0    2.0   \n",
       "342560       2.792969      2.892578    2.0    4.0    2.0    2.0    2.0    0.0   \n",
       "342561       2.599609      2.789062    2.0    2.0    4.0    2.0    2.0    2.0   \n",
       "342562       2.490234      2.726562    0.0    2.0    2.0    4.0    2.0    2.0   \n",
       "342563       2.593750      2.771484    2.0    0.0    2.0    2.0    4.0    2.0   \n",
       "...               ...           ...    ...    ...    ...    ...    ...    ...   \n",
       "45956915     0.079956      0.083252    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "45956916     0.074463      0.080261    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "45956917     0.069336      0.077393    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "46025081     0.064514      0.074646    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "46025082     0.060089      0.071960    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "          lag_7  \n",
       "342559      1.0  \n",
       "342560      2.0  \n",
       "342561      0.0  \n",
       "342562      2.0  \n",
       "342563      2.0  \n",
       "...         ...  \n",
       "45956915    0.0  \n",
       "45956916    0.0  \n",
       "45956917    0.0  \n",
       "46025081    0.0  \n",
       "46025082    0.0  \n",
       "\n",
       "[45174237 rows x 42 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'selected_features' in locals():\n",
    "    selected_features = ['id', 'item_id', 'dept_id', 'store_id', 'cat_id', 'state_id', 'd', 'sales', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'event_name_1', 'event_type_1', \n",
    "                         'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'before_christmas', 'sell_price', 'Dayofyear', 'week', 'mday', 'lag_price_1', \n",
    "                         'emean_7_7', 'esmean_7_7', 'emean_7_28', 'esmean_7_28', 'emean_28_7', 'esmean_28_7', 'emean_28_28', 'esmean_28_28', 'lag_1', 'lag_2', 'mean_lag_3_lag_4_lag_5_lag_6_lag_7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(dt, selected_features):\n",
    "    for col in dt.columns:\n",
    "        if col not in selected_features:\n",
    "            del dt[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_features(df, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 45174237 entries, 342559 to 46025082\n",
      "Data columns (total 37 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   id                object        \n",
      " 1   item_id           int16         \n",
      " 2   dept_id           int16         \n",
      " 3   store_id          int16         \n",
      " 4   cat_id            int16         \n",
      " 5   state_id          int16         \n",
      " 6   d                 object        \n",
      " 7   sales             float16       \n",
      " 8   date              datetime64[ns]\n",
      " 9   wm_yr_wk          int16         \n",
      " 10  weekday           uint8         \n",
      " 11  wday              uint8         \n",
      " 12  month             uint8         \n",
      " 13  year              uint8         \n",
      " 14  event_name_1      uint8         \n",
      " 15  event_type_1      uint8         \n",
      " 16  event_name_2      uint8         \n",
      " 17  event_type_2      uint8         \n",
      " 18  snap_CA           uint8         \n",
      " 19  snap_TX           uint8         \n",
      " 20  snap_WI           uint8         \n",
      " 21  before_christmas  uint16        \n",
      " 22  sell_price        float16       \n",
      " 23  Dayofyear         uint16        \n",
      " 24  week              uint8         \n",
      " 25  mday              uint8         \n",
      " 26  lag_price_1       float16       \n",
      " 27  emean_7_7         float16       \n",
      " 28  esmean_7_7        float16       \n",
      " 29  emean_7_28        float16       \n",
      " 30  esmean_7_28       float16       \n",
      " 31  emean_28_7        float16       \n",
      " 32  esmean_28_7       float16       \n",
      " 33  emean_28_28       float16       \n",
      " 34  esmean_28_28      float16       \n",
      " 35  lag_1             float16       \n",
      " 36  lag_2             float16       \n",
      "dtypes: datetime64[ns](1), float16(13), int16(6), object(2), uint16(2), uint8(13)\n",
      "memory usage: 3.7+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]\n",
    "if not 'cat_feats' in locals():\n",
    "    cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id', \n",
    "                 \"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\", 'snap_CA', 'snap_TX', 'snap_WI', 'year', 'month', 'wday', 'mday']\n",
    "train_cols = df.columns[~df.columns.isin(useless_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_features(df, list(train_cols) + ['sales',  'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 45174237 entries, 342559 to 46025082\n",
      "Data columns (total 33 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   id                object \n",
      " 1   item_id           int16  \n",
      " 2   dept_id           int16  \n",
      " 3   store_id          int16  \n",
      " 4   cat_id            int16  \n",
      " 5   state_id          int16  \n",
      " 6   sales             float16\n",
      " 7   wday              uint8  \n",
      " 8   month             uint8  \n",
      " 9   year              uint8  \n",
      " 10  event_name_1      uint8  \n",
      " 11  event_type_1      uint8  \n",
      " 12  event_name_2      uint8  \n",
      " 13  event_type_2      uint8  \n",
      " 14  snap_CA           uint8  \n",
      " 15  snap_TX           uint8  \n",
      " 16  snap_WI           uint8  \n",
      " 17  before_christmas  uint16 \n",
      " 18  sell_price        float16\n",
      " 19  Dayofyear         uint16 \n",
      " 20  week              uint8  \n",
      " 21  mday              uint8  \n",
      " 22  lag_price_1       float16\n",
      " 23  emean_7_7         float16\n",
      " 24  esmean_7_7        float16\n",
      " 25  emean_7_28        float16\n",
      " 26  esmean_7_28       float16\n",
      " 27  emean_28_7        float16\n",
      " 28  esmean_28_7       float16\n",
      " 29  emean_28_28       float16\n",
      " 30  esmean_28_28      float16\n",
      " 31  lag_1             float16\n",
      " 32  lag_2             float16\n",
      "dtypes: float16(13), int16(5), object(1), uint16(2), uint8(12)\n",
      "memory usage: 2.9+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(777)\n",
    "\n",
    "valid_size = int(df.shape[0] * 0.1)\n",
    "np.random.seed(777)\n",
    "\n",
    "valid_idx = np.random.choice(df.index.values, valid_size, replace=False)\n",
    "train_idx = np.setdiff1d(df.index.values, valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.loc[train_idx]\n",
    "valid_df = df.loc[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 394 ms, total: 1min 31s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "scale_df = m5_common.rmsse_scales(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data, cat_cols=None, output_col=None):\n",
    "        self.n = len(data)\n",
    "        if output_col:\n",
    "            self.y = data[output_col].astype(np.float32).values.reshape(-1, 1)\n",
    "        else:\n",
    "            self.y = np.zeros((self.n, 1))\n",
    "        self.cat_cols = cat_cols if cat_cols else []\n",
    "        \n",
    "        self.cont_cols = [\n",
    "            col for col in data.columns if col not in self.cat_cols + [output_col, 'id']\n",
    "        ]\n",
    "        \n",
    "        self.cont_X = data[self.cont_cols].astype(np.float32).values\n",
    "        self.cat_X = data[cat_cols].astype(np.int16).values\n",
    "        \n",
    "        self.ids = data['id'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the total number of samples.\n",
    "        \"\"\"\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generates one sample of data.\n",
    "        \"\"\"\n",
    "        return [self.y[idx], self.cont_X[idx], self.cat_X[idx], self.ids[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TabularDataset(train_df, cat_cols=cat_feats, output_col='sales')\n",
    "valid_ds = TabularDataset(valid_df, cat_cols=cat_feats, output_col='sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 4096 * 4\n",
    "num_workers = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batchsize, shuffle=True, num_workers=num_workers)\n",
    "valid_dl = DataLoader(valid_ds, batchsize, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30490, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_map = {}\n",
    "for _, row in scale_df.iterrows():\n",
    "    scale_map[row['id']] = row['scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16384, 15])\n",
      "16384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 3.4215, 23.3217,  2.3044,  ...,  4.4874,  0.8044,  0.8828])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_batch = next(iter(train_dl))\n",
    "# for b in one_batch:\n",
    "print(one_batch[1].shape)\n",
    "print(len(one_batch[3]))\n",
    "torch.tensor([scale_map[id] for id in one_batch[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>0.722280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>0.566946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>0.336297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>7.056485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>2.547071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30485</th>\n",
       "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
       "      <td>1.342573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
       "      <td>1.049163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30487</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>2.316946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30488</th>\n",
       "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
       "      <td>1.108264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30489</th>\n",
       "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
       "      <td>2.771967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30490 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id     scale\n",
       "0      HOBBIES_1_001_CA_1_evaluation  0.722280\n",
       "1      HOBBIES_1_002_CA_1_evaluation  0.566946\n",
       "2      HOBBIES_1_003_CA_1_evaluation  0.336297\n",
       "3      HOBBIES_1_004_CA_1_evaluation  7.056485\n",
       "4      HOBBIES_1_005_CA_1_evaluation  2.547071\n",
       "...                              ...       ...\n",
       "30485    FOODS_3_823_WI_3_evaluation  1.342573\n",
       "30486    FOODS_3_824_WI_3_evaluation  1.049163\n",
       "30487    FOODS_3_825_WI_3_evaluation  2.316946\n",
       "30488    FOODS_3_826_WI_3_evaluation  1.108264\n",
       "30489    FOODS_3_827_WI_3_evaluation  2.771967\n",
       "\n",
       "[30490 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3049, 500),\n",
       " (7, 4),\n",
       " (10, 5),\n",
       " (3, 2),\n",
       " (3, 2),\n",
       " (31, 16),\n",
       " (5, 3),\n",
       " (5, 3),\n",
       " (3, 2),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (6, 3),\n",
       " (12, 6),\n",
       " (7, 4),\n",
       " (31, 16)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_dims = [int(train_df[col].nunique()) for col in cat_feats]\n",
    "emb_dims = [(x, min(50, (x + 1) // 2)) for x in cat_dims]\n",
    "emb_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_clamp = train_df['sales'].max() * 1.3\n",
    "\n",
    "def clamp_pred(pred):\n",
    "    return torch.clamp(pred, 0.0, max_clamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_df\n",
    "del valid_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, emb_dims, no_of_cont, lin_layer_sizes, output_size, emb_dropout, lin_layer_dropouts):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n",
    "        self.no_of_embs = sum([y for x, y in emb_dims])\n",
    "        self.no_of_cont = no_of_cont\n",
    "        \n",
    "        # Linear Layers\n",
    "        first_lin_layer = nn.Linear(self.no_of_embs + self.no_of_cont, lin_layer_sizes[0])\n",
    "        self.lin_layers = nn.ModuleList(\n",
    "            [first_lin_layer] + [nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1]) for i in range(len(lin_layer_sizes) - 1)]\n",
    "        )\n",
    "        \n",
    "        for lin_layer in self.lin_layers:\n",
    "            nn.init.kaiming_normal_(lin_layer.weight.data)\n",
    "            \n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(lin_layer_sizes[-1], output_size)\n",
    "        nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
    "            \n",
    "        # Batch Norm Layers\n",
    "        self.first_bn_layer = nn.BatchNorm1d(self.no_of_cont)\n",
    "        self.bn_layers = nn.ModuleList([nn.BatchNorm1d(size) for size in lin_layer_sizes])\n",
    "        \n",
    "        # Dropout Layers\n",
    "        self.emb_dropout_layer = nn.Dropout(emb_dropout)\n",
    "        self.droput_layers = nn.ModuleList(\n",
    "            [nn.Dropout(size) for size in lin_layer_dropouts]\n",
    "        )\n",
    "        \n",
    "    def forward(self, cont_data, cat_data):\n",
    "        \n",
    "        x = [emb_layer(cat_data[:, i]) for i, emb_layer in enumerate(self.emb_layers)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_dropout_layer(x)\n",
    "        \n",
    "        normalized_cont_data = self.first_bn_layer(cont_data)\n",
    "        x = torch.cat([x, normalized_cont_data], 1)\n",
    "        \n",
    "        for lin_layer, dropout_layer, bn_layer in zip(\n",
    "            self.lin_layers, self.droput_layers, self.bn_layers\n",
    "        ):\n",
    "            x = F.relu(lin_layer(x))\n",
    "            x = bn_layer(x)\n",
    "            x = dropout_layer(x)\n",
    "\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "no_of_cont = len([c for c in list(train_cols) if c not in cat_feats])\n",
    "if not 'lin_layer_sizes' in locals():\n",
    "    lin_layer_sizes = [200, 400]\n",
    "model = FeedForwardNN(emb_dims, no_of_cont=no_of_cont, lin_layer_sizes=lin_layer_sizes, output_size=1, emb_dropout=0.04, lin_layer_dropouts=[0.001,0.01]).to(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "\n",
    "tweedie_variance_power = 1.5\n",
    "rho = tensor(tweedie_variance_power).cuda()\n",
    "eps = tensor(1e-10).cuda()\n",
    "\n",
    "def tweedie_loss(pred, targ):\n",
    "    pred, targ = pred.contiguous().view(-1), targ.contiguous().view(-1)\n",
    "    pred = torch.where(pred < eps, eps, pred)\n",
    "    a = targ * torch.exp((1 - rho) * torch.log(pred)) / (1 - rho)\n",
    "    b = torch.exp((2 - rho) * torch.log(pred)) / (2 - rho)\n",
    "    return torch.mean(-a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "    \n",
    "class YearMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, yhat, y, year_factor):\n",
    "        mse = torch.mean((yhat - y)**2 * year_factor)\n",
    "        return mse\n",
    "    \n",
    "class RMSSELoss(nn.Module):\n",
    "    def __init__(self, scale_df):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.scale_df = scale_df\n",
    "        \n",
    "    def forward(self, yhat, y, id):\n",
    "        score = torch.mean((yhat - y)**2)\n",
    "        scale = torch.mean(torch.tensor([scale_map[i] for i in id]).cuda())\n",
    "        return torch.sqrt(score / scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'lr' in locals():\n",
    "    lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = RMSSELoss(scale_df)\n",
    "rmse = RMSELoss()\n",
    "# criterion = tweedie_loss\n",
    "if not 'epochs' in locals():\n",
    "    epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_dev(cat_x, cont_x, y):\n",
    "    cat_x = cat_x.long().to(device)\n",
    "    cont_x = cont_x.to(device)\n",
    "    y  = y.to(device)\n",
    "    return cat_x, cont_x, y\n",
    "\n",
    "def mov_avg(x, x_prev, a=0.9):\n",
    "    return x_prev * a + x * (1 - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit(mlflow):\n",
    "    best_rmse_metric = 10000.0\n",
    "    best_model = None\n",
    "    steps = len(train_dl) * 3\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n",
    "    for epoch in tqdm(range(epochs), total=epochs):\n",
    "        model.train()\n",
    "        prev_loss_avg = 0.0\n",
    "        for i, (y, cont_x, cat_x, id) in tqdm(enumerate(train_dl), total=len(train_dl)):\n",
    "\n",
    "            cat_x, cont_x, y = move_to_dev(cat_x, cont_x, y)\n",
    "\n",
    "            # Forward Pass\n",
    "            preds = model(cont_x, cat_x)\n",
    "            loss = criterion.forward(preds, y, id)\n",
    "            prev_loss_avg = mov_avg(loss, prev_loss_avg)\n",
    "            if i % 100 == 0:\n",
    "                print(f'{i}/{len(train_dl)} lr: {scheduler.get_last_lr()} Loss avg: {prev_loss_avg}\\r', end='')\n",
    "\n",
    "            # Backward Pass and Optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        if epoch > 0 and epoch % 3 == 0:\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n",
    "\n",
    "        ## Validation\n",
    "        model.eval()\n",
    "        nv = len(valid_dl)\n",
    "        with torch.no_grad():\n",
    "            tot_loss = 0.\n",
    "            for y, cont_x, cat_x, id in valid_dl:\n",
    "                cat_x, cont_x, y = move_to_dev(cat_x, cont_x, y)\n",
    "                pred = model(cont_x, cat_x)\n",
    "                pred = clamp_pred(pred)\n",
    "                tot_loss += rmse(pred, y)\n",
    "\n",
    "            rmse_metric = tot_loss/nv\n",
    "            if rmse_metric < best_rmse_metric:\n",
    "                best_rmse_metric = rmse_metric\n",
    "                best_model = copy.deepcopy(model)\n",
    "                print('Replaced best model')\n",
    "                mlflow.log_metric(key=\"Best Validation RMSE\", value=round(best_rmse_metric.item(), 5))\n",
    "            mlflow.log_metric(key=\"Validation RMSE\", value=round(rmse_metric.item(), 5))\n",
    "            print(f'epoch: {epoch + 1} loss: {rmse_metric.item()}')\n",
    "            \n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pred(model_pred):\n",
    "    max_lags = h * 2 + 1\n",
    "    sub = 0.\n",
    "    cols = [f\"F{i}\" for i in range(1,29)]\n",
    "    te = m5_common.create_dt(cal, prices, False, first_day=FIRST_DAY, path=path)\n",
    "    replace_cats(te)\n",
    "    model_pred.eval()\n",
    "\n",
    "    for tdelta in tqdm(range(0, h), total=h):\n",
    "        day = fday + timedelta(days=tdelta)\n",
    "        print(tdelta, day)\n",
    "        tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()\n",
    "        create_fea(tst, False)\n",
    "        remove_features(tst, selected_features + ['id'])\n",
    "        tst = tst.loc[tst.date == day, list(train_cols) + ['id']]\n",
    "        # Prepare data loader and predict\n",
    "        test_ds = TabularDataset(tst, cat_cols=cat_feats, output_col=None)\n",
    "        test_dl = DataLoader(test_ds, len(tst), shuffle=False, num_workers=1)\n",
    "        y, cont_x, cat_x, id = next(iter(test_dl))\n",
    "        cat_x, cont_x, y = move_to_dev(cat_x, cont_x, y)\n",
    "        preds = model_pred(cont_x, cat_x)\n",
    "        preds = clamp_pred(preds)\n",
    "        te.loc[te.date == day, \"sales\"] = preds.squeeze().detach().cpu().numpy()\n",
    "\n",
    "    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n",
    "    te_sub[\"id\"] = te_sub[\"id\"].str.replace(\"evaluation$\", \"validation\")\n",
    "    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
    "    te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n",
    "    te_sub.fillna(0., inplace = True)\n",
    "    te_sub.sort_values(\"id\", inplace = True)\n",
    "    te_sub.reset_index(drop=True, inplace = True)\n",
    "    sub = te_sub\n",
    "\n",
    "    sub2 = sub.copy()\n",
    "    sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "    sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
    "    sub.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluation metric\n",
    "## from https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834 and edited to get scores at all levels\n",
    "class WRMSSEEvaluator(object):\n",
    "\n",
    "    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n",
    "        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n",
    "        train_target_columns = train_y.columns.tolist()\n",
    "        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n",
    "\n",
    "        train_df['all_id'] = 0  # for lv1 aggregation\n",
    "\n",
    "        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n",
    "        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n",
    "\n",
    "        if not all([c in valid_df.columns for c in id_columns]):\n",
    "            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.calendar = calendar\n",
    "        self.prices = prices\n",
    "\n",
    "        self.weight_columns = weight_columns\n",
    "        self.id_columns = id_columns\n",
    "        self.valid_target_columns = valid_target_columns\n",
    "\n",
    "        weight_df = self.get_weight_df()\n",
    "\n",
    "        self.group_ids = (\n",
    "            'all_id',\n",
    "            'cat_id',\n",
    "            'state_id',\n",
    "            'dept_id',\n",
    "            'store_id',\n",
    "            'item_id',\n",
    "            ['state_id', 'cat_id'],\n",
    "            ['state_id', 'dept_id'],\n",
    "            ['store_id', 'cat_id'],\n",
    "            ['store_id', 'dept_id'],\n",
    "            ['item_id', 'state_id'],\n",
    "            ['item_id', 'store_id']\n",
    "        )\n",
    "\n",
    "        for i, group_id in enumerate(tqdm(self.group_ids)):\n",
    "            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n",
    "            scale = []\n",
    "            for _, row in train_y.iterrows():\n",
    "                series = row.values[np.argmax(row.values != 0):]\n",
    "                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n",
    "            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n",
    "            setattr(self, f'lv{i + 1}_train_df', train_y)\n",
    "            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n",
    "\n",
    "            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n",
    "            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n",
    "\n",
    "    def get_weight_df(self) -> pd.DataFrame:\n",
    "        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n",
    "        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n",
    "        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n",
    "        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n",
    "\n",
    "        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n",
    "        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n",
    "        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n",
    "        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n",
    "        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n",
    "        return weight_df\n",
    "\n",
    "    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n",
    "        valid_y = getattr(self, f'lv{lv}_valid_df')\n",
    "        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n",
    "        scale = getattr(self, f'lv{lv}_scale')\n",
    "        return (score / scale).map(np.sqrt)\n",
    "\n",
    "    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n",
    "        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n",
    "\n",
    "        if isinstance(valid_preds, np.ndarray):\n",
    "            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n",
    "\n",
    "        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n",
    "\n",
    "        group_ids = []\n",
    "        all_scores = []\n",
    "        for i, group_id in enumerate(self.group_ids):\n",
    "            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n",
    "            weight = getattr(self, f'lv{i + 1}_weight')\n",
    "            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n",
    "            group_ids.append(group_id)\n",
    "            all_scores.append(lv_scores.sum())\n",
    "\n",
    "        return group_ids, all_scores\n",
    "\n",
    "## public LB rank\n",
    "def get_lb_rank(df_lb, score):\n",
    "    \"\"\"\n",
    "    Get rank on public LB as of 2020-05-31 23:59:59\n",
    "    \"\"\"\n",
    "    return (df_lb.Score <= score).sum() + 1\n",
    "\n",
    "def validation_ranking(mlflow):\n",
    "    ## new train data\n",
    "    df_train_full = pd.read_csv(path/\"sales_train_evaluation.csv\")\n",
    "    df_train_full.iloc[:, -31:].head()\n",
    "    \n",
    "    df_lb = pd.read_csv(path/\"m5-forecasting-accuracy-publicleaderboard-rank.csv\")\n",
    "    \n",
    "    ## reading data\n",
    "    df_calendar = pd.read_csv(path/\"calendar.csv\")\n",
    "    df_prices = pd.read_csv(path/\"sell_prices.csv\")\n",
    "    df_sample_submission = pd.read_csv(path/\"sample_submission.csv\")\n",
    "    df_sample_submission[\"order\"] = range(df_sample_submission.shape[0])\n",
    "\n",
    "    df_train = df_train_full.iloc[:, :-28]\n",
    "    df_valid = df_train_full.iloc[:, -28:]\n",
    "\n",
    "    evaluator = WRMSSEEvaluator(df_train, df_valid, df_calendar, df_prices)\n",
    "    \n",
    "    ## structure of validation data\n",
    "    preds_valid = df_valid.copy() + np.random.randint(100, size = df_valid.shape)\n",
    "\n",
    "    ## evaluating submission from public kernel M5 - Three shades of Dark: Darker magic\n",
    "    preds_valid = pd.read_csv(\"submission.csv\")\n",
    "    preds_valid = preds_valid[preds_valid.id.str.contains(\"validation\")]\n",
    "    preds_valid = preds_valid.merge(df_sample_submission[[\"id\", \"order\"]], on = \"id\").sort_values(\"order\").drop([\"id\", \"order\"], axis = 1).reset_index(drop = True)\n",
    "    preds_valid.rename(columns = {\n",
    "        \"F1\": \"d_1914\", \"F2\": \"d_1915\", \"F3\": \"d_1916\", \"F4\": \"d_1917\", \"F5\": \"d_1918\", \"F6\": \"d_1919\", \"F7\": \"d_1920\",\n",
    "        \"F8\": \"d_1921\", \"F9\": \"d_1922\", \"F10\": \"d_1923\", \"F11\": \"d_1924\", \"F12\": \"d_1925\", \"F13\": \"d_1926\", \"F14\": \"d_1927\",\n",
    "        \"F15\": \"d_1928\", \"F16\": \"d_1929\", \"F17\": \"d_1930\", \"F18\": \"d_1931\", \"F19\": \"d_1932\", \"F20\": \"d_1933\", \"F21\": \"d_1934\",\n",
    "        \"F22\": \"d_1935\", \"F23\": \"d_1936\", \"F24\": \"d_1937\", \"F25\": \"d_1938\", \"F26\": \"d_1939\", \"F27\": \"d_1940\", \"F28\": \"d_1941\"\n",
    "    }, inplace = True)\n",
    "\n",
    "    groups, scores = evaluator.score(preds_valid)\n",
    "\n",
    "    score_public_lb = np.mean(scores)\n",
    "    score_public_rank = get_lb_rank(df_lb, score_public_lb)\n",
    "\n",
    "    for i in range(len(groups)):\n",
    "        print(f\"Score for group {groups[i]}: {round(scores[i], 5)}\")\n",
    "\n",
    "    print(f\"\\nPublic LB Score: {round(score_public_lb, 5)}\")\n",
    "    if mlflow is not None:\n",
    "        mlflow.log_metric(key=\"Public LB Score\", value=round(score_public_lb, 5))\n",
    "        mlflow.set_tag(key=\"Public LB Rank\", value=str(score_public_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed218152b5242ab9c287666b7a35909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529f280b1668407091e59e6812da0791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.000764828423991816] Loss avg: 0.79602682590484627\n",
      "Replaced best model\n",
      "epoch: 1 loss: 2.231393575668335\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1af6b765814e979a7941ce3597baf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.0002651276353496774] Loss avg: 0.79075533151626599\n",
      "Replaced best model\n",
      "epoch: 2 loss: 2.1929075717926025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9cbe377fe048eab8f18f9fbfe62e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [2.9921135786037704e-07] Loss avg: 0.7621117830276489\n",
      "Replaced best model\n",
      "epoch: 3 loss: 2.1749000549316406\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b65e93c6b814683a4ac1a8db515a550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.0002351715760081836] Loss avg: 0.74683398008346563\n",
      "epoch: 4 loss: 2.1803605556488037\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f340a8438b743c7bc3418ffacfeea65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.000764828423991816] Loss avg: 0.77162778377532965\n",
      "epoch: 5 loss: 2.183570384979248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf280833e07f4744ac39da92b0a878da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.0002651276353496774] Loss avg: 0.74884724617004419\n",
      "Replaced best model\n",
      "epoch: 6 loss: 2.1671011447906494\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7592b4d6c84c8991ac6ac718b0237e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [2.9921135786037704e-07] Loss avg: 0.7488617897033691\n",
      "Replaced best model\n",
      "epoch: 7 loss: 2.1566545963287354\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bb7fd5947f459f8f027f375942f9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.000764828423991816] Loss avg: 0.82818824052810679\n",
      "epoch: 8 loss: 2.188462018966675\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ba2dbd00fd4a1f8912056422408b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.0002651276353496774] Loss avg: 0.79201644659042366\n",
      "Replaced best model\n",
      "epoch: 9 loss: 2.155851125717163\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c58848eb51b44a2b3129e5f6f1d1dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [2.9921135786037704e-07] Loss avg: 0.7615416646003723\n",
      "Replaced best model\n",
      "epoch: 10 loss: 2.1463732719421387\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d186d688da3148918d3a509948d3d009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.000764828423991816] Loss avg: 0.77099353075027474\n",
      "epoch: 11 loss: 2.1654090881347656\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43ce5f966f94eba9fcddbce561e2d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.0002651276353496774] Loss avg: 0.76600557565689096\n",
      "Replaced best model\n",
      "epoch: 12 loss: 2.1427719593048096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5e6f7246d6444ab3e36e268f806eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [2.9921135786037704e-07] Loss avg: 0.7330094575881958\n",
      "epoch: 13 loss: 2.1436216831207275\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df83021863f6418395f68dfb48384186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.000764828423991816] Loss avg: 0.78790503740310677\n",
      "epoch: 14 loss: 2.1563589572906494\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12794e1971a4388bb577e930aa5d0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.0002651276353496774] Loss avg: 0.74789118766784674\n",
      "Replaced best model\n",
      "epoch: 15 loss: 2.1395976543426514\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bf3b8aee754c8c8acccbfa1eac23a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [2.9921135786037704e-07] Loss avg: 0.7361591458320618\n",
      "Replaced best model\n",
      "epoch: 16 loss: 2.13457989692688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff3152415db4aab969c7073d4f88931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.000764828423991816] Loss avg: 0.76432979106903088\n",
      "epoch: 17 loss: 2.152217388153076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b3df41d4764272a9b5c4b39c97333b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.0002651276353496774] Loss avg: 0.80180776119232184\n",
      "Replaced best model\n",
      "epoch: 18 loss: 2.1301565170288086\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4211fa7044947f1b97fad2990774419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [2.9921135786037704e-07] Loss avg: 0.7382914423942566\n",
      "epoch: 19 loss: 2.134352684020996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf7ef2a8097480d96441f05eaf6ead7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.000764828423991816] Loss avg: 0.77669674158096318\n",
      "epoch: 20 loss: 2.140892267227173\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4458e792b74b2cb1dd8a66cda760bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.0002651276353496774] Loss avg: 0.74280524253845216\n",
      "epoch: 21 loss: 2.1304547786712646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376f093b54bb48758ff14f86ec1dbc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [2.9921135786037704e-07] Loss avg: 0.7357199192047119\n",
      "Replaced best model\n",
      "epoch: 22 loss: 2.126316785812378\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0cfef25c674049a4f3518495b070a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.000764828423991816] Loss avg: 0.81928914785385132\n",
      "epoch: 23 loss: 2.1489808559417725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81bec944af64c73a5f1a55dbeb5e255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.0002651276353496774] Loss avg: 0.74130159616470349\n",
      "epoch: 24 loss: 2.146413564682007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d161e47aed904818bf9fb49481121b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [2.9921135786037704e-07] Loss avg: 0.7436394095420837\n",
      "epoch: 25 loss: 2.1301565170288086\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eda7576716645e3b1c9ff351a29d2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.000764828423991816] Loss avg: 0.75420033931732188\n",
      "epoch: 26 loss: 2.1532814502716064\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb4aa020dd648c6a193db8ddcd086f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.0002651276353496774] Loss avg: 0.73096036911010747\n",
      "epoch: 27 loss: 2.133375406265259\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd42e4dad0e46dc9774082387a8c6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [2.9921135786037704e-07] Loss avg: 0.7311385869979858\n",
      "epoch: 28 loss: 2.1386799812316895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e4e4112ad24ab191729574fb5ef3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.000764828423991816] Loss avg: 0.74473333358764659\n",
      "epoch: 29 loss: 2.1331446170806885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a6f96656894075b3650633869bc08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2482 lr: [0.0002651276353496774] Loss avg: 0.72960746288299566\n",
      "epoch: 30 loss: 2.1314404010772705\n",
      "\n",
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6}\n",
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11}\n",
      "{2011: 0, 2012: 1, 2013: 2, 2014: 3, 2015: 4, 2016: 5}\n",
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7217d24de8a145649d072779de3bcdf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2016-04-25 00:00:00\n",
      "1 2016-04-26 00:00:00\n",
      "2 2016-04-27 00:00:00\n",
      "3 2016-04-28 00:00:00\n",
      "4 2016-04-29 00:00:00\n",
      "5 2016-04-30 00:00:00\n",
      "6 2016-05-01 00:00:00\n",
      "7 2016-05-02 00:00:00\n",
      "8 2016-05-03 00:00:00\n",
      "9 2016-05-04 00:00:00\n",
      "10 2016-05-05 00:00:00\n",
      "11 2016-05-06 00:00:00\n",
      "12 2016-05-07 00:00:00\n",
      "13 2016-05-08 00:00:00\n",
      "14 2016-05-09 00:00:00\n",
      "15 2016-05-10 00:00:00\n",
      "16 2016-05-11 00:00:00\n",
      "17 2016-05-12 00:00:00\n",
      "18 2016-05-13 00:00:00\n",
      "19 2016-05-14 00:00:00\n",
      "20 2016-05-15 00:00:00\n",
      "21 2016-05-16 00:00:00\n",
      "22 2016-05-17 00:00:00\n",
      "23 2016-05-18 00:00:00\n",
      "24 2016-05-19 00:00:00\n",
      "25 2016-05-20 00:00:00\n",
      "26 2016-05-21 00:00:00\n",
      "27 2016-05-22 00:00:00\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd875713f6944d68d67e927326fd3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score for group all_id: 0.36679\n",
      "Score for group cat_id: 0.38926\n",
      "Score for group state_id: 0.43148\n",
      "Score for group dept_id: 0.47518\n",
      "Score for group store_id: 0.51739\n",
      "Score for group item_id: 0.82317\n",
      "Score for group ['state_id', 'cat_id']: 0.47402\n",
      "Score for group ['state_id', 'dept_id']: 0.55145\n",
      "Score for group ['store_id', 'cat_id']: 0.56319\n",
      "Score for group ['store_id', 'dept_id']: 0.64447\n",
      "Score for group ['item_id', 'state_id']: 0.83077\n",
      "Score for group ['item_id', 'store_id']: 0.83058\n",
      "\n",
      "Public LB Score: 0.57481\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    mlflow.log_params({'epochs': epochs, 'lr': lr, 'lin_layer_sizes': str(lin_layer_sizes)})\n",
    "    if not 'num_cycles' in locals():\n",
    "        num_cycles = 1\n",
    "    for i in range(num_cycles):\n",
    "        model_pred = fit(mlflow)\n",
    "    run_pred(model_pred)\n",
    "    validation_ranking(mlflow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
