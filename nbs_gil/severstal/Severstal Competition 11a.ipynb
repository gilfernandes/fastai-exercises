{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from fastai.vision import *\n",
    "from fastai import *\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = pd.read_csv('/root/.fastai/data/severstal/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId_ClassId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002cc93b.jpg_1</td>\n",
       "      <td>29102 12 29346 24 29602 24 29858 24 30114 24 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002cc93b.jpg_2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002cc93b.jpg_3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002cc93b.jpg_4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00031f466.jpg_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId_ClassId                                      EncodedPixels\n",
       "0  0002cc93b.jpg_1  29102 12 29346 24 29602 24 29858 24 30114 24 3...\n",
       "1  0002cc93b.jpg_2                                                NaN\n",
       "2  0002cc93b.jpg_3                                                NaN\n",
       "3  0002cc93b.jpg_4                                                NaN\n",
       "4  00031f466.jpg_1                                                NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/root/.fastai/data/severstal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/root/.fastai/data/severstal/severstal-steel-defect-detection.zip'),\n",
       " PosixPath('/root/.fastai/data/severstal/test_images'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images'),\n",
       " PosixPath('/root/.fastai/data/severstal/train.csv'),\n",
       " PosixPath('/root/.fastai/data/severstal/submission.csv'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images.zip'),\n",
       " PosixPath('/root/.fastai/data/severstal/sample_submission.csv'),\n",
       " PosixPath('/root/.fastai/data/severstal/test_images.zip')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/root/.fastai/data/severstal/train_images/18ba753ff.jpg'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images/0519989b3.jpg'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images/7933cbe21.jpg')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = get_image_files(path/'train_images')\n",
    "train_images[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check maximum size of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_img_max_size(folder):\n",
    "    max_height = 0\n",
    "    max_width = 0\n",
    "    for train_image in train_images:\n",
    "        img = open_image(train_image)\n",
    "        if max_height < img.shape[1]:\n",
    "            max_height = img.shape[1]\n",
    "        if max_width < img.shape[2]:\n",
    "            max_width = img.shape[2]\n",
    "    return max_height, max_width\n",
    "\n",
    "def show_image(images, index):\n",
    "    img_f = images[index]\n",
    "    print(type(img_f))\n",
    "    img = open_image(img_f)\n",
    "    print(img)\n",
    "    img.show(figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_path = Path('/kaggle/mask')\n",
    "if not os.path.exists(mask_path):\n",
    "    os.makedirs(str(mask_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_encoded_to_array(encoded_pixels):\n",
    "    pos_array = []\n",
    "    len_array = []\n",
    "    splits = encoded_pixels.split()\n",
    "    pos_array = [int(n) - 1 for i, n in enumerate(splits) if i % 2 == 0]\n",
    "    len_array = [int(n) for i, n in enumerate(splits) if i % 2 == 1]\n",
    "    return pos_array, len_array\n",
    "        \n",
    "def convert_to_pair(pos_array, rows):\n",
    "    return [(p % rows, p // rows) for p in pos_array]\n",
    "\n",
    "def create_positions(single_pos, size):\n",
    "    return [i for i in range(single_pos, single_pos + size)]\n",
    "\n",
    "def create_positions_pairs(single_pos, size, row_size):\n",
    "    return convert_to_pair(create_positions(single_pos, size), row_size)\n",
    "\n",
    "def convert_to_mask(encoded_pixels, row_size, col_size, category):\n",
    "    pos_array, len_array = convert_encoded_to_array(encoded_pixels)\n",
    "    mask = np.zeros([row_size, col_size])\n",
    "    for(p, l) in zip(pos_array, len_array):\n",
    "        for row, col in create_positions_pairs(p, l, row_size):\n",
    "            mask[row][col] = category\n",
    "    return mask\n",
    "\n",
    "def save_to_image(masked, image_name):\n",
    "    im = PIL.Image.fromarray(masked)\n",
    "    im = im.convert(\"L\")\n",
    "    image_name = re.sub(r'(.+)\\.jpg', r'\\1', image_name) + \".png\"\n",
    "    real_path = mask_path/image_name\n",
    "    im.save(real_path)\n",
    "    return real_path\n",
    "\n",
    "def open_single_image(path):\n",
    "    img = open_image(path)\n",
    "    img.show(figsize=(20,20))\n",
    "    \n",
    "def get_y_fn(x):\n",
    "    return mask_path/(x.stem + '.png')\n",
    "\n",
    "def group_by(train_images, train_pd):\n",
    "    tran_dict = {image.name:[] for image in train_images}\n",
    "    pattern = re.compile('(.+)_(\\d+)')\n",
    "    for index, image_path in train_pd.iterrows():\n",
    "        m = pattern.match(image_path['ImageId_ClassId'])\n",
    "        file_name = m.group(1)\n",
    "        category = m.group(2)\n",
    "        tran_dict[file_name].append((int(category), image_path['EncodedPixels']))\n",
    "    return tran_dict\n",
    "\n",
    "def display_image_with_mask(img_name):\n",
    "    full_image = path/'train_images'/img_name\n",
    "    print(full_image)\n",
    "    open_single_image(full_image)\n",
    "    mask_image = get_y_fn(full_image)\n",
    "    mask = open_mask(mask_image)\n",
    "    print(full_image)\n",
    "    mask.show(figsize=(20, 20), alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_categories_mask = group_by(train_images, train_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create mask files and save these to kaggle/mask/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 256\n",
    "image_width = 1600\n",
    "for image_name, cat_list in grouped_categories_mask.items():\n",
    "    masked = np.zeros([image_height, image_width])\n",
    "    for cat_mask in cat_list:\n",
    "        encoded_pixels = cat_mask[1]\n",
    "        if pd.notna(cat_mask[1]):\n",
    "            masked += convert_to_mask(encoded_pixels, image_height, image_width, cat_mask[0])\n",
    "    if np.amax(masked) > 4:\n",
    "        print(f'Check {image_name} for max category {np.amax(masked)}')\n",
    "    save_to_image(masked, image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limited_dihedral_affine(k:partial(uniform_int,0,3)):\n",
    "    \"Randomly flip `x` image based on `k`.\"\n",
    "    x = -1 if k&1 else 1\n",
    "    y = -1 if k&2 else 1\n",
    "    if k&4: return [[0, x, 0.],\n",
    "                    [y, 0, 0],\n",
    "                    [0, 0, 1.]]\n",
    "    return [[x, 0, 0.],\n",
    "            [0, y, 0],\n",
    "            [0, 0, 1.]]\n",
    "\n",
    "dihedral_affine = TfmAffine(limited_dihedral_affine)\n",
    "\n",
    "def get_extra_transforms(max_rotate:float=3., max_zoom:float=1.1,\n",
    "                   max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75,\n",
    "                   p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None)->Collection[Transform]:\n",
    "    \"Utility func to easily create a list of flip, rotate, `zoom`, warp, lighting transforms.\"\n",
    "    p_lightings = [p_lighting, p_lighting + 0.2, p_lighting + 0.4, p_lighting + 0.6, p_lighting + 0.7]\n",
    "    max_lightings = [max_lighting, max_lighting + 0.2, max_lighting + 0.4, max_lighting + 0.6, max_lighting + 0.7]\n",
    "    res = [rand_crop(), dihedral_affine(), \n",
    "           symmetric_warp(magnitude=(-max_warp,max_warp), p=p_affine),\n",
    "           rotate(degrees=(-max_rotate,max_rotate), p=p_affine),\n",
    "           rand_zoom(scale=(1., max_zoom), p=p_affine)]\n",
    "    res.extend([brightness(change=(0.5*(1-mp[0]), 0.5*(1+mp[0])), p=mp[1]) for mp in zip(max_lightings, p_lightings)])\n",
    "    res.extend([contrast(scale=(1-mp[0], 1/(1-mp[0])), p=mp[1]) for mp in zip(max_lightings, p_lightings)])\n",
    "    #       train                   , valid\n",
    "    return (res, [crop_pad()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = (path/'train_images').ls()\n",
    "src_size = np.array(open_image(str(train_images[0])).shape[1:])\n",
    "valid_pct = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = array(['0', '1', '2', '3', '4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = (SegmentationItemList.from_folder(path/'train_images')\n",
    "       .split_by_rand_pct(valid_pct=valid_pct)\n",
    "       .label_from_func(get_y_fn, classes=codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "size = src_size//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (src.transform(get_extra_transforms(), size=size, tfm_y=True)\n",
    "        .add_test(ImageList.from_folder(path/'test_images'), tfms=None, tfm_y=False)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create learner and training\n",
    "Starting with low resolution training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "name2id = {v:k for k,v in enumerate(codes)}\n",
    "void_code = name2id['0']\n",
    "\n",
    "def acc_camvid(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    mask = target != void_code\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    comparison = argmax[mask]==target[mask]\n",
    "    return torch.tensor(0.) if comparison.numel() == 0 else comparison.float().mean()\n",
    "\n",
    "def acc_camvid_with_zero_check(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        if(torch.sum(argmax[b]).item() == 0.0 and torch.sum(target[b]).item() == 0.0):\n",
    "            total[b] = 1\n",
    "        else:\n",
    "            mask = target[b] != void_code\n",
    "            comparison = argmax[b][mask]==target[b][mask]\n",
    "            total[b] = torch.tensor(0.) if comparison.numel() == 0 else comparison.float().mean()\n",
    "    return total.mean()\n",
    "\n",
    "\n",
    "def calc_dice_coefficients(argmax, target, cats):\n",
    "    def calc_dice_coefficient(seg, gt, cat: int):\n",
    "        mask_seg = seg == cat\n",
    "        mask_gt = gt == cat\n",
    "        sum_seg = torch.sum(mask_seg.float())\n",
    "        sum_gt = torch.sum(mask_gt.float())\n",
    "        if sum_seg + sum_gt == 0:\n",
    "            return torch.tensor(1.0)\n",
    "        return (torch.sum((seg[gt == cat] / cat).float()) * 2.0) / (sum_seg + sum_gt)\n",
    "\n",
    "    total_avg = torch.empty([len(cats)])\n",
    "    for i, c in enumerate(cats):\n",
    "        total_avg[i] = calc_dice_coefficient(argmax, target, c)\n",
    "    return total_avg.mean()\n",
    "\n",
    "\n",
    "def dice_coefficient(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    cats = [1, 2, 3, 4]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        total[b] = calc_dice_coefficients(argmax[b], target[b], cats)\n",
    "    return total.mean()\n",
    "\n",
    "def calc_dice_coefficients_2(argmax, target, cats):\n",
    "    def calc_dice_coefficient(seg, gt, cat: int):\n",
    "        mask_seg = seg == cat\n",
    "        mask_gt = gt == cat\n",
    "        sum_seg = torch.sum(mask_seg.float())\n",
    "        sum_gt = torch.sum(mask_gt.float())\n",
    "        return (torch.sum((seg[gt == cat] / cat).float())), (sum_seg + sum_gt)\n",
    "\n",
    "    total_avg = torch.empty([len(cats), 2])\n",
    "    for i, c in enumerate(cats):\n",
    "        total_avg[i][0], total_avg[i][1] = calc_dice_coefficient(argmax, target, c)\n",
    "    total_sum = total_avg.sum(axis=0)\n",
    "    if (total_sum[1] == 0.0):\n",
    "        return torch.tensor(1.0)\n",
    "    return total_sum[0] * 2.0 / total_sum[1]\n",
    "\n",
    "\n",
    "def dice_coefficient_2(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    cats = [1, 2, 3, 4]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        total[b] = calc_dice_coefficients_2(argmax[b], target[b], cats)\n",
    "    return total.mean()\n",
    "\n",
    "\n",
    "def accuracy_simple(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    return (input.argmax(dim=1)==target).float().mean()\n",
    "\n",
    "\n",
    "def dice_coeff(pred, target):\n",
    "    smooth = 1.\n",
    "    num = pred.size(0)\n",
    "    m1 = pred.view(num, -1)  # Flatten\n",
    "    m2 = target.view(num, -1)  # Flatten\n",
    "    intersection = (m1 * m2).sum()\n",
    "    return (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedCrossEntropyFlat():\n",
    "    \"Same as `func`, but flattens input and target.\"\n",
    "    def __init__(self):\n",
    "        self.func = nn.CrossEntropyLoss(weight=torch.tensor([2.0, .5, .5, .5, .5])).cuda()\n",
    "\n",
    "    def __repr__(self): return f\"SimpleCrossEntropyFlat\"\n",
    "    \n",
    "    def dice_loss(self, true, logits, eps=1e-4):\n",
    "        \"\"\"Computes the Sørensen–Dice loss.\n",
    "        Note that PyTorch optimizers minimize a loss. In this\n",
    "        case, we would like to maximize the dice loss so we\n",
    "        return the negated dice loss.\n",
    "        Args:\n",
    "            true: a tensor of shape [B, 1, H, W].\n",
    "            logits: a tensor of shape [B, C, H, W]. Corresponds to\n",
    "                the raw output or logits of the model.\n",
    "            eps: added to the denominator for numerical stability.\n",
    "        Returns:\n",
    "            dice_loss: the Sørensen–Dice loss.\n",
    "        \"\"\"\n",
    "        num_classes = logits.shape[1]\n",
    "        true_1_hot = torch.eye(num_classes)[true.squeeze(1)]\n",
    "        true_1_hot = true_1_hot.permute(0, 3, 2, 1).float().transpose(dim0=2, dim1=3)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        probas_one_hot = self.convert_max_to_one_hot(probas)\n",
    "        true_1_hot = true_1_hot.type(logits.type())\n",
    "        intersection_count = ((probas_one_hot == true_1_hot) & (true_1_hot == 1.)).float().sum()\n",
    "        true_count = true_1_hot.sum()\n",
    "        probas_count = probas_one_hot.sum()\n",
    "        all_losses = 1 - (2. * intersection_count / (probas_count + true_count + eps))\n",
    "        all_losses.requires_grad=True\n",
    "        return all_losses\n",
    "\n",
    "    def convert_max_to_one_hot(self, tensor1):\n",
    "        tensor1_shape = tensor1.shape\n",
    "        tensor1_input_view = tensor1.view(tensor1_shape[0], tensor1_shape[1], -1)\n",
    "        return (tensor1_input_view == tensor1_input_view.max(dim = 1, keepdim=True)[0]).float().view_as(tensor1)\n",
    "    \n",
    "    \n",
    "    def __call__(self, input:Tensor, target:Tensor)->Rank0Tensor:\n",
    "        input_changed = input.transpose(1,-1).contiguous()\n",
    "        target_changed = target.transpose(1,-1).contiguous()\n",
    "        input_changed = input_changed.view(-1, input_changed.shape[-1])\n",
    "        cel = self.func.__call__(input_changed, target_changed.view(-1))\n",
    "        dice_loss = self.dice_loss(target, input)\n",
    "#         print('cross entropy loss', cel, 'dice_loss', dice_loss)\n",
    "        return dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import callbacks\n",
    "\n",
    "def train_learner(learn, slice_lr, epochs=10, pct_start=0.8, best_model_name='best_model', \n",
    "                  patience_early_stop=4, patience_reduce_lr = 3):\n",
    "    learn.fit_one_cycle(epochs, slice_lr, pct_start=pct_start, \n",
    "                    callbacks=[callbacks.SaveModelCallback(learn, monitor='dice_coefficient',mode='max', name=best_model_name),\n",
    "                              callbacks.EarlyStoppingCallback(learn=learn, monitor='dice_coefficient', patience=patience_early_stop),\n",
    "                              callbacks.ReduceLROnPlateauCallback(learn=learn, monitor='dice_coefficient', patience=patience_reduce_lr),\n",
    "                              callbacks.TerminateOnNaNCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=accuracy_simple, acc_camvid_with_zero_check, dice_coefficient, dice_coefficient_2\n",
    "wd=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCrossEntropyFlat"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd, bottle=True)\n",
    "# learn.loss_func = CrossEntropyFlat(axis=1, weight=torch.tensor([1.5, .5, .5, .5, .5]).cuda())\n",
    "learn.loss_func = CombinedCrossEntropyFlat()\n",
    "learn.loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model_dir = Path('/kaggle/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = to_fp16(learn, loss_scale=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEICAYAAABI7RO5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXxU1fn48c+Tyb4HEkJIAmHfZA8g7iAqWuu+gPvS2tpirdpW7WL9aW1trfqtVm2tVawLuCsqFkFFLbKFnQSBEJaEJSRAQgLZ5/n9MTc4hJCFZJgsz/v1mhcz555z77nXOM+cc+49R1QVY4wxpqkC/F0BY4wx7YsFDmOMMc1igcMYY0yzWOAwxhjTLBY4jDHGNIsFDmOMMc3i08AhIlNEZIOIZIvIffVs7yUin4nIGhFZICIpXttuFJFNzutGr/QxIrLW2edTIiK+PAdjjDFHEl89xyEiLmAjcA6QBywDpqlqlleet4CPVPVlEZkE3Kyq14tIFyADSAcUWA6MUdX9IrIUuBNYDMwBnlLVTxqqS3x8vKalpbX6ORpjTEe2fPnyQlVNqJse6MNjjgOyVTUHQERmARcDWV55hgB3Oe+/AN533p8HzFPVfU7ZecAUEVkARKvqIif9P8AlQIOBIy0tjYyMjNY4J2OM6TREZFt96b7sqkoGcr0+5zlp3lYDlzvvLwWiRKRrA2WTnfcN7dMYY4wP+TJw1Df2ULdf7BfAmSKyEjgT2AFUN1C2Kfv0HFzkNhHJEJGMgoKCptfaGGNMg3wZOPKAVK/PKcBO7wyqulNVL1PVUcBvnLTiBsrmOe+PuU+vfT+vqumqmp6QcFQXnTHGmOPky8CxDOgvIr1FJBiYCsz2ziAi8SJSW4f7gRed93OBc0UkTkTigHOBuaq6CygRkZOdu6luAD7w4TkYY4ypw2eBQ1Wrgel4gsB64E1VzRSRh0TkIifbWcAGEdkIJAKPOGX3AQ/jCT7LgIdqB8qB24EXgGxgM40MjBtjjGldPrsdty1JT09Xu6vKGGOaR0SWq2p63XR7ctwYY0yzWOAwpo1xu5WZS7ezbkexv6tiTL0scBjTxvzjq83c/+5aLnz6fzy7INsnx/hmcyF/nbuBqhq3T/ZvOjZfPjluTKdVUV3DZ+v3EOwK4LT+8YQGudiwu4RAl9A3IfKo/HtLK3htyXbG9Irj8U83csGw7gjCY3M3EBQQQK+u4dS4ldQu4QxOisYV0PQp2qpr3FTWuHl9yXYKSysJEHh2wWYASiuque/8QZRWVBMfGdJq5286NgscfrCruIzisioGJkZhczR2LJk7i3ngg0y2Fh5k78FKAAYnRXPukESeXZBNjVu5+5wBTJ/U/4hyD36YxYerdyICiVGhPHr5cIICAth7sIJH5qw/Iu+I1Fje+tEECkorWJ1bxPkndT/m35HbrVz+j0Wszi06Iv2q9BQCXQHM+GYrM77ZCsArt47j9P72zJNpnAWO41RWWcMLX+dw82m9iQxp2mUsKa/ivnfX8vGaXQD847oxTDmpuy+raRpQXeOmRpWQQFeL9zU/K59//28LWbsOEBIYwOn947l4ZDJlVTXc+84a/vbZJk7vH09IYABPfZbNVWNTiQ4NIjTIxbysfD5cvZNT+nZlVW4Rf7zsJKJDgwCYddsENuwuobyqhiBXAN9sLuQPH6/nzlkrWbZ1H4Wllbx663hO6x9fb70+XLOT1blFXJ2eysRBCXSLDiV33yEuGtEDt8KZAxLI3FHM60tzeeHrLRY4TJNY4Gim8qoaFufsZW7mbmYuzSUxOpSrxqY2XhB4aeFWPl6zi+kT+/FGRi7vrsizwHGCud3KjG+2smF3CQs27iExOpQPfnrq4V/sNW5l696D9XYnASzdso/Y8CAGJEYBnq6eJ+dt5MWFW0iNC2dEaiwPXTSUtPiIw2XOGphAeZWbuPAgcgoPMn/9l5z9+JeEBbmYPqkff/h4PcOSY3jxprG4AoQg15FDjwO7Rx1+P6RHNOt3lfDOijwGJEYS5ArgsU83cGq/rogI+QfK6RYVgohQUFLBo598y5CkaP502TACnO6t0T3jAHAJnDe0O+cN7U5AgPB/8zexpfAgvb3q3pB9BytZmF1IkEuYPDiRQJcNmXYWFjia6aGPsnh9yfbDnw+UVzWpnNutvLEsl9P6xfOL8wZysLKa1xZvp7isioKScnrEhhEebP85WmJjfgmvLt5Gn/gIpo3veVRLoqrGze2vrmD++ny6RgQTGRrImrxiVmwvol9CJO+tzOOdFTtYu6OYl28ZR1RoIL97fx0BIvTsEk5EiIu3lueREBnCvLvPBIXrX1zC2h3FTBvXkwcuHEJo0NGtl/DgQMKDPe/7JkRy7pBEFufspbLGzQMfZDIiNZYZN42tt2x9HrtiOA9fMpTw4EDeWLade99Zy/z1ewgNCuCGF5dy1+QB3HZGH370Sgb7D1Xy/PXph4PGsVwzrid/+2wTs1ft5M7J/RvMC/Dakm08ODuTqhrPc2B3TOrHPecObFL9TftnDwA2w5KcvVz9/GLSe8Wxde8hCksr+OHpvfnN94Y0WvarjQXc8OJS/n7NKC4c3oNVuUVc8sxCbjujDzMWbuWikT3446XDqKpxE9HErq/OTlVxK7gCBLdbufTZhazdUYxbYVD3KP55/Rh6dfX8ena7lfvfXcsbGbk8cOEQbj41jUOVNYx7ZD6jesaxs6iMnMKDJMeGUVXjJsgVQEFJBd2iQ+iTEEnuvkPs2F/G+D5dWJhdyNi0LuwpqWDH/jKevXY0k4ckNrne5VU11LiV7fsOsWjzXq47uRfBgcf3a72qxs05T3xJkCuAGlVyCg7iChBGpMSwYnsRz107mvOHJTVpX1c89w2HKmuYc+fpR6S73UrtEEr+gQpeWbyVZ77YzJkDErjn3AG8tHArs1fv5OOfncag7tGHyz392Sa6RYdw9diex3Vuxv+O9QCgfUM1karyyJz19IgJ5ZVbxxMaFMDEvy5g94GKJpWfm7mbiGAX5zhfMCNSYhjXuwvPf5UDwLsr8vhyYwGFpRXcdkYf7j9/sM/Opa1yu7XBX8ZbCg/SNTKY6NAgig5Vct2/lxAfGcJLN43lpW+2sjqvmCeuGkFseBB3v7maS5/9hlm3ncz6XQf4ZO1u/pu5m+kT+3HLab0BiAgJ5Mr0VGZ8s5W48CBm3XYy43t34a2MPH71zhpO7x/P09NGEes0F1QVEeGVxdt44tMNBAcG8Mqt4xjfp2uzzrO2ZTE4KZrBSdGN5G5YkCuAX5w3kOmvryTIJTw1bRTvrsgjc+cBfvu9wU0OGuDptnpkznpW5xbx2pJtCMI143vy5/9+y/Z9h4gODSJr1wHAM7j+h0uGERwYwO+/P4R5Wfn888scnrx6JADzsvJ5fN5GEqNDuCo91W4C6WCsxdFE/123ix+/uoLHrhjOlemeMY2r/7kIVXjzxxMaLKuqnPbnLzgpOZp/Xv9d8F6+bT+XP/cN3xuWxGff5hMXHky/bpEsydnHst9MJiY8qEV1bk8e+GAdy7bu572fnHJUl01ltZu73lzFx2t2EewK4I+XDeOd5XksytkLwHlDE5mbmc/EgQn8+8axBAQIWwoPcuU/vqG0opryKjdBLmH6xP787Ox+R3yJqSp7D1YSFRp4uGtLVVmZW8Tw5Jhj9ttX17gJEGm0C+hE2VVcRliQ63CQOx65+w4x8a8LqHYrwa4AQgIDKKmoBqB3fAQ1buXGU9I4qUf0UcHywdmZvLZkGwvvncTcrHz+NGc9blXKq9zM/OHJDEiMpKvd7tvuWIujhd5buYPE6BAuG/3drO7dY0JZsX1/o2U37SllR1EZ0yf1OyJ9TK843vrxBIb2iGZnURldIkLYWVTGhU//j9mrd3D9hLTWPo0TquhQJZ9/u4eLRyYf8dzB3tIK7pi5ktP6x/OTs/qxMLuQ/yzyLDT2wtc5R92qOnPpdj5es4sfn9mXxTl7+c17a6modvObCwbzr69zmJuZz3Un9+TB7w89/EXeOz6Cp6aN4rb/LOeec/rxk4n96n32QUSOen5BRA4PIB9LWxsITooJa/E+UruE8/btp/D5+nwmDU6kV5dwHpidSWpcGL+aMqjBsjedksYri7dx0d8XsvtAOWcMSODeKQP5/tP/49oXFhMbHswnd55OYnRoi+tp/M8CRxNUVNfw9aZCLhl15Bdg9+hQ8g9UHO7CqKu8qoaKajefZu4GPHfX1DU2rQsA/bp57pzpEhHM4KRoXlm8jWvG92rWg16+tim/hL4JkU36lV1eVcMPXs4gY5snsIYGuThjQAKHKqq59oUlbNpTyqKcvVRWu/n3/7aQ1jWcft0ieeaLzVw2OoUesZ4vwuKyKp76bBMT+nTl3ikDWZ1XzCXPLCQ5NowbT0ljVM9YdhWXc+HwpKP+G5zSN57Vvz+3TV3Dtm5kaiwjU2MPf3562qgmlUuLj+BfN4zhrjdWM3VsKo9cOgxXgHBqv3gydx6grLKGn81cycu3jGvyTQCm7bLA0QSLc/ZxqLKGyYO7HZGeGB1KZbWb/Yeq6BJxdBfB795fx1ebChCECX26NvlX4fSJ/fjp6yt4Y1ku14xveGCx+FAVkaGBPv9ynJu5mx+9spzbz+rLvQ38+nxp4RZmfLOV6NAg1u4oJi48iPveXUtltZuBiVGUVlSz/1Al/74xnUc+Xs//zd/EgMRIXrxpLKow+YkveWTOev4+bRSqcM+bqykuq+LXFwxGRBiZGssDFw5hYPcoggMDSHcC77FY0DhxJg1KZPlvJx/RGnv22tEAfLZ+Dz9/YxWXPLOQXl3DGZ4Sy62n9bYg0k5Z4GjAo598y4HyKsKCXIQEBnBK3yMfsuoe42l27y4uPypwVNW4+W/mbkrKPX3Ef7jkpCYf94Jh3RnXuwsPfphJeVUNN52Sxpodxfx17gb+euWIw8ctOlTJmY8t4IoxKfzuQs+dXcu37Sf/QDnnDe3eal+aNW7lr3M3ECDwjy83M3FgN8b1PvoLu7LazTNfbKakvIqCkgqeuWY0+w5V8rv313HmgASWb9vPgMRInr5mFKN7xnFa/3gKSytJjAo5/GXzk7P68eT8jUQEuxjVM4756/N54MIhDEuJOXyc2sFt0/bU7cKLch5kvGRUMjXOMzTZe0qZm5nPZ+vzef6GdJvqpB2ywNGAPSXlfL2pkH4JkQxKij7q11Ftf23+gXKG9Djy7pglOfsoKa/mvKGJ1LiVSYOObK00RER47trR3PvOGh76KIvPv91DYWkF3+4u4Y9z1vPUtFEs3bKP+evzKS6r4j+LtnLjhDT2HarkuheWUFZVw+TBibxw43djWqpKcVnV4cHTqho35//tay4fncLtZ/Wttx67i8t5f9UOig5VsWlPKY9fOYLHP93Awx9lMah7FAMSo7jxlDSCAwNQ9czoWlhawcu3jGN87y6EBrmocSvJsaGc1i+BwIAjB5NDAl0kxx7ZCrtjUj+q3W6e/jybt5bnMaZXHDefmtbka2farsvHpHD5GM8Y4Zy1u7jLaYHMuu1kUuLC/Vw70xwWOBowtEcM767YQUl5FZeMTD5qe6+unj/2rF0HmFgnMMzL2k1YkIu/TR11XM3xrpEh/OuGdGYty+Xhj7I4VFnD6J6xzF69k7T4CJ76bBPgGWDP3FnM37/YxMrtRXSNDGZc7y68v3IHZZU1hAW7yN5Tyu2vLmfTnlL+cd1oppyUxFcbC8jeU8oT8zYQGx7E6f3jj/if1+1WfjZzJUu3ehZevGxUMpeNTqZGlV+9vYZ1O4tRhdz9h7hsdAp/+CiLjG37GZ4Sw+n94g8HCFeAMGlQ059xCAgQ7jl3IJEhgTzzRTYPfn+o3crZAV0wLIkesWFc/8ISfvLaCv7v6pGkdY1oM3epmYb5NHCIyBTgb4ALeEFVH62zvSfwMhDr5LlPVeeIyLXAL72yDgdGq+oqEVkAJAFlzrZzVXWPL+o/1GlFlFe5GeQ17UOt+MgQRqTEMH99Pj+deOQdU8u27mes86v7eIkI08b1ZEKfrizduo8LhydxzhNf8dRnm0iKCeXC4UlMHdeTF77O4Y1lubgV/nL5cKLDAnl3xQ6ydh2gT3wED3+URf6BcgYkRnL/u2sZ2iOG91buINa53ff+d9cyeXA3Xrhx7OFjv5mRy9Kt+/j55P4EBwZwy6m9EREuH53ClsKDnDkggbcy8nh7eR4fr9lFoEt4+OKhXJme2ir/8//ozL784PQ+NkbRgY1MjeXxq0bwo1eXM+nxL4kKCWRocjR3nzOw3q5Q03b4LHCIiAt4BjgHyAOWichsVc3yyvZbPGuRPyciQ4A5QJqqvga85uxnGPCBqq7yKnetqvp8LVjv7qeB3et/UGvy4ESemL+RPSXldIvydF1VVNewMb+EiYNaZ8K4tPiIw3Mf/fZ7g7n9tRXceXZ/po7zDJxfM64XM5fmEhHs4nvDk9jnzMr6y7dXk1NwEIBfXzCISYMSuezZhVzw1NeUVdYwbVxP7ji7H7//IJOlW/YdvjtMVXlx4RaGJcdw59n9j/jF7wqQw4PjIYEBvLMij0OVNbz/01OPuBunNVjQ6PjOHdqdz+4+k4yt+1m7o5j3V+5gxjdbLHC0cb5scYwDslU1B0BEZgEXA96BQ4Hab+QYYGc9+5kGzPRhPY8pOjSInl3C2b7vUL0tDoDJQxJ5fN5G5mbmc/3JvQDYuLuUardyUo+Yesu0xPnDkvjfvROP6FYalhLD5MGJDEiMJCIkkPBgFzFhQeQUHKRPQgRjesZxw4Q0QoNcvPfTU/nTnG9Jjg3l55MHEBcRzCn94vlk3W52FJWREhfO0i372Jhfyl8uH95gN9HI1FjG9IqjZ5fwVg8apvPokxBJn4RIrhqbyv5DlazcXtR4IeNXvgwcyUCu1+c8YHydPA8Cn4rIHUAEMLme/VyNJ+B4e0lEaoB3gD+oDx9/H5EaS41biavndlvwzIk0JCmal7/ZynXjeyIirNvpWfLzpOTWDxxAvQOJ3gPhIsKQpGgW5ezl55MHcNGIHoe39U2IPCIveKY/AViTV0yPmDAe/3QjseFBfN+rXH1EhLcbeWremOYYmRrLR2t2sau4jMSoUBvzaKN8+fhrff/F637BTwNmqGoKcAHwiogcrpOIjAcOqeo6rzLXquow4HTndX29Bxe5TUQyRCSjoKDguE/idxcO5uVbxh5zu4jwwzN6k72nlAUbPcdZt6OY6NBAUuJa/jTv8RrfpwvdokI4twmT7w3qHk2wK4DVuUW8vnQ7S7fu4zcXDCYsuPHxGRGxwWvTampbrhP+9DlnPPYFC7ML/VwjUx9fBo48wHuhihSO7oq6FXgTQFUXAaGA98MSU6nTTaWqO5x/S4DX8XSJHUVVn1fVdFVNT0g4/rGGblGhh5/qPpYLh/egW1QIry7axv6Dlcxfn8+I1Fi/fqHeMak/X/zirCYNzgcHBjA0OZr56/N5+vNNjOvdhSvGpDRazpjWNtSre9ftVn7+xiqqbV30NseXgWMZ0F9EeotIMJ4gMLtOnu3A2QAiMhhP4ChwPgcAVwKzajOLSKCIxDvvg4ALgXX4WZArgCvGpPDFhj388D8Z7D9Yxa/Oa3huH19zBUizpme//cy+bC44SP6BCqZP7GetCOMXYcEuLhudzJ1n9+eB7w+loKSCr63V0eb4bIxDVatFZDowF8+tti+qaqaIPARkqOps4B7gXyJyF55urJu8xivOAPJqB9cdIcBcJ2i4gPnAv3x1Ds1x9dhUnl2wmRXb9/OXK0Yc8aRze3DOkETOGZLIvoOVnH6MZUiNORGeuMozNXtltWfVxHeW5zFxYNMfoDW+Z9Oqt6JXFm0ltUs4Z7XTP3K3W1HsNljTdvz+g3XMXJbrWWYg7OhlBkrKq7jxxaXcc+5ATu1nP3ha27GmVW9bc0O3c9dPSGu3QQM8T21b0DBtyeVjUqisdvPr99by81krydt/6IjtbyzLZcX2ImYu3X6MPRhfsClHjDFt1rDkGPp3i+TjNbsAmL9+Dw9cOIQr01OodisvLdwKwJcbCw4v+Wt8z66yMabNEvHMVDBtXE8+vesMhvaI5lfvrOHWlzP4zXtr2VFUxrRxqZSUV/PO8jxq3B2/670tsDEOY0y74XamZv/rpxs4VFnD9Sf34r7zBzH+j59RWlHNjRN68f8ubvoSBqZhxxrjsMBhjGl3isuqWJyzl4kDuxEcGMCeknIe/mg987J2s+i+s9lRVEaAyFHLHZjmsTXHjTEdRkxYEOcN7X74c7eoUH46sS8frt7JNS8s4dvdB0iIDGHhfZNs3MMH7IoaYzqEQd2juf7kXqgqp/dPYE9JBZ+t98mKC52etTiMMR3Gw84SzdU1bk778xe88HUOCVEhrM4t4voJvaz10UoscBhjOpxAVwB3nzOAX72zhsuf+waAV5dso0dMGH+bOpKuts55i1jgMMZ0SFeNTUUEMrbuZ2TPWN5buYMlW/by8EdZ/N/UUf6uXrtmgcMY02FdmZ7KlemeSbqnjevJE/M28tRnmzhjQAKRIYGcMSChRcs7d1YWOIwxncb0if1YmF3I3W+uBmDauFT+dNlwP9eq/bGRImNMpxEcGMCz147m8tEpXDCsOzOX5rI4Z6+/q9XuWOAwxnQqidGhPH7VCJ64aiRJMaE8/ukGOsOD0K3JAocxplMKDXLx4zP7smzrfhbn7PN3ddoVCxzGmE7r6rGphAe7mJu5299VaVcscBhjOq3QIBcDEqPYsLvE31VpV3waOERkiohsEJFsEbmvnu09ReQLEVkpImtE5AInPU1EykRklfP6h1eZMSKy1tnnU2KLYxtjWmBQ9yi+3X3AxjmawWeBQ0RcwDPA+cAQYJqIDKmT7bfAm6o6CpgKPOu1bbOqjnReP/ZKfw64DejvvKb46hyMMR3foO5R7D9URUFJhb+r0m74ssUxDshW1RxVrQRmARfXyaNA7bzHMcDOhnYoIklAtKouUs/Pg/8Al7RutY0xncnA7p6voPXWXdVkvgwcyUCu1+c8J83bg8B1IpIHzAHu8NrW2+nC+lJETvfaZ14j+zTGmCYb1D0KgA27D/i5Ju2HLwNHfWMPdTsRpwEzVDUFuAB4RUQCgF1AT6cL627gdRGJbuI+PQcXuU1EMkQko6Cg4LhPwhjTscVFBNM7PoI5a3fbOEcT+TJw5AGpXp9TOLor6lbgTQBVXQSEAvGqWqGqe5305cBmYICzz5RG9olT7nlVTVfV9ISEhFY4HWNMR3XLab1ZlVvEInuKvEl8GTiWAf1FpLeIBOMZ/J5dJ8924GwAERmMJ3AUiEiCM7iOiPTBMwieo6q7gBIROdm5m+oG4AMfnoMxphO4ckwK8ZEhPLdgs7+r0i74LHCoajUwHZgLrMdz91SmiDwkIhc52e4Bfigiq4GZwE3OoPcZwBon/W3gx6pa+2jn7cALQDaelsgnvjoHY0znEBrk4gen9+brTYWsySvyd3XaPOkMfXrp6emakZHh72oYY9qwkvIqTn30cyYN6mbrdThEZLmqptdNtyfHjTEGiAoN4vT+CazMtRZHYyxwGGOMY1D3KLbtPcTBimoAKqvdfq5R22SBwxhjHAOdZzpWbi/ihheXMvmJL6morvFzrdoeCxzGGOMYnOR5ivwnry3nq40FbN93iA9X7/JzrdoeCxzGGONIjg0jMiSQA+XV3DGpHwMSI3nxf1vswcA6LHAYY4wjIEAY1D2K6NBAfnB6H35wWh+ydh1gwQabfcJboL8rYIwxbcn/u3go5VU1xIQFcenoZJ76fBN//u+3AJw1MAFbycFaHMYYc4ShPWIY06sLAEGuAH59wWCy95Ry84xlfLjGxjvAAocxxjTogmFJrH3wPIanxPDQh1kUl1X5u0p+Z4HDGGMaERbs4o+XDmPfwQoem/utv6vjdxY4jDGmCU5KjuHGU9J4bcl2vu3ka3dY4DDGmCb6yVn9UIWvNnbuu6wscBhjTBMlRIXQq2s4K7Z17vmsLHAYY0wzjO4Zx/Lt+zv1Q4EWOIwxphlG94qjoKSCvP1l/q6K31jgMMaYZhjdMxaAFdv3+7km/mOBwxhjmmFAYhTBrgCydnbeO6t8GjhEZIqIbBCRbBG5r57tPUXkCxFZKSJrROQCJ/0cEVkuImudfyd5lVng7HOV8+rmy3MwxhhvQa4ABnSPJGtX5w0cPpurSkRcwDPAOUAesExEZqtqlle23+JZi/w5ERkCzAHSgELg+6q6U0ROwrNuebJXuWtV1daCNcb4xdCkGOatz0dVO+XcVb5scYwDslU1R1UrgVnAxXXyKBDtvI8BdgKo6kpV3emkZwKhIhLiw7oaY0yTDekRzb6DleQfqPB3VfzCl4EjGcj1+pzHka0GgAeB60QkD09r44569nM5sFJVvf8LveR0U/1OOmO4N8b41dAent+7mTuL/VwT//Bl4KjvC73ujc/TgBmqmgJcALwiIofrJCJDgT8DP/Iqc62qDgNOd17X13twkdtEJENEMgoKOvdTnsaY1jU4KZrAAGHp1n3+ropf+DJw5AGpXp9TcLqivNwKvAmgqouAUCAeQERSgPeAG1R1c20BVd3h/FsCvI6nS+woqvq8qqaranpCQkKrnJAxxgBEhAQyvk8XPlu/x99V8QtfBo5lQH8R6S0iwcBUYHadPNuBswFEZDCewFEgIrHAx8D9qrqwNrOIBIpIbWAJAi4E1vnwHIwxpl6TByeSvaeUrYUH/V2VE85ngUNVq4HpeO6IWo/n7qlMEXlIRC5yst0D/FBEVgMzgZvU8xz/dKAf8Ls6t92GAHNFZA2wCtgB/MtX52CMMccyeXAiAPPX5/u5JieedIb5VtLT0zUjw+7eNca0rkmPL6BXl3BeurneHvN2T0SWq2p63XR7ctwYY47TKX27snTLPqpq3P6uygllgcMYY47TKX3jOVhZw5q8znVbrgUOY4w5Tif36QrAos2Ffq7JiWWBwxhjjlOXiGD6xEewdoe1OIwxxjTRwO5RbNhd4u9qnFAWOIwxpgUGdo9i275DlFXW+LsqJ4wFDmOMaYGBiVGowqY9JVRU11Dj7viPOPhsWnVjjOkMBnaPAuCivy/EFSBM6Ebfv40AABfGSURBVNOVV38w3s+18i1rcRhjTAv06hpx+P2AxCgWbi5k/8FKP9bI9yxwGGNMC7gChOtO7snPJ/fnkUtPQhW+zu7Yt+daV5UxxrTQHy4ZBkCNW4kLD2LBhj1cNKKHn2vlO9biMMaYVuIKEE7rn8DCDt7isMBhjDGtaFRqLPkHKsg/UO7vqviMBQ5jjGlFI1JjAFidW+TnmvhOkwKHiPQVkRDn/Vki8jNnsSVjjDFehiTF4AoQVud18sABvAPUiEg/4N9AbzzLthpjjPESFuxiQGJUh54xt6mBw+2s6Hcp8H+qeheQ5LtqGWNM+zUyNZYV2/azt7TC31XxiaYGjioRmQbcCHzkpAX5pkrGGNO+3XxqGhXVbh6bu8HfVfGJpgaOm4EJwCOqukVEegOvNlZIRKaIyAYRyRaR++rZ3lNEvhCRlSKyRkQu8Np2v1Nug4ic19R9GmOMvw1IjOK6k3vxZkYuB8qr/F2dVtekwKGqWar6M1WdKSJxQJSqPtpQGRFxAc8A5wNDgGkiMqROtt8Cb6rqKGAq8KxTdojzeSgwBXhWRFxN3Kcxxvjd5MGJuLVj3l3V1LuqFohItIh0AVYDL4nIE40UGwdkq2qOqlYCs4CL6+RRINp5HwPsdN5fDMxS1QpV3QJkO/tryj6NMcbvhqfGIAIrt3fSwAHEqOoB4DLgJVUdA0xupEwykOv1Oc9J8/YgcJ2I5AFzgDsaKduUfQIgIreJSIaIZBQUFDRSVWOMaV3RoUH07xbJiu37/V2VVtfUwBEoIknAVXw3ON4YqSet7kT104AZqpoCXAC8IiIBDZRtyj49iarPq2q6qqYnJCQ0scrGGNN6RveMY+X2IlQ71hodTQ0cDwFzgc2qukxE+gCbGimTB6R6fU7hu66oWrcCbwKo6iIgFIhvoGxT9mmMMW3CqJ6xFJdVkVN40N9VaVVNHRx/S1WHq+rtzuccVb28kWLLgP4i0ltEgvEMds+uk2c7cDaAiAzGEzgKnHxTRSTEuYOrP7C0ifs0xpg2YVTPOKDjjXM0dXA8RUTeE5E9IpIvIu+ISEpDZZwHBqfjaamsx3P3VKaIPCQiFznZ7gF+KCKrgZnATeqRiaclkgX8F/ipqtYca5/NP21jjPG9fgmRRIUEsrKDjXNIU/reRGQenilGXnGSrgOuVdVzfFi3VpOenq4ZGRn+roYxphO6/t9LKCyt5LUfjKdLRLC/q9MsIrJcVdPrpjd1jCNBVV9S1WrnNQOwEWdjjGnEqJ5xrN91gNEPz2Ppln3+rk6raGrgKBSR62ofwhOR64C9vqyYMcZ0BN8blsSYXnHEhAXx/Fc5/q5Oq2hq4LgFz624u4FdwBV4piExxhjTgIHdo3jn9lO4cUIvPvs2n5yCUn9XqcWaelfVdlW9SFUTVLWbql6C52FAY4wxTXD9hDTCglw8+sm3/q5Ki7VkBcC7W60WxhjTwSVEhfDTif34NCufT9bu8nd1WqQlgaO+p7iNMcYcw62n9WZEaix3zlrFqnY8+WFLAkfHeobeGGN8LDTIxX9uHgcCc9pxqyOwoY0iUkL9AUKAMJ/UyBhjOrCY8CAGdY9i3Y72u7Rsg4FDVaNOVEWMMaazGNojhjlrd6GqiLS/Xv+WdFUZY4w5DkN7RFNcVkXe/jJ/V+W4WOAwxpgTbGgPz/p1mTsP+Lkmx8cChzHGnGCDk6JxBQhZO9vnOIcFDmOMOcFCg1z0TYhgnbU4jDHGNNXQHjFkWovDGGNMUw3tEU3+gQoKSir8XZVms8BhjDF+MLRHDEC7bHVY4DDGGD8Y0o7vrPJp4BCRKSKyQUSyReS+erY/KSKrnNdGESly0id6pa8SkXIRucTZNkNEtnhtG+nLczDGGF+ICQsiOTaMDbtL/F2VZmvwyfGWEBEX8AxwDpAHLBOR2aqaVZtHVe/yyn8HMMpJ/wIY6aR3AbKBT712/0tVfdtXdTfGmBOhT0IEWwoP+rsazebLFsc4IFtVc1S1EpgFXNxA/mnAzHrSrwA+UdVDPqijMcb4TZ/4CHIKSlFtX3PG+jJwJAO5Xp/znLSjiEgvoDfweT2bp3J0QHlERNY4XV0hx9jnbSKSISIZBQUFza+9Mcb4WJ+ESA5W1rCnnd1Z5cvAUd/MXccKq1OBt1W15ogdiCQBw4C5Xsn3A4OAsUAX4N76dqiqz6tquqqmJyQkNLfuxhjjc30SIgDIKWhf3VW+DBx5QKrX5xRg5zHy1teqAM865++palVtgqruUo8K4CU8XWLGGNPu9I53Akdh+1qH3JeBYxnQX0R6i0gwnuAwu24mERkIxAGL6tnHUeMeTisE8cxFfAmwrpXrbYwxJ0SPmDBCgwLaXYvDZ3dVqWq1iEzH083kAl5U1UwReQjIUNXaIDINmKV1RodEJA1Pi+XLOrt+TUQS8HSFrQJ+7KtzMMYYXwoIENK6tr87q3wWOABUdQ4wp07aA3U+P3iMslupZzBdVSe1Xg2NMca/+iREkNXOHgK0J8eNMcaP+sRHkru/jMpqt7+r0mQWOIwxxo/6JERQ41a272s/j6pZ4DDGGD86fGdVQfu5s8oChzHG+FGfhEgActrRALlPB8eNMcY0LCYsiPjIYLYUHCT/QDnvr9xBSGAA109IwxVQ33PU/meBwxhj/GxYcgyfrNvFopy9h8c6Mrbt5+/XjPZzzepnXVXGGONnD140FFXYVVzGWz+ewE2npPHRml2UlFc1XtgPrMVhjDF+1qtrBK//8GQOVVYzNq0Lefs9rY6CkgqiQoP8XLujWeAwxpg2YFhKzOH3CZGhgCdw1A6etyXWVWWMMW1MQpRntYiC0rY53boFDmOMaWO6OYFjzwELHMYYY5ogJiyIIJdYi8MYY0zTBAQI8ZEhFLTRlQEtcBhjTBuUEBXSZpeUtcBhjDFtULcoa3EYY4xphgQLHMYYY5ojITKEfQcrqHFr45lPMJ8GDhGZIiIbRCRbRO6rZ/uTIrLKeW0UkSKvbTVe22Z7pfcWkSUisklE3nDWMzfGmA4lIToUt9ImWx0+Cxwi4gKeAc4HhgDTRGSIdx5VvUtVR6rqSOBp4F2vzWW121T1Iq/0PwNPqmp/YD9wq6/OwRhj/CUlLgyAHUVtb4EnX7Y4xgHZqpqjqpXALODiBvJPA2Y2tEMREWAS8LaT9DJwSSvU1Rhj2pRUJ3Dk7S/zc02O5svAkQzken3Oc9KOIiK9gN7A517JoSKSISKLRaQ2OHQFilS1ugn7vM0pn1FQUNCS8zDGmBMuOTYcaJuBw5eTHNa3AsmxRnmmAm+rao1XWk9V3SkifYDPRWQtcKCp+1TV54HnAdLT09ve6JIxxjQgLNhFfGTw4Zly2xJftjjygFSvzynAzmPknUqdbipV3en8mwMsAEYBhUCsiNQGvIb2aYwx7VpyXHibbHH4MnAsA/o7d0EF4wkOs+tmEpGBQBywyCstTkRCnPfxwKlAlqoq8AVwhZP1RuADH56DMcb4TUpcGDs6U+BwxiGmA3OB9cCbqpopIg+JiPddUtOAWU5QqDUYyBCR1XgCxaOqmuVsuxe4W0Sy8Yx5/NtX52CMMf6UEhdGXlEZ7jb2LIdPF3JS1TnAnDppD9T5/GA95b4Bhh1jnzl47tgyxpgOLSUunMpqNwWlFSRGh/q7OofZk+PGGNNGDejmWf1vbV6xn2tyJAscxhjTRo1IjSXYFcCyrfv8XZUjWOAwxpg2KjTIxYjUGJZsscBhjDGmicb17sK6HcU88ekGdha1jTusLHAYY0wbdkb/BKrdylOfZ3PJMwvJ3ef/BwItcBhjTBs2vk9Xlvz6bD664zT2lFTw33W7/V0l396Oa4wxpuUSo0NJjA4lISqEDfkl/q6OtTiMMaa9GJAYySYLHMYYY5pqQGIUG/NL/f4kuQUOY4xpJwYkRlFWVcMOP99dZYHDGGPaiQGJUQBs2O3f7ioLHMYY004MSIxEBNbu8O8UJBY4jDGmnYgKDWJ4cgwLswv9Wg8LHMYY046c1j+elblFlJRX+a0OFjiMMaYdObVfPDVuZUmO/+avssBhjDHtyJhecQS5hOXb9/utDhY4jDGmHQkJdJEYHerXCQ99GjhEZIqIbBCRbBG5r57tT4rIKue1UUSKnPSRIrJIRDJFZI2IXO1VZoaIbPEqN9KX52CMMW1Nj9gwdhWV++34PpurSkRcwDPAOUAesExEZnutHY6q3uWV/w5glPPxEHCDqm4SkR7AchGZq6pFzvZfqurbvqq7Mca0ZT1iQsnY1jG7qsYB2aqao6qVwCzg4gbyTwNmAqjqRlXd5LzfCewBEnxYV2OMaTeSYsPIP1Dut6lHfBk4koFcr895TtpRRKQX0Bv4vJ5t44BgYLNX8iNOF9aTIhLSelU2xpi2r0dMKFU1ym8/WMcHq3ac8OP7MnBIPWnHCo9TgbdVteaIHYgkAa8AN6uq20m+HxgEjAW6APfWe3CR20QkQ0QyCgoKjqf+xhjTJiXFhAHw+pLt3Dlr1Qk/vi8DRx6Q6vU5Bdh5jLxTcbqpaolINPAx8FtVXVybrqq71KMCeAlPl9hRVPV5VU1X1fSEBOvlMsZ0HEmxoX49vi8DxzKgv4j0FpFgPMFhdt1MIjIQiAMWeaUFA+8B/1HVt+rkT3L+FeASYJ3PzsAYY9qgHk6LAyAx+sT31vsscKhqNTAdmAusB95U1UwReUhELvLKOg2Ypare3VhXAWcAN9Vz2+1rIrIWWAvEA3/w1TkYY0xbFBseRFiQC4ADZdUn/Phy5Pd1x5Senq4ZGRn+roYxxrSaj9bsZH5WPu+v2knWQ+cRHtz6T1eIyHJVTa+bbk+OG2NMO3Th8B6c0jcegL2llSf02BY4jDGmneoSEQzAr99by6/fW3vCjmuBwxhj2qmukZ7A8fWmQj7NzD9hx7XAYYwx7VR85Hd3VBWWVlBacWIGyi1wGGNMO1XbVVVra+HBE3JcCxzGGNNOhQe7CA367mt8614LHMYYYxogInSNCDkcPLbtPXRCjuuzadWNMcb4XmJ0CD1iQ9m29xBbTlBXlQUOY4xpxx69fDiuAOH+d9eyzbqqjDHGNGZAYhR9EyLp3y2S9btKqKx2N16ohSxwGGNMBzBxYDdKK6pZsmUvew6U89LCLdT4aKEn66oyxpgO4NR+8YQGBTAvK5//bSrkn1/lUFBSwa+mDGr1Y1mLwxhjOoCwYBen9Utgflb+4burnl2wmVW5Ra1+LGtxGGNMB3HWwATmr8+nuKyAswd1Y/KQRIYnx7T6cSxwGGNMB3FqP89suQcraxjdK45p43r65DjWVWWMMR1EWtdwesR4lpUdkhTts+NY4DDGmA5CRDjFaXUM6dFOA4eITBGRDSKSLSL31bP9Sa+lYTeKSJHXthtFZJPzutErfYyIrHX2+ZSz9rgxxhjgB6f35p5zBtAtyndrkftsjENEXMAzwDlAHrBMRGaralZtHlW9yyv/HcAo530X4PdAOqDAcqfsfuA54DZgMTAHmAJ84qvzMMaY9mRQ92gGdfddawN82+IYB2Srao6qVgKzgIsbyD8NmOm8Pw+Yp6r7nGAxD5giIklAtKouUs9i6f8BLvHdKRhjjKnLl4EjGcj1+pznpB1FRHoBvYHPGymb7LxvdJ/GGGN8w5eBo76xh2M9/z4VeFtVaxop2+R9ishtIpIhIhkFBQWNVtYYY0zT+DJw5AGpXp9TgJ3HyDuV77qpGiqb57xvdJ+q+ryqpqtqekJCQjOrbowx5lh8GTiWAf1FpLeIBOMJDrPrZhKRgUAcsMgreS5wrojEiUgccC4wV1V3ASUicrJzN9UNwAc+PAdjjDF1+OyuKlWtFpHpeIKAC3hRVTNF5CEgQ1Vrg8g0YJYz2F1bdp+IPIwn+AA8pKr7nPe3AzOAMDx3U9kdVcYYcwKJ1/d1h5Wenq4ZGRn+roYxxrQrIrJcVdPrptuT48YYY5qlU7Q4RKQA2ObvejRTPFDo70q0Y3b9WsauX8t1hGvYS1WPuruoUwSO9khEMuprIpqmsevXMnb9Wq4jX0PrqjLGGNMsFjiMMcY0iwWOtut5f1egnbPr1zJ2/Vquw15DG+MwxhjTLNbiMMYY0ywWOHxMRF4UkT0isu44yh5z0SoRucNZJCtTRP7SurVuW3x1DZ3tvxARFZH41qtx2+KL6ycij4nItyKyRkTeE5HY1q+5f7Xkuh1jfx1mcToLHL43A89iU8ejdtGq/s5rCoCITMSztslwVR0K/LXl1WzTZtDK1xBARFLxLDS2vYX1a+tm0PrXbx5wkqoOBzYC97ewjm3RDI7juonIAhFJq5NWuzjdeDxrFf3emYcPGvgbbasscPiYqn4F7PNOE5G+IvJfEVkuIl+LyKC65RpZtOp24FFVrXCOsce3Z+FfPrqGAE8Cv+LY0/13CL64fqr6qapWO1kXc+Ss1R3C8V63Y+hQi9NZ4PCP54E7VHUM8Avg2XryNLRo1QDgdBFZIiJfishYn9a2bWrRNRSRi4Adqrra1xVto1r6N+jtFjrPZKNNuW716VCL0/lsdlxTPxGJBE4B3vLqyqxvVfmGFq0KxDMV/cnAWOBNEemjneQWuZZeQxEJB36DZ7r+TqeV/gZr9/UboBp4rTXr2BY1dN1E5GbgTietHzBHRCqBLap6Ka2wOF1bYoHjxAsAilR1pHeiiLiA5c7H2Xj6PY+1aFUe8K4TKJaKiBvPvDidZanDll7DvniWKl7tfAGkACtEZJyq7vZx3duC1vgbxBngvRA4u5P8aKn3ugGo6kvAS+AZ4wBuUtWtXlnygLO8PqcAC2jG4nRtiXVVnWCqegDYIiJXAojHCFWtUdWRzuuBRhateh+Y5JQfAATT/idTa7KWXkNVXauq3VQ1TVXT8PzPO7qTBI1W+RsUkSnAvcBFqnrIX+dyIh3rujWxeMdanE5V7eXDF54lcXcBVXi+oG7F82v3v8BqIAt44Bhl04F1wGbg73z3wGYw8KqzbQUwyd/n2d6uYZ08W4F4f59ne7p+QDaePvtVzusf/j7PtnLd8LQk0upJv8W5btnAzc35G21rL3ty3BhjTLNYV5UxxphmscBhjDGmWSxwGGOMaRYLHMYYY5rFAocxxphmscBhOiURKT3Bx3tBRIa00r5qRGSViKwTkQ8bm5lWRGJF5CetcWxjwBZyMp2UiJSqamQr7i9Qv5v0z6e86y4iLwMbVfWRBvKnAR+p6kknon6m47MWhzEOEUkQkXdEZJnzOtVJHyci34jISuffgU76TSLyloh8CHwqImc5U2q/LZ61Kl5zngaunWo73XlfKiKPiMhqEVksIolOel/n8zIReaiJraJFfDdxY6SIfCYiK8SzvsPFTp5Hgb5OK+UxJ+8vneOsEZH/14qX0XQCFjiM+c7fgCdVdSxwOfCCk/4tcIaqjgIeAP7oVWYCcKOqTnI+jwJ+DgwB+gCn1nOcCGCxqo4AvgJ+6HX8vznHb3S+ImduqbPxzCsFUA5cqqqjgYnA407gug/YrJ6pRH4pIufiWfdhHDASGCMiZzR2PGNq2SSHxnxnMjDEa+bTaBGJAmKAl0WkP56ZS4O8ysxTVe81G5aqah6AiKwC0oD/1TlOJfCR8345nsWkwBOEatdieJ1jL9AV5rXv5XjWdgDPTKt/dIKAG09LJLGe8uc6r5XO50g8geSrYxzPmCNY4DDmOwHABFUt804UkaeBL1T1Ume8YIHX5oN19lHh9b6G+v8fq9LvBhePlachZao6UkRi8ASgnwJPAdcCCcAYVa0Ska1AaD3lBfiTqv6zmcc1BrCuKmO8fQpMr/0gIrXTZ8cAO5z3N/nw+IvxdJEBTG0ss6oWAz8DfiEiQXjquccJGhOBXk7WEiDKq+hc4BbxrC+BiCSLSLdWOgfTCVjgMJ1VuIjkeb3uxvMlnO4MGGcBP3by/gX4k4gsBFw+rNPPgbtFZCmQBBQ3VkBVV+KZqXUqnsWU0kUkA0/r41snz15goXP77mOq+imerrBFIrIWeJsjA4sxDbLbcY1pI8SzMmGZqqqITAWmqerFjZUz5kSzMQ5j2o4xwN+dO6GK8KzfYEybYy0OY4wxzWJjHMYYY5rFAocxxphmscBhjDGmWSxwGGOMaRYLHMYYY5rFAocxxphm+f+noCzmtx1oXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_find(learn, num_it=300)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_simple</th>\n",
       "      <th>acc_camvid_with_zero_check</th>\n",
       "      <th>dice_coefficient</th>\n",
       "      <th>dice_coefficient_2</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.865102</td>\n",
       "      <td>0.875629</td>\n",
       "      <td>0.124383</td>\n",
       "      <td>0.044462</td>\n",
       "      <td>0.072165</td>\n",
       "      <td>0.022877</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.784808</td>\n",
       "      <td>0.863730</td>\n",
       "      <td>0.136290</td>\n",
       "      <td>0.042586</td>\n",
       "      <td>0.071098</td>\n",
       "      <td>0.021983</td>\n",
       "      <td>04:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.725004</td>\n",
       "      <td>0.828222</td>\n",
       "      <td>0.171815</td>\n",
       "      <td>0.038054</td>\n",
       "      <td>0.077849</td>\n",
       "      <td>0.023980</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.669932</td>\n",
       "      <td>0.588293</td>\n",
       "      <td>0.411746</td>\n",
       "      <td>0.036051</td>\n",
       "      <td>0.080068</td>\n",
       "      <td>0.034494</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.622816</td>\n",
       "      <td>0.497489</td>\n",
       "      <td>0.502565</td>\n",
       "      <td>0.048251</td>\n",
       "      <td>0.092618</td>\n",
       "      <td>0.045472</td>\n",
       "      <td>03:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.527818</td>\n",
       "      <td>0.494573</td>\n",
       "      <td>0.505499</td>\n",
       "      <td>0.071973</td>\n",
       "      <td>0.118664</td>\n",
       "      <td>0.050523</td>\n",
       "      <td>04:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.556169</td>\n",
       "      <td>0.433459</td>\n",
       "      <td>0.566591</td>\n",
       "      <td>0.077885</td>\n",
       "      <td>0.253065</td>\n",
       "      <td>0.059839</td>\n",
       "      <td>04:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.528316</td>\n",
       "      <td>0.418687</td>\n",
       "      <td>0.581353</td>\n",
       "      <td>0.075354</td>\n",
       "      <td>0.431715</td>\n",
       "      <td>0.059368</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.572401</td>\n",
       "      <td>0.414747</td>\n",
       "      <td>0.585298</td>\n",
       "      <td>0.074040</td>\n",
       "      <td>0.552012</td>\n",
       "      <td>0.058696</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.521823</td>\n",
       "      <td>0.413901</td>\n",
       "      <td>0.586142</td>\n",
       "      <td>0.073781</td>\n",
       "      <td>0.551771</td>\n",
       "      <td>0.058382</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.517214</td>\n",
       "      <td>0.413462</td>\n",
       "      <td>0.586582</td>\n",
       "      <td>0.073790</td>\n",
       "      <td>0.551403</td>\n",
       "      <td>0.058306</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.517897</td>\n",
       "      <td>0.413422</td>\n",
       "      <td>0.586616</td>\n",
       "      <td>0.073801</td>\n",
       "      <td>0.551346</td>\n",
       "      <td>0.058281</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with dice_coefficient value: 0.07216453552246094.\n",
      "Better model found at epoch 2 with dice_coefficient value: 0.07784908264875412.\n",
      "Better model found at epoch 3 with dice_coefficient value: 0.08006787300109863.\n",
      "Better model found at epoch 4 with dice_coefficient value: 0.0926179438829422.\n",
      "Better model found at epoch 5 with dice_coefficient value: 0.11866370588541031.\n",
      "Better model found at epoch 6 with dice_coefficient value: 0.25306522846221924.\n",
      "Better model found at epoch 7 with dice_coefficient value: 0.43171536922454834.\n",
      "Better model found at epoch 8 with dice_coefficient value: 0.5520116090774536.\n"
     ]
    }
   ],
   "source": [
    "train_learner(learn, slice(lr), epochs=12, pct_start=0.8, best_model_name='bestmodel-frozen-1', \n",
    "              patience_early_stop=4, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('bestmodel-frozen-1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(file='/kaggle/model/export-1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = slice(lr/100,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_learner(learn, lrs, epochs=10, pct_start=0.8, best_model_name='bestmodel-unfrozen-1', \n",
    "              patience_early_stop=4, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('bestmodel-unfrozen-1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(file='/kaggle/model/export-2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = (SegmentationItemList.from_folder(path/'train_images')\n",
    "       .split_by_rand_pct(valid_pct=valid_pct)\n",
    "       .label_from_func(get_y_fn, classes=codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (src.transform(get_extra_transforms(), size=src_size, tfm_y=True)\n",
    "        .add_test(ImageList.from_folder(path/'test_images'), tfms=None, tfm_y=False)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd, bottle=True)\n",
    "learn.model_dir = Path('/kaggle/model')\n",
    "learn.loss_func = CrossEntropyFlat(axis=1, weight=torch.tensor([2.0, .5, .5, .5, .5]).cuda())\n",
    "learn = to_fp16(learn, loss_scale=4.0)\n",
    "learn.load('bestmodel-unfrozen-1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn, num_it=400)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_learner(learn, slice(lr), epochs=10, pct_start=0.8, best_model_name='bestmodel-frozen-3', \n",
    "              patience_early_stop=4, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-3');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('bestmodel-3');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(file='/kaggle/model/export-3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = slice(lr/1000,lr/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_learner(learn, lrs, epochs=10, pct_start=0.8, best_model_name='bestmodel-4', \n",
    "              patience_early_stop=4, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-4');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('bestmodel-4');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(file='/kaggle/model/export-4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!cp /kaggle/model/export.pkl /opt/fastai/fastai-exercises/nbs_gil\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'export-4.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn=None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = (path/'test_images').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv /kaggle/model/export-4.pkl /kaggle/model/export.pkl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_learn = load_learner('/kaggle/model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_path):\n",
    "    pred_class, pred_idx, outputs = inference_learn.predict(open_image(str(img_path)))\n",
    "    return pred_class, pred_idx, outputs\n",
    "\n",
    "def encode_classes(pred_class_data):\n",
    "    pixels = np.concatenate([[0], torch.transpose(pred_class_data.squeeze(), 0, 1).flatten(), [0]])\n",
    "    classes_dict = {1: [], 2: [], 3: [], 4: []}\n",
    "    count = 0\n",
    "    previous = pixels[0]\n",
    "    for i, val in enumerate(pixels):\n",
    "        if val != previous:\n",
    "            if previous in classes_dict:\n",
    "                classes_dict[previous].append((i - count, count))\n",
    "            count = 0\n",
    "        previous = val\n",
    "        count += 1\n",
    "    return classes_dict\n",
    "\n",
    "\n",
    "def convert_classes_to_text(classes_dict, clazz):\n",
    "    return ' '.join([f'{v[0]} {v[1]}' for v in classes_dict[clazz]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_to_predict = train_images[16].name\n",
    "display_image_with_mask(image_to_predict)\n",
    "pred_class, pred_idx, outputs = predict(path/f'train_images/{image_to_predict}')\n",
    "pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(pred_class.data.squeeze(), 0, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking encoding methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_all = encode_classes(pred_class.data)\n",
    "print(convert_classes_to_text(encoded_all, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = train_images[16]\n",
    "print(get_y_fn(image_name))\n",
    "img = open_mask(get_y_fn(image_name))\n",
    "img_data = img.data\n",
    "print(convert_classes_to_text(encode_classes(img_data), 3))\n",
    "img_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through the test images and create submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "defect_classes = [1, 2, 3, 4]\n",
    "with open('submission.csv', 'w') as submission_file:\n",
    "    submission_file.write('ImageId_ClassId,EncodedPixels\\n')\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        pred_class, pred_idx, outputs = predict(test_image)\n",
    "        encoded_all = encode_classes(pred_class.data)\n",
    "        for defect_class in defect_classes:\n",
    "            submission_file.write(f'{test_image.name}_{defect_class},{convert_classes_to_text(encoded_all, defect_class)}\\n')\n",
    "        if i % 5 == 0:\n",
    "            print(f'Processed {i} images\\r', end='')\n",
    "            \n",
    "print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative prediction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,y = learn.get_preds(ds_type=DatasetType.Test, with_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class_data = preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((path/'test_images').ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test_ds.x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
