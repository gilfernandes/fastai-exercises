{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from fastai.vision import *\n",
    "from fastai import *\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from fastai.vision.models.cadene_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = pd.read_csv('/root/.fastai/data/severstal/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId_ClassId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002cc93b.jpg_1</td>\n",
       "      <td>29102 12 29346 24 29602 24 29858 24 30114 24 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002cc93b.jpg_2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002cc93b.jpg_3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002cc93b.jpg_4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00031f466.jpg_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId_ClassId                                      EncodedPixels\n",
       "0  0002cc93b.jpg_1  29102 12 29346 24 29602 24 29858 24 30114 24 3...\n",
       "1  0002cc93b.jpg_2                                                NaN\n",
       "2  0002cc93b.jpg_3                                                NaN\n",
       "3  0002cc93b.jpg_4                                                NaN\n",
       "4  00031f466.jpg_1                                                NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/root/.fastai/data/severstal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/root/.fastai/data/severstal/train_images.zip'),\n",
       " PosixPath('/root/.fastai/data/severstal/sample_submission.csv'),\n",
       " PosixPath('/root/.fastai/data/severstal/test_images.zip'),\n",
       " PosixPath('/root/.fastai/data/severstal/train.csv'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images'),\n",
       " PosixPath('/root/.fastai/data/severstal/test_images')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/root/.fastai/data/severstal/train_images/5e581254c.jpg'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images/fd2f7b4f4.jpg'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images/82f4c0b69.jpg')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = get_image_files(path/'train_images')\n",
    "train_images[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check maximum size of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_img_max_size(folder):\n",
    "    max_height = 0\n",
    "    max_width = 0\n",
    "    for train_image in train_images:\n",
    "        img = open_image(train_image)\n",
    "        if max_height < img.shape[1]:\n",
    "            max_height = img.shape[1]\n",
    "        if max_width < img.shape[2]:\n",
    "            max_width = img.shape[2]\n",
    "    return max_height, max_width\n",
    "\n",
    "def show_image(images, index):\n",
    "    img_f = images[index]\n",
    "    print(type(img_f))\n",
    "    img = open_image(img_f)\n",
    "    print(img)\n",
    "    img.show(figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_path = Path('/kaggle/mask')\n",
    "if not os.path.exists(mask_path):\n",
    "    os.makedirs(str(mask_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_encoded_to_array(encoded_pixels):\n",
    "    pos_array = []\n",
    "    len_array = []\n",
    "    splits = encoded_pixels.split()\n",
    "    pos_array = [int(n) - 1 for i, n in enumerate(splits) if i % 2 == 0]\n",
    "    len_array = [int(n) for i, n in enumerate(splits) if i % 2 == 1]\n",
    "    return pos_array, len_array\n",
    "        \n",
    "def convert_to_pair(pos_array, rows):\n",
    "    return [(p % rows, p // rows) for p in pos_array]\n",
    "\n",
    "def create_positions(single_pos, size):\n",
    "    return [i for i in range(single_pos, single_pos + size)]\n",
    "\n",
    "def create_positions_pairs(single_pos, size, row_size):\n",
    "    return convert_to_pair(create_positions(single_pos, size), row_size)\n",
    "\n",
    "def convert_to_mask(encoded_pixels, row_size, col_size, category):\n",
    "    pos_array, len_array = convert_encoded_to_array(encoded_pixels)\n",
    "    mask = np.zeros([row_size, col_size])\n",
    "    for(p, l) in zip(pos_array, len_array):\n",
    "        for row, col in create_positions_pairs(p, l, row_size):\n",
    "            mask[row][col] = category\n",
    "    return mask\n",
    "\n",
    "def save_to_image(masked, image_name):\n",
    "    im = PIL.Image.fromarray(masked)\n",
    "    im = im.convert(\"L\")\n",
    "    image_name = re.sub(r'(.+)\\.jpg', r'\\1', image_name) + \".png\"\n",
    "    real_path = mask_path/image_name\n",
    "    im.save(real_path)\n",
    "    return real_path\n",
    "\n",
    "def open_single_image(path):\n",
    "    img = open_image(path)\n",
    "    img.show(figsize=(20,20))\n",
    "    \n",
    "def get_y_fn(x):\n",
    "    return mask_path/(x.stem + '.png')\n",
    "\n",
    "def group_by(train_images, train_pd):\n",
    "    tran_dict = {image.name:[] for image in train_images}\n",
    "    pattern = re.compile('(.+)_(\\d+)')\n",
    "    for index, image_path in train_pd.iterrows():\n",
    "        m = pattern.match(image_path['ImageId_ClassId'])\n",
    "        file_name = m.group(1)\n",
    "        category = m.group(2)\n",
    "        tran_dict[file_name].append((int(category), image_path['EncodedPixels']))\n",
    "    return tran_dict\n",
    "\n",
    "def display_image_with_mask(img_name):\n",
    "    full_image = path/'train_images'/img_name\n",
    "    print(full_image)\n",
    "    open_single_image(full_image)\n",
    "    mask_image = get_y_fn(full_image)\n",
    "    mask = open_mask(mask_image)\n",
    "    print(full_image)\n",
    "    mask.show(figsize=(20, 20), alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limited_dihedral_affine(k:partial(uniform_int,0,3)):\n",
    "    \"Randomly flip `x` image based on `k`.\"\n",
    "    x = -1 if k&1 else 1\n",
    "    y = -1 if k&2 else 1\n",
    "    if k&4: return [[0, x, 0.],\n",
    "                    [y, 0, 0],\n",
    "                    [0, 0, 1.]]\n",
    "    return [[x, 0, 0.],\n",
    "            [0, y, 0],\n",
    "            [0, 0, 1.]]\n",
    "\n",
    "dihedral_affine = TfmAffine(limited_dihedral_affine)\n",
    "\n",
    "def get_extra_transforms(max_rotate:float=3., max_zoom:float=1.1,\n",
    "                   max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75,\n",
    "                   p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None)->Collection[Transform]:\n",
    "    \"Utility func to easily create a list of flip, rotate, `zoom`, warp, lighting transforms.\"\n",
    "    p_lightings = [p_lighting, p_lighting + 0.2, p_lighting + 0.4, p_lighting + 0.6, p_lighting + 0.7]\n",
    "    max_lightings = [max_lighting, max_lighting + 0.2, max_lighting + 0.4, max_lighting + 0.6, max_lighting + 0.7]\n",
    "    res = [rand_crop(), dihedral_affine(), \n",
    "           symmetric_warp(magnitude=(-max_warp,max_warp), p=p_affine),\n",
    "           rotate(degrees=(-max_rotate,max_rotate), p=p_affine),\n",
    "           rand_zoom(scale=(1., max_zoom), p=p_affine)]\n",
    "    res.extend([brightness(change=(0.5*(1-mp[0]), 0.5*(1+mp[0])), p=mp[1]) for mp in zip(max_lightings, p_lightings)])\n",
    "    res.extend([contrast(scale=(1-mp[0], 1/(1-mp[0])), p=mp[1]) for mp in zip(max_lightings, p_lightings)])\n",
    "    #       train                   , valid\n",
    "    return (res, [crop_pad()])\n",
    "\n",
    "def get_simple_transforms(max_rotate:float=3., max_zoom:float=1.1,\n",
    "                   max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75,\n",
    "                   p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None)->Collection[Transform]:\n",
    "    \"Utility func to easily create a list of flip, rotate, `zoom`, warp, lighting transforms.\"\n",
    "    res = [\n",
    "#         rand_crop(),\n",
    "        symmetric_warp(magnitude=(-max_warp,max_warp), p=p_affine),\n",
    "        rotate(degrees=(-max_rotate,max_rotate), p=p_affine),\n",
    "        rand_zoom(scale=(1., max_zoom), p=p_affine)\n",
    "          ]\n",
    "    #       train                   , valid\n",
    "    return (res, [crop_pad()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = array(['0', '1', '2', '3', '4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = (path/'train_images').ls()\n",
    "src_size = np.array(open_image(str(train_images[0])).shape[1:])\n",
    "valid_pct = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_bunch(bs, size, start_pos, end_pos):\n",
    "    src = (SegmentationItemList.from_folder(path/'train_images')\n",
    "       .split_by_rand_pct(valid_pct=valid_pct)\n",
    "       .label_from_func(get_y_fn, classes=codes))\n",
    "    test_files = [f.name for f in get_image_files(path/'test_images')]\n",
    "    test_files = test_files[start_pos:end_pos]\n",
    "    print('len(test_files)', len(test_files))\n",
    "    data = (src.transform(get_transforms(max_rotate=25), size=size, tfm_y=True)\n",
    "        .add_test(ImageList.from_df(path=path/'test_images', df=pd.DataFrame(test_files)), \n",
    "                  tfms=None, tfm_y=False)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))\n",
    "    return src, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create learner and training\n",
    "Starting with low resolution training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "name2id = {v:k for k,v in enumerate(codes)}\n",
    "void_code = name2id['0']\n",
    "\n",
    "def acc_camvid(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    mask = target != void_code\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    comparison = argmax[mask]==target[mask]\n",
    "    return torch.tensor(0.) if comparison.numel() == 0 else comparison.float().mean()\n",
    "\n",
    "def acc_camvid_with_zero_check(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        if(torch.sum(argmax[b]).item() == 0.0 and torch.sum(target[b]).item() == 0.0):\n",
    "            total[b] = 1\n",
    "        else:\n",
    "            mask = target[b] != void_code\n",
    "            comparison = argmax[b][mask]==target[b][mask]\n",
    "            total[b] = torch.tensor(0.) if comparison.numel() == 0 else comparison.float().mean()\n",
    "    return total.mean()\n",
    "\n",
    "\n",
    "def calc_dice_coefficients(argmax, target, cats):\n",
    "    def calc_dice_coefficient(seg, gt, cat: int):\n",
    "        mask_seg = seg == cat\n",
    "        mask_gt = gt == cat\n",
    "        sum_seg = torch.sum(mask_seg.float())\n",
    "        sum_gt = torch.sum(mask_gt.float())\n",
    "        if sum_seg + sum_gt == 0:\n",
    "            return torch.tensor(1.0)\n",
    "        return (torch.sum((seg[gt == cat] / cat).float()) * 2.0) / (sum_seg + sum_gt)\n",
    "\n",
    "    total_avg = torch.empty([len(cats)])\n",
    "    for i, c in enumerate(cats):\n",
    "        total_avg[i] = calc_dice_coefficient(argmax, target, c)\n",
    "    return total_avg.mean()\n",
    "\n",
    "\n",
    "def dice_coefficient(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    cats = [1, 2, 3, 4]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        total[b] = calc_dice_coefficients(argmax[b], target[b], cats)\n",
    "    return total.mean()\n",
    "\n",
    "def calc_dice_coefficients_2(argmax, target, cats):\n",
    "    def calc_dice_coefficient(seg, gt, cat: int):\n",
    "        mask_seg = seg == cat\n",
    "        mask_gt = gt == cat\n",
    "        sum_seg = torch.sum(mask_seg.float())\n",
    "        sum_gt = torch.sum(mask_gt.float())\n",
    "        return (torch.sum((seg[gt == cat] / cat).float())), (sum_seg + sum_gt)\n",
    "\n",
    "    total_avg = torch.empty([len(cats), 2])\n",
    "    for i, c in enumerate(cats):\n",
    "        total_avg[i][0], total_avg[i][1] = calc_dice_coefficient(argmax, target, c)\n",
    "    total_sum = total_avg.sum(axis=0)\n",
    "    if (total_sum[1] == 0.0):\n",
    "        return torch.tensor(1.0)\n",
    "    return total_sum[0] * 2.0 / total_sum[1]\n",
    "\n",
    "\n",
    "def dice_coefficient_2(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    cats = [1, 2, 3, 4]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        total[b] = calc_dice_coefficients_2(argmax[b], target[b], cats)\n",
    "    return total.mean()\n",
    "\n",
    "\n",
    "def accuracy_simple(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    return (input.argmax(dim=1)==target).float().mean()\n",
    "\n",
    "\n",
    "def dice_coeff(pred, target):\n",
    "    smooth = 1.\n",
    "    num = pred.size(0)\n",
    "    m1 = pred.view(num, -1)  # Flatten\n",
    "    m2 = target.view(num, -1)  # Flatten\n",
    "    intersection = (m1 * m2).sum()\n",
    "    return (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDiceLoss(nn.Module):\n",
    "    def __init__(self, zero_cat_factor=0.1):\n",
    "        super().__init__()\n",
    "        self.zero_cat_factor = zero_cat_factor\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return self.dice_loss(target, input, self.zero_cat_factor)\n",
    "\n",
    "    def dice_loss(self, target, output, eps=1e-7, zero_cat_factor=0.1):\n",
    "        '''\n",
    "        Soft dice loss calculation for arbitrary batch size, number of classes, and number of spatial dimensions.\n",
    "        Assumes the `channels_last` format.\n",
    "\n",
    "        # Arguments\n",
    "            target: b x 1 x X x Y( x Z...) ground truth\n",
    "            output: b x c x X x Y( x Z...) Network output, must sum to 1 over c channel (such as after softmax)\n",
    "            epsilon: Used for numerical stability to avoid divide by zero errors\n",
    "\n",
    "        # References\n",
    "            V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation\n",
    "            https://arxiv.org/abs/1606.04797\n",
    "            More details on Dice loss formulation\n",
    "            https://mediatum.ub.tum.de/doc/1395260/1395260.pdf (page 72)\n",
    "\n",
    "            Adapted from https://github.com/Lasagne/Recipes/issues/99#issuecomment-347775022\n",
    "        '''\n",
    "\n",
    "        # skip the batch and class axis for calculating Dice score\n",
    "        num_classes = output.shape[1]\n",
    "        y_true = F.one_hot(target.long().squeeze(), num_classes)\n",
    "        y_pred = F.softmax(output, dim=1).permute(0, 2, 3, 1)\n",
    "        y_true = y_true.type(y_pred.type())\n",
    "        y_true = y_true.permute(0, 3, 1, 2)\n",
    "        y_true[:,0,:] *= zero_cat_factor # Factor used to take power away from the zeroth category\n",
    "        y_true = y_true.permute(0, 2, 3, 1)\n",
    "        axes = tuple(range(1, len(y_pred.shape)-1))\n",
    "        numerator = 2. * torch.sum(y_pred * y_true, axes)\n",
    "        denominator = torch.sum(y_pred ** 2 + y_true ** 2, axes)\n",
    "        # When intersection and cardinality are all zero you have 100% score and not 0% score\n",
    "        # For this we use the eps parameter\n",
    "        loss_array = ((numerator + eps) / (denominator + eps))\n",
    "        loss_array = (loss_array).mean(dim=0)\n",
    "        return ((1 - torch.mean(loss_array)) + F.cross_entropy(output, target.squeeze())) / 2.\n",
    "\n",
    "    def __del__(self): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_path):\n",
    "    pred_class, pred_idx, outputs = inference_learn.predict(open_image(str(img_path)))\n",
    "    return pred_class, pred_idx, outputs\n",
    "\n",
    "def encode_classes(pred_class_data):\n",
    "    pixels = np.concatenate([[0], torch.transpose(pred_class_data.squeeze(), 0, 1).flatten(), [0]])\n",
    "    classes_dict = {1: [], 2: [], 3: [], 4: []}\n",
    "    count = 0\n",
    "    previous = pixels[0]\n",
    "    for i, val in enumerate(pixels):\n",
    "        if val != previous:\n",
    "            if previous in classes_dict:\n",
    "                classes_dict[previous].append((i - count, count))\n",
    "            count = 0\n",
    "        previous = val\n",
    "        count += 1\n",
    "    return classes_dict\n",
    "\n",
    "\n",
    "def convert_classes_to_text(classes_dict, clazz):\n",
    "    return ' '.join([f'{v[0]} {v[1]}' for v in classes_dict[clazz]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def create_tta_predictions(start_pos, end_pos, model_to_load='export-4-best', append_to_file=False, submission_file='submission.csv'):\n",
    "    print(f'TTA prediction from {start_pos} to {end_pos}.')\n",
    "    bs = 4\n",
    "    src, data = create_data_bunch(bs, src_size, start_pos, end_pos)\n",
    "    metrics=accuracy_simple, acc_camvid_with_zero_check, dice_coefficient, dice_coefficient_2\n",
    "    learn = unet_learner(data, models.resnet34, metrics=metrics, wd=1e-2, bottle=True)\n",
    "    learn.loss_func = CombinedDiceLoss(zero_cat_factor=0.5)\n",
    "    learn.model_dir = Path('/kaggle/model')\n",
    "    learn.load(model_to_load);\n",
    "    # ys = final_preds\n",
    "    ys, y = learn.TTA(scale=1.1, ds_type=DatasetType.Test)\n",
    "    # get the actual predictions\n",
    "    pred_class = torch.argmax(ys, dim=1)\n",
    "    test_images = data.test_dl.dataset.items\n",
    "    assert pred_class.shape[0] == test_images.shape[0], f'{pred_class.shape[0]} != {test_images.shape[0]}'\n",
    "    test_images = [Path(f) for f in test_images]\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    defect_classes = [1, 2, 3, 4]\n",
    "    append_flag = 'a' if append_to_file else 'w'\n",
    "    with open(submission_file, append_flag) as submission_file:\n",
    "        if append_to_file:\n",
    "            submission_file.write('ImageId_ClassId,EncodedPixels\\n')\n",
    "        for i, test_image in enumerate(test_images):\n",
    "            encoded_all = encode_classes(pred_class[i])\n",
    "            for defect_class in defect_classes:\n",
    "                submission_file.write(f'{test_image.name}_{defect_class},{convert_classes_to_text(encoded_all, defect_class)}\\n')\n",
    "            if i % 5 == 0:\n",
    "                print(f'Processed {i} images\\r', end='')\n",
    "\n",
    "    print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4' class='' max='8', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      50.00% [4/8 04:04<04:04]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='226', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/226 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!rm submission.csv\n",
    "test_files = get_image_files(path/'test_images')\n",
    "batch_size = len(test_files) // 2\n",
    "create_tta_predictions(0, batch_size)\n",
    "create_tta_predictions(batch_size, len(test_files), append_to_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through the test images and create submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 114314 Oct 18 16:13 submission.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -latr 'submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804f41eb.jpg_1,\r\n",
      "1804f41eb.jpg_2,\r\n",
      "1804f41eb.jpg_3,\r\n",
      "1804f41eb.jpg_4,\r\n",
      "c90f155dd.jpg_1,\r\n",
      "c90f155dd.jpg_2,\r\n",
      "c90f155dd.jpg_3,\r\n",
      "c90f155dd.jpg_4,\r\n",
      "e0b422958.jpg_1,\r\n",
      "e0b422958.jpg_2,\r\n",
      "e0b422958.jpg_3,\r\n",
      "e0b422958.jpg_4,\r\n",
      "a631d53aa.jpg_1,\r\n",
      "a631d53aa.jpg_2,\r\n",
      "a631d53aa.jpg_3,\r\n",
      "a631d53aa.jpg_4,\r\n",
      "d01da361f.jpg_1,\r\n",
      "d01da361f.jpg_2,\r\n",
      "d01da361f.jpg_3,\r\n",
      "d01da361f.jpg_4,\r\n",
      "86fe3cf8c.jpg_1,\r\n",
      "86fe3cf8c.jpg_2,\r\n",
      "86fe3cf8c.jpg_3,\r\n",
      "86fe3cf8c.jpg_4,\r\n",
      "54eb4b690.jpg_1,\r\n",
      "54eb4b690.jpg_2,\r\n",
      "54eb4b690.jpg_3,\r\n",
      "54eb4b690.jpg_4,\r\n",
      "2efa6b22f.jpg_1,\r\n",
      "2efa6b22f.jpg_2,\r\n",
      "2efa6b22f.jpg_3,\r\n",
      "2efa6b22f.jpg_4,\r\n",
      "d6128fbfc.jpg_1,\r\n",
      "d6128fbfc.jpg_2,\r\n",
      "d6128fbfc.jpg_3,\r\n",
      "d6128fbfc.jpg_4,\r\n",
      "f625f93a1.jpg_1,\r\n",
      "f625f93a1.jpg_2,\r\n",
      "f625f93a1.jpg_3,\r\n",
      "f625f93a1.jpg_4,\r\n",
      "499a9893b.jpg_1,\r\n",
      "499a9893b.jpg_2,\r\n",
      "499a9893b.jpg_3,\r\n",
      "499a9893b.jpg_4,\r\n",
      "4c5671c92.jpg_1,\r\n",
      "4c5671c92.jpg_2,\r\n",
      "4c5671c92.jpg_3,212130 1 212133 2 212141 7 212319 1 212321 2 212354 1 212362 1 212366 1 212382 1 212385 2 212389 3 212395 10 212566 1 212569 2 212573 10 212585 1 212609 2 212614 5 212634 1 212637 23 212662 1 212817 2 212821 22 212847 1 212849 2 212855 1 212857 2 212862 5 212869 6 212876 6 212886 1 212889 2 212893 24 212918 1 213070 69 213140 7 213149 22 213172 1 213175 1 213177 7 213185 4 213190 1 213258 5 213266 2 213320 1 213322 107 213430 21 213513 8 213522 5 213573 134 213708 1 213710 1 213769 6 213776 1 213778 3 213829 139 213969 2 214025 14 214050 1 214058 1 214066 1 214069 162 214281 8 214290 4 214322 1 214325 4 214330 1 214333 2 214336 149 214541 3 214581 3 214589 3 214595 1 214597 144 214797 1 214857 34 214892 105 215052 5 215091 4 215106 147 215373 26 215409 2 215414 29 215444 3 215448 3 215452 18 215473 6 215480 27 215508 1 215631 5 215637 20 215677 16 215694 4 215703 2 215710 13 215727 35 215997 2 216000 3\r\n",
      "4c5671c92.jpg_4,\r\n",
      "f05581eef.jpg_1,\r\n",
      "f05581eef.jpg_2,\r\n",
      "f05581eef.jpg_3,\r\n",
      "f05581eef.jpg_4,\r\n",
      "d04a7fb6c.jpg_1,\r\n",
      "d04a7fb6c.jpg_2,\r\n",
      "d04a7fb6c.jpg_3,\r\n",
      "d04a7fb6c.jpg_4,\r\n",
      "7dd388867.jpg_1,\r\n",
      "7dd388867.jpg_2,\r\n",
      "7dd388867.jpg_3,\r\n",
      "7dd388867.jpg_4,\r\n",
      "7dd7fc396.jpg_1,\r\n",
      "7dd7fc396.jpg_2,\r\n",
      "7dd7fc396.jpg_3,\r\n",
      "7dd7fc396.jpg_4,\r\n",
      "2a830069f.jpg_1,\r\n",
      "2a830069f.jpg_2,\r\n",
      "2a830069f.jpg_3,\r\n",
      "2a830069f.jpg_4,\r\n",
      "44d86ff71.jpg_1,\r\n",
      "44d86ff71.jpg_2,\r\n",
      "44d86ff71.jpg_3,\r\n",
      "44d86ff71.jpg_4,\r\n",
      "6d29df253.jpg_1,\r\n",
      "6d29df253.jpg_2,\r\n",
      "6d29df253.jpg_3,\r\n",
      "6d29df253.jpg_4,\r\n",
      "c86f91dbf.jpg_1,\r\n",
      "c86f91dbf.jpg_2,\r\n",
      "c86f91dbf.jpg_3,\r\n",
      "c86f91dbf.jpg_4,\r\n",
      "674ecec48.jpg_1,\r\n",
      "674ecec48.jpg_2,\r\n",
      "674ecec48.jpg_3,\r\n",
      "674ecec48.jpg_4,\r\n",
      "541e5bfec.jpg_1,\r\n",
      "541e5bfec.jpg_2,\r\n",
      "541e5bfec.jpg_3,\r\n",
      "541e5bfec.jpg_4,\r\n",
      "67fd6ce4d.jpg_1,\r\n",
      "67fd6ce4d.jpg_2,\r\n",
      "67fd6ce4d.jpg_3,\r\n",
      "67fd6ce4d.jpg_4,\r\n",
      "9d81c8973.jpg_1,\r\n",
      "9d81c8973.jpg_2,\r\n",
      "9d81c8973.jpg_3,\r\n",
      "9d81c8973.jpg_4,\r\n",
      "d89072b2f.jpg_1,\r\n",
      "d89072b2f.jpg_2,\r\n",
      "d89072b2f.jpg_3,\r\n",
      "d89072b2f.jpg_4,\r\n",
      "443d3ddd0.jpg_1,\r\n",
      "443d3ddd0.jpg_2,\r\n",
      "443d3ddd0.jpg_3,\r\n",
      "443d3ddd0.jpg_4,\r\n",
      "d0d0681d3.jpg_1,\r\n",
      "d0d0681d3.jpg_2,\r\n",
      "d0d0681d3.jpg_3,\r\n",
      "d0d0681d3.jpg_4,\r\n",
      "2233caaaf.jpg_1,\r\n",
      "2233caaaf.jpg_2,\r\n",
      "2233caaaf.jpg_3,\r\n",
      "2233caaaf.jpg_4,\r\n",
      "e237a3954.jpg_1,\r\n",
      "e237a3954.jpg_2,\r\n",
      "e237a3954.jpg_3,\r\n",
      "e237a3954.jpg_4,\r\n",
      "492e3ed43.jpg_1,\r\n",
      "492e3ed43.jpg_2,\r\n",
      "492e3ed43.jpg_3,\r\n",
      "492e3ed43.jpg_4,\r\n",
      "c1fac7911.jpg_1,\r\n",
      "c1fac7911.jpg_2,\r\n",
      "c1fac7911.jpg_3,\r\n",
      "c1fac7911.jpg_4,\r\n",
      "28c2b84a3.jpg_1,\r\n",
      "28c2b84a3.jpg_2,\r\n",
      "28c2b84a3.jpg_3,\r\n",
      "28c2b84a3.jpg_4,\r\n",
      "ab3a9a988.jpg_1,\r\n",
      "ab3a9a988.jpg_2,\r\n",
      "ab3a9a988.jpg_3,\r\n",
      "ab3a9a988.jpg_4,\r\n",
      "84e348664.jpg_1,\r\n",
      "84e348664.jpg_2,\r\n",
      "84e348664.jpg_3,\r\n",
      "84e348664.jpg_4,\r\n",
      "95eb51548.jpg_1,\r\n",
      "95eb51548.jpg_2,\r\n",
      "95eb51548.jpg_3,\r\n",
      "95eb51548.jpg_4,\r\n",
      "6a45da8e9.jpg_1,\r\n",
      "6a45da8e9.jpg_2,\r\n",
      "6a45da8e9.jpg_3,\r\n",
      "6a45da8e9.jpg_4,\r\n",
      "0e39b3fcc.jpg_1,\r\n",
      "0e39b3fcc.jpg_2,\r\n",
      "0e39b3fcc.jpg_3,\r\n",
      "0e39b3fcc.jpg_4,\r\n",
      "1c38cbe2a.jpg_1,\r\n",
      "1c38cbe2a.jpg_2,\r\n",
      "1c38cbe2a.jpg_3,\r\n",
      "1c38cbe2a.jpg_4,\r\n",
      "e120be595.jpg_1,\r\n",
      "e120be595.jpg_2,\r\n",
      "e120be595.jpg_3,\r\n",
      "e120be595.jpg_4,\r\n",
      "ac290f376.jpg_1,\r\n",
      "ac290f376.jpg_2,\r\n",
      "ac290f376.jpg_3,\r\n",
      "ac290f376.jpg_4,\r\n",
      "4d429c29f.jpg_1,\r\n",
      "4d429c29f.jpg_2,\r\n",
      "4d429c29f.jpg_3,\r\n",
      "4d429c29f.jpg_4,\r\n",
      "06dc73f58.jpg_1,\r\n",
      "06dc73f58.jpg_2,\r\n",
      "06dc73f58.jpg_3,\r\n",
      "06dc73f58.jpg_4,\r\n",
      "ea2ae621a.jpg_1,\r\n",
      "ea2ae621a.jpg_2,\r\n",
      "ea2ae621a.jpg_3,\r\n",
      "ea2ae621a.jpg_4,\r\n",
      "0384b28ff.jpg_1,\r\n",
      "0384b28ff.jpg_2,\r\n",
      "0384b28ff.jpg_3,\r\n",
      "0384b28ff.jpg_4,\r\n",
      "3b2a280dd.jpg_1,\r\n",
      "3b2a280dd.jpg_2,\r\n",
      "3b2a280dd.jpg_3,\r\n",
      "3b2a280dd.jpg_4,\r\n",
      "690fbb138.jpg_1,\r\n",
      "690fbb138.jpg_2,\r\n",
      "690fbb138.jpg_3,\r\n",
      "690fbb138.jpg_4,\r\n",
      "cdfb634a5.jpg_1,\r\n",
      "cdfb634a5.jpg_2,\r\n",
      "cdfb634a5.jpg_3,\r\n",
      "cdfb634a5.jpg_4,\r\n",
      "335d78292.jpg_1,\r\n",
      "335d78292.jpg_2,\r\n",
      "335d78292.jpg_3,\r\n",
      "335d78292.jpg_4,\r\n",
      "54a757589.jpg_1,\r\n",
      "54a757589.jpg_2,\r\n",
      "54a757589.jpg_3,\r\n",
      "54a757589.jpg_4,\r\n",
      "938120712.jpg_1,\r\n",
      "938120712.jpg_2,\r\n",
      "938120712.jpg_3,\r\n",
      "938120712.jpg_4,\r\n"
     ]
    }
   ],
   "source": [
    "!head -n200 'submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative prediction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,y = learn.get_preds(ds_type=DatasetType.Test, with_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class_data = preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((path/'test_images').ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test_ds.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking encoding methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_all = encode_classes(pred_class.data)\n",
    "print(convert_classes_to_text(encoded_all, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = train_images[16]\n",
    "print(get_y_fn(image_name))\n",
    "img = open_mask(get_y_fn(image_name))\n",
    "img_data = img.data\n",
    "print(convert_classes_to_text(encode_classes(img_data), 3))\n",
    "img_data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
