{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from fastai.vision import *\n",
    "from fastai import *\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from fastai.vision.models.cadene_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = pd.read_csv('/root/.fastai/data/severstal/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId_ClassId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002cc93b.jpg_1</td>\n",
       "      <td>29102 12 29346 24 29602 24 29858 24 30114 24 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002cc93b.jpg_2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002cc93b.jpg_3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002cc93b.jpg_4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00031f466.jpg_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId_ClassId                                      EncodedPixels\n",
       "0  0002cc93b.jpg_1  29102 12 29346 24 29602 24 29858 24 30114 24 3...\n",
       "1  0002cc93b.jpg_2                                                NaN\n",
       "2  0002cc93b.jpg_3                                                NaN\n",
       "3  0002cc93b.jpg_4                                                NaN\n",
       "4  00031f466.jpg_1                                                NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/root/.fastai/data/severstal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/root/.fastai/data/severstal/train_images.zip'),\n",
       " PosixPath('/root/.fastai/data/severstal/sample_submission.csv'),\n",
       " PosixPath('/root/.fastai/data/severstal/test_images.zip'),\n",
       " PosixPath('/root/.fastai/data/severstal/train.csv'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images'),\n",
       " PosixPath('/root/.fastai/data/severstal/test_images')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/root/.fastai/data/severstal/train_images/5e581254c.jpg'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images/fd2f7b4f4.jpg'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images/82f4c0b69.jpg')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = get_image_files(path/'train_images')\n",
    "train_images[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check maximum size of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_img_max_size(folder):\n",
    "    max_height = 0\n",
    "    max_width = 0\n",
    "    for train_image in train_images:\n",
    "        img = open_image(train_image)\n",
    "        if max_height < img.shape[1]:\n",
    "            max_height = img.shape[1]\n",
    "        if max_width < img.shape[2]:\n",
    "            max_width = img.shape[2]\n",
    "    return max_height, max_width\n",
    "\n",
    "def show_image(images, index):\n",
    "    img_f = images[index]\n",
    "    print(type(img_f))\n",
    "    img = open_image(img_f)\n",
    "    print(img)\n",
    "    img.show(figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_path = Path('/kaggle/mask')\n",
    "if not os.path.exists(mask_path):\n",
    "    os.makedirs(str(mask_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_encoded_to_array(encoded_pixels):\n",
    "    pos_array = []\n",
    "    len_array = []\n",
    "    splits = encoded_pixels.split()\n",
    "    pos_array = [int(n) - 1 for i, n in enumerate(splits) if i % 2 == 0]\n",
    "    len_array = [int(n) for i, n in enumerate(splits) if i % 2 == 1]\n",
    "    return pos_array, len_array\n",
    "        \n",
    "def convert_to_pair(pos_array, rows):\n",
    "    return [(p % rows, p // rows) for p in pos_array]\n",
    "\n",
    "def create_positions(single_pos, size):\n",
    "    return [i for i in range(single_pos, single_pos + size)]\n",
    "\n",
    "def create_positions_pairs(single_pos, size, row_size):\n",
    "    return convert_to_pair(create_positions(single_pos, size), row_size)\n",
    "\n",
    "def convert_to_mask(encoded_pixels, row_size, col_size, category):\n",
    "    pos_array, len_array = convert_encoded_to_array(encoded_pixels)\n",
    "    mask = np.zeros([row_size, col_size])\n",
    "    for(p, l) in zip(pos_array, len_array):\n",
    "        for row, col in create_positions_pairs(p, l, row_size):\n",
    "            mask[row][col] = category\n",
    "    return mask\n",
    "\n",
    "def save_to_image(masked, image_name):\n",
    "    im = PIL.Image.fromarray(masked)\n",
    "    im = im.convert(\"L\")\n",
    "    image_name = re.sub(r'(.+)\\.jpg', r'\\1', image_name) + \".png\"\n",
    "    real_path = mask_path/image_name\n",
    "    im.save(real_path)\n",
    "    return real_path\n",
    "\n",
    "def open_single_image(path):\n",
    "    img = open_image(path)\n",
    "    img.show(figsize=(20,20))\n",
    "    \n",
    "def get_y_fn(x):\n",
    "    return mask_path/(x.stem + '.png')\n",
    "\n",
    "def group_by(train_images, train_pd):\n",
    "    tran_dict = {image.name:[] for image in train_images}\n",
    "    pattern = re.compile('(.+)_(\\d+)')\n",
    "    for index, image_path in train_pd.iterrows():\n",
    "        m = pattern.match(image_path['ImageId_ClassId'])\n",
    "        file_name = m.group(1)\n",
    "        category = m.group(2)\n",
    "        tran_dict[file_name].append((int(category), image_path['EncodedPixels']))\n",
    "    return tran_dict\n",
    "\n",
    "def display_image_with_mask(img_name):\n",
    "    full_image = path/'train_images'/img_name\n",
    "    print(full_image)\n",
    "    open_single_image(full_image)\n",
    "    mask_image = get_y_fn(full_image)\n",
    "    mask = open_mask(mask_image)\n",
    "    print(full_image)\n",
    "    mask.show(figsize=(20, 20), alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_categories_mask = group_by(train_images, train_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create mask files and save these to kaggle/mask/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 256\n",
    "image_width = 1600\n",
    "if not os.path.exists(mask_path/'0002cc93b.png'):\n",
    "    for image_name, cat_list in grouped_categories_mask.items():\n",
    "        masked = np.zeros([image_height, image_width])\n",
    "        for cat_mask in cat_list:\n",
    "            encoded_pixels = cat_mask[1]\n",
    "            if pd.notna(cat_mask[1]):\n",
    "                masked += convert_to_mask(encoded_pixels, image_height, image_width, cat_mask[0])\n",
    "        if np.amax(masked) > 4:\n",
    "            print(f'Check {image_name} for max category {np.amax(masked)}')\n",
    "        save_to_image(masked, image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limited_dihedral_affine(k:partial(uniform_int,0,3)):\n",
    "    \"Randomly flip `x` image based on `k`.\"\n",
    "    x = -1 if k&1 else 1\n",
    "    y = -1 if k&2 else 1\n",
    "    if k&4: return [[0, x, 0.],\n",
    "                    [y, 0, 0],\n",
    "                    [0, 0, 1.]]\n",
    "    return [[x, 0, 0.],\n",
    "            [0, y, 0],\n",
    "            [0, 0, 1.]]\n",
    "\n",
    "dihedral_affine = TfmAffine(limited_dihedral_affine)\n",
    "\n",
    "def get_extra_transforms(max_rotate:float=3., max_zoom:float=1.1,\n",
    "                   max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75,\n",
    "                   p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None)->Collection[Transform]:\n",
    "    \"Utility func to easily create a list of flip, rotate, `zoom`, warp, lighting transforms.\"\n",
    "    p_lightings = [p_lighting, p_lighting + 0.2, p_lighting + 0.4, p_lighting + 0.6, p_lighting + 0.7]\n",
    "    max_lightings = [max_lighting, max_lighting + 0.2, max_lighting + 0.4, max_lighting + 0.6, max_lighting + 0.7]\n",
    "    res = [rand_crop(), dihedral_affine(), \n",
    "           symmetric_warp(magnitude=(-max_warp,max_warp), p=p_affine),\n",
    "           rotate(degrees=(-max_rotate,max_rotate), p=p_affine),\n",
    "           rand_zoom(scale=(1., max_zoom), p=p_affine)]\n",
    "    res.extend([brightness(change=(0.5*(1-mp[0]), 0.5*(1+mp[0])), p=mp[1]) for mp in zip(max_lightings, p_lightings)])\n",
    "    res.extend([contrast(scale=(1-mp[0], 1/(1-mp[0])), p=mp[1]) for mp in zip(max_lightings, p_lightings)])\n",
    "    #       train                   , valid\n",
    "    return (res, [crop_pad()])\n",
    "\n",
    "def get_simple_transforms(max_rotate:float=3., max_zoom:float=1.1,\n",
    "                   max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75,\n",
    "                   p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None)->Collection[Transform]:\n",
    "    \"Utility func to easily create a list of flip, rotate, `zoom`, warp, lighting transforms.\"\n",
    "    res = [\n",
    "        rand_crop(),\n",
    "        symmetric_warp(magnitude=(-max_warp,max_warp), p=p_affine),\n",
    "        rotate(degrees=(-max_rotate,max_rotate), p=p_affine),\n",
    "        rand_zoom(scale=(1., max_zoom), p=p_affine)\n",
    "          ]\n",
    "    #       train                   , valid\n",
    "    return (res, [crop_pad()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = (path/'train_images').ls()\n",
    "src_size = np.array(open_image(str(train_images[0])).shape[1:])\n",
    "valid_pct = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = array(['0', '1', '2', '3', '4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_bunch(bs, size):\n",
    "    src = (SegmentationItemList.from_folder(path/'train_images')\n",
    "       .split_by_rand_pct(valid_pct=valid_pct)\n",
    "       .label_from_func(get_y_fn, classes=codes))\n",
    "    data = (src.transform(get_simple_transforms(), size=size, tfm_y=True)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))\n",
    "    return src, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "size = src_size//4\n",
    "src, data = create_data_bunch(bs, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create learner and training\n",
    "Starting with low resolution training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "name2id = {v:k for k,v in enumerate(codes)}\n",
    "void_code = name2id['0']\n",
    "\n",
    "def acc_camvid(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    mask = target != void_code\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    comparison = argmax[mask]==target[mask]\n",
    "    return torch.tensor(0.) if comparison.numel() == 0 else comparison.float().mean()\n",
    "\n",
    "def acc_camvid_with_zero_check(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        if(torch.sum(argmax[b]).item() == 0.0 and torch.sum(target[b]).item() == 0.0):\n",
    "            total[b] = 1\n",
    "        else:\n",
    "            mask = target[b] != void_code\n",
    "            comparison = argmax[b][mask]==target[b][mask]\n",
    "            total[b] = torch.tensor(0.) if comparison.numel() == 0 else comparison.float().mean()\n",
    "    return total.mean()\n",
    "\n",
    "\n",
    "def calc_dice_coefficients(argmax, target, cats):\n",
    "    def calc_dice_coefficient(seg, gt, cat: int):\n",
    "        mask_seg = seg == cat\n",
    "        mask_gt = gt == cat\n",
    "        sum_seg = torch.sum(mask_seg.float())\n",
    "        sum_gt = torch.sum(mask_gt.float())\n",
    "        if sum_seg + sum_gt == 0:\n",
    "            return torch.tensor(1.0)\n",
    "        return (torch.sum((seg[gt == cat] / cat).float()) * 2.0) / (sum_seg + sum_gt)\n",
    "\n",
    "    total_avg = torch.empty([len(cats)])\n",
    "    for i, c in enumerate(cats):\n",
    "        total_avg[i] = calc_dice_coefficient(argmax, target, c)\n",
    "    return total_avg.mean()\n",
    "\n",
    "\n",
    "def dice_coefficient(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    cats = [1, 2, 3, 4]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        total[b] = calc_dice_coefficients(argmax[b], target[b], cats)\n",
    "    return total.mean()\n",
    "\n",
    "def calc_dice_coefficients_2(argmax, target, cats):\n",
    "    def calc_dice_coefficient(seg, gt, cat: int):\n",
    "        mask_seg = seg == cat\n",
    "        mask_gt = gt == cat\n",
    "        sum_seg = torch.sum(mask_seg.float())\n",
    "        sum_gt = torch.sum(mask_gt.float())\n",
    "        return (torch.sum((seg[gt == cat] / cat).float())), (sum_seg + sum_gt)\n",
    "\n",
    "    total_avg = torch.empty([len(cats), 2])\n",
    "    for i, c in enumerate(cats):\n",
    "        total_avg[i][0], total_avg[i][1] = calc_dice_coefficient(argmax, target, c)\n",
    "    total_sum = total_avg.sum(axis=0)\n",
    "    if (total_sum[1] == 0.0):\n",
    "        return torch.tensor(1.0)\n",
    "    return total_sum[0] * 2.0 / total_sum[1]\n",
    "\n",
    "\n",
    "def dice_coefficient_2(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    cats = [1, 2, 3, 4]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        total[b] = calc_dice_coefficients_2(argmax[b], target[b], cats)\n",
    "    return total.mean()\n",
    "\n",
    "\n",
    "def accuracy_simple(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    return (input.argmax(dim=1)==target).float().mean()\n",
    "\n",
    "\n",
    "def dice_coeff(pred, target):\n",
    "    smooth = 1.\n",
    "    num = pred.size(0)\n",
    "    m1 = pred.view(num, -1)  # Flatten\n",
    "    m2 = target.view(num, -1)  # Flatten\n",
    "    intersection = (m1 * m2).sum()\n",
    "    return (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import callbacks\n",
    "\n",
    "def train_learner(learn, slice_lr, epochs=10, pct_start=0.8, best_model_name='best_model', \n",
    "                  patience_early_stop=4, patience_reduce_lr = 3):\n",
    "    learn.fit_one_cycle(epochs, slice_lr, pct_start=pct_start, \n",
    "                    callbacks=[callbacks.SaveModelCallback(learn, monitor='dice_coefficient',mode='max', name=best_model_name),\n",
    "                              callbacks.EarlyStoppingCallback(learn=learn, monitor='dice_coefficient', patience=patience_early_stop),\n",
    "                              callbacks.ReduceLROnPlateauCallback(learn=learn, monitor='dice_coefficient', patience=patience_reduce_lr),\n",
    "                              callbacks.TerminateOnNaNCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=accuracy_simple, acc_camvid_with_zero_check, dice_coefficient, dice_coefficient_2\n",
    "wd=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlattenedLoss of CrossEntropyLoss()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = unet_learner(data, models.cadene_models.se_resnext101_32x4d, metrics=metrics, wd=wd, bottle=True)\n",
    "learn.loss_func = CrossEntropyFlat(axis=1, weight=torch.tensor([1.0, .5, .5, .5, .5]).cuda())\n",
    "learn.loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model_dir = Path('/kaggle/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = to_fp16(learn, loss_scale=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_find(learn, num_it=400)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_simple</th>\n",
       "      <th>acc_camvid_with_zero_check</th>\n",
       "      <th>dice_coefficient</th>\n",
       "      <th>dice_coefficient_2</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.079465</td>\n",
       "      <td>0.070184</td>\n",
       "      <td>0.968949</td>\n",
       "      <td>0.282110</td>\n",
       "      <td>0.779290</td>\n",
       "      <td>0.305784</td>\n",
       "      <td>13:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.065962</td>\n",
       "      <td>0.067080</td>\n",
       "      <td>0.969753</td>\n",
       "      <td>0.406711</td>\n",
       "      <td>0.831279</td>\n",
       "      <td>0.425754</td>\n",
       "      <td>14:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.069439</td>\n",
       "      <td>0.058352</td>\n",
       "      <td>0.971552</td>\n",
       "      <td>0.369319</td>\n",
       "      <td>0.825838</td>\n",
       "      <td>0.403176</td>\n",
       "      <td>14:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.042994</td>\n",
       "      <td>0.059964</td>\n",
       "      <td>0.972395</td>\n",
       "      <td>0.488230</td>\n",
       "      <td>0.861900</td>\n",
       "      <td>0.516732</td>\n",
       "      <td>14:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.049287</td>\n",
       "      <td>0.053670</td>\n",
       "      <td>0.973012</td>\n",
       "      <td>0.488519</td>\n",
       "      <td>0.830538</td>\n",
       "      <td>0.527425</td>\n",
       "      <td>14:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.048172</td>\n",
       "      <td>0.052401</td>\n",
       "      <td>0.974216</td>\n",
       "      <td>0.481397</td>\n",
       "      <td>0.861550</td>\n",
       "      <td>0.529530</td>\n",
       "      <td>14:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.047702</td>\n",
       "      <td>0.046819</td>\n",
       "      <td>0.974215</td>\n",
       "      <td>0.526293</td>\n",
       "      <td>0.868534</td>\n",
       "      <td>0.569109</td>\n",
       "      <td>14:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.045501</td>\n",
       "      <td>0.046576</td>\n",
       "      <td>0.975839</td>\n",
       "      <td>0.547176</td>\n",
       "      <td>0.877475</td>\n",
       "      <td>0.591088</td>\n",
       "      <td>14:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>0.044861</td>\n",
       "      <td>0.976625</td>\n",
       "      <td>0.557457</td>\n",
       "      <td>0.880714</td>\n",
       "      <td>0.604801</td>\n",
       "      <td>14:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.038246</td>\n",
       "      <td>0.045160</td>\n",
       "      <td>0.976825</td>\n",
       "      <td>0.569423</td>\n",
       "      <td>0.882640</td>\n",
       "      <td>0.613236</td>\n",
       "      <td>14:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with dice_coefficient value: 0.7792901396751404.\n",
      "Better model found at epoch 1 with dice_coefficient value: 0.8312792181968689.\n",
      "Better model found at epoch 3 with dice_coefficient value: 0.8619004487991333.\n",
      "Better model found at epoch 6 with dice_coefficient value: 0.8685335516929626.\n",
      "Better model found at epoch 7 with dice_coefficient value: 0.8774750232696533.\n",
      "Better model found at epoch 8 with dice_coefficient value: 0.8807142972946167.\n",
      "Better model found at epoch 9 with dice_coefficient value: 0.8826403617858887.\n"
     ]
    }
   ],
   "source": [
    "train_learner(learn, slice(lr), epochs=10, pct_start=0.8, best_model_name='bestmodel-frozen-1', \n",
    "              patience_early_stop=4, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('bestmodel-frozen-1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.export(file='/kaggle/model/export-1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = slice(lr/100,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_simple</th>\n",
       "      <th>acc_camvid_with_zero_check</th>\n",
       "      <th>dice_coefficient</th>\n",
       "      <th>dice_coefficient_2</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.036766</td>\n",
       "      <td>0.043039</td>\n",
       "      <td>0.977224</td>\n",
       "      <td>0.576937</td>\n",
       "      <td>0.884602</td>\n",
       "      <td>0.624022</td>\n",
       "      <td>17:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.032017</td>\n",
       "      <td>0.044012</td>\n",
       "      <td>0.977054</td>\n",
       "      <td>0.576920</td>\n",
       "      <td>0.887595</td>\n",
       "      <td>0.627352</td>\n",
       "      <td>17:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.036470</td>\n",
       "      <td>0.044755</td>\n",
       "      <td>0.976361</td>\n",
       "      <td>0.570400</td>\n",
       "      <td>0.889218</td>\n",
       "      <td>0.622423</td>\n",
       "      <td>17:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.039023</td>\n",
       "      <td>0.044083</td>\n",
       "      <td>0.975924</td>\n",
       "      <td>0.572087</td>\n",
       "      <td>0.890352</td>\n",
       "      <td>0.619576</td>\n",
       "      <td>17:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037332</td>\n",
       "      <td>0.043268</td>\n",
       "      <td>0.977742</td>\n",
       "      <td>0.584112</td>\n",
       "      <td>0.891151</td>\n",
       "      <td>0.636069</td>\n",
       "      <td>17:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.038090</td>\n",
       "      <td>0.041199</td>\n",
       "      <td>0.978212</td>\n",
       "      <td>0.606827</td>\n",
       "      <td>0.890613</td>\n",
       "      <td>0.650380</td>\n",
       "      <td>17:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.034170</td>\n",
       "      <td>0.047464</td>\n",
       "      <td>0.975795</td>\n",
       "      <td>0.568621</td>\n",
       "      <td>0.879348</td>\n",
       "      <td>0.611216</td>\n",
       "      <td>17:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.034824</td>\n",
       "      <td>0.041910</td>\n",
       "      <td>0.977698</td>\n",
       "      <td>0.592473</td>\n",
       "      <td>0.891336</td>\n",
       "      <td>0.644190</td>\n",
       "      <td>17:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.031516</td>\n",
       "      <td>0.041058</td>\n",
       "      <td>0.977192</td>\n",
       "      <td>0.580247</td>\n",
       "      <td>0.892387</td>\n",
       "      <td>0.633142</td>\n",
       "      <td>17:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.031415</td>\n",
       "      <td>0.042716</td>\n",
       "      <td>0.977605</td>\n",
       "      <td>0.589519</td>\n",
       "      <td>0.891461</td>\n",
       "      <td>0.641481</td>\n",
       "      <td>17:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with dice_coefficient value: 0.8846023678779602.\n",
      "Better model found at epoch 1 with dice_coefficient value: 0.8875951170921326.\n",
      "Better model found at epoch 2 with dice_coefficient value: 0.889218270778656.\n",
      "Better model found at epoch 3 with dice_coefficient value: 0.8903520703315735.\n",
      "Better model found at epoch 4 with dice_coefficient value: 0.8911510109901428.\n",
      "Better model found at epoch 7 with dice_coefficient value: 0.8913360238075256.\n",
      "Better model found at epoch 8 with dice_coefficient value: 0.8923865556716919.\n"
     ]
    }
   ],
   "source": [
    "train_learner(learn, lrs, epochs=10, pct_start=0.8, best_model_name='bestmodel-unfrozen-1', \n",
    "              patience_early_stop=4, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'DynamicUnet.__init__.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2fd3fff21061>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/kaggle/model/export-2.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, file, destroy)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mxtra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cls'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mtry_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdestroy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/fastai/torch_core.py\u001b[0m in \u001b[0;36mtry_save\u001b[0;34m(state, path, file)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtry_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mPathLikeOrBinaryStream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_pathlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{e}\\n Can't write {path/file}. Pass an absolute writable pathlib obj `fname`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \"\"\"\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \"\"\"\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0mserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_storages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'DynamicUnet.__init__.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "learn.export(file='/kaggle/model/export-2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn=None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_large_learner(bs=4, size=size, transform_func=get_simple_transforms, model_to_load='bestmodel-unfrozen-1'):\n",
    "    data = (src.transform(transform_func(), size=size, tfm_y=True)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))\n",
    "    learn = unet_learner(data, models.cadene_models.se_resnext101_32x4d, metrics=metrics, wd=wd, bottle=True)\n",
    "    learn.model_dir = Path('/kaggle/model')\n",
    "    learn.loss_func = CrossEntropyFlat(axis=1, weight=torch.tensor([1.5, .5, .5, .5, .5]).cuda())\n",
    "    learn = to_fp16(learn, loss_scale=8.0)\n",
    "    if model_to_load is not None:\n",
    "        learn.load(model_to_load)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_large_learner(bs=bs, size=src_size, transform_func=get_simple_transforms, model_to_load='bestmodel-unfrozen-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_find(learn, num_it=400)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=6e-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      60.00% [3/5 4:40:00<3:06:40]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_simple</th>\n",
       "      <th>acc_camvid_with_zero_check</th>\n",
       "      <th>dice_coefficient</th>\n",
       "      <th>dice_coefficient_2</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.051314</td>\n",
       "      <td>0.055195</td>\n",
       "      <td>0.968907</td>\n",
       "      <td>0.339916</td>\n",
       "      <td>0.810896</td>\n",
       "      <td>0.372016</td>\n",
       "      <td>1:33:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.058454</td>\n",
       "      <td>0.054237</td>\n",
       "      <td>0.969428</td>\n",
       "      <td>0.421480</td>\n",
       "      <td>0.846516</td>\n",
       "      <td>0.454154</td>\n",
       "      <td>1:33:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.068239</td>\n",
       "      <td>0.053321</td>\n",
       "      <td>0.970236</td>\n",
       "      <td>0.431206</td>\n",
       "      <td>0.850170</td>\n",
       "      <td>0.461081</td>\n",
       "      <td>1:32:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='9760' class='' max='11312', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      86.28% [9760/11312 1:17:23<12:18 0.0661]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with dice_coefficient value: 0.8108955025672913.\n",
      "Better model found at epoch 1 with dice_coefficient value: 0.8465156555175781.\n",
      "Better model found at epoch 2 with dice_coefficient value: 0.8501698970794678.\n"
     ]
    }
   ],
   "source": [
    "train_learner(learn, slice(lr), epochs=5, pct_start=0.8, best_model_name='bestmodel-frozen-3', \n",
    "              patience_early_stop=4, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-3');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('bestmodel-frozen-3');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(file='/kaggle/model/export-3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_large_learner(bs=bs, transform_func=get_extra_transforms, model_to_load='bestmodel-frozen-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = slice(lr/1000,lr/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_learner(learn, lrs, epochs=10, pct_start=0.8, best_model_name='bestmodel-4', \n",
    "              patience_early_stop=5, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-4');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('bestmodel-4');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(file='/kaggle/model/export-4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!cp /kaggle/model/export.pkl /opt/fastai/fastai-exercises/nbs_gil\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'export-4.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn=None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = (path/'test_images').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_learn = load_learner('/kaggle/model/', file='export-2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_learn = to_fp16(inference_learn, loss_scale=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_path):\n",
    "    pred_class, pred_idx, outputs = inference_learn.predict(open_image(str(img_path)))\n",
    "    return pred_class, pred_idx, outputs\n",
    "\n",
    "def encode_classes(pred_class_data):\n",
    "    pixels = np.concatenate([[0], torch.transpose(pred_class_data.squeeze(), 0, 1).flatten(), [0]])\n",
    "    classes_dict = {1: [], 2: [], 3: [], 4: []}\n",
    "    count = 0\n",
    "    previous = pixels[0]\n",
    "    for i, val in enumerate(pixels):\n",
    "        if val != previous:\n",
    "            if previous in classes_dict:\n",
    "                classes_dict[previous].append((i - count, count))\n",
    "            count = 0\n",
    "        previous = val\n",
    "        count += 1\n",
    "    return classes_dict\n",
    "\n",
    "\n",
    "def convert_classes_to_text(classes_dict, clazz):\n",
    "    return ' '.join([f'{v[0]} {v[1]}' for v in classes_dict[clazz]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_to_predict = train_images[16].name\n",
    "display_image_with_mask(image_to_predict)\n",
    "pred_class, pred_idx, outputs = predict(path/f'train_images/{image_to_predict}')\n",
    "pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(pred_class.data.squeeze(), 0, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking encoding methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_all = encode_classes(pred_class.data)\n",
    "print(convert_classes_to_text(encoded_all, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = train_images[16]\n",
    "print(get_y_fn(image_name))\n",
    "img = open_mask(get_y_fn(image_name))\n",
    "img_data = img.data\n",
    "print(convert_classes_to_text(encode_classes(img_data), 3))\n",
    "img_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through the test images and create submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "defect_classes = [1, 2, 3, 4]\n",
    "with open('submission.csv', 'w') as submission_file:\n",
    "    submission_file.write('ImageId_ClassId,EncodedPixels\\n')\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        pred_class, pred_idx, outputs = predict(test_image)\n",
    "        encoded_all = encode_classes(pred_class.data)\n",
    "        for defect_class in defect_classes:\n",
    "            submission_file.write(f'{test_image.name}_{defect_class},{convert_classes_to_text(encoded_all, defect_class)}\\n')\n",
    "        if i % 5 == 0:\n",
    "            print(f'Processed {i} images\\r', end='')\n",
    "            \n",
    "print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative prediction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,y = learn.get_preds(ds_type=DatasetType.Test, with_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class_data = preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((path/'test_images').ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test_ds.x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
