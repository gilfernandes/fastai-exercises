{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from fastai.vision import *\n",
    "from fastai import *\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from fastai.vision.models.cadene_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = pd.read_csv('/root/.fastai/data/severstal/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId_ClassId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002cc93b.jpg_1</td>\n",
       "      <td>29102 12 29346 24 29602 24 29858 24 30114 24 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002cc93b.jpg_2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002cc93b.jpg_3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002cc93b.jpg_4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00031f466.jpg_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId_ClassId                                      EncodedPixels\n",
       "0  0002cc93b.jpg_1  29102 12 29346 24 29602 24 29858 24 30114 24 3...\n",
       "1  0002cc93b.jpg_2                                                NaN\n",
       "2  0002cc93b.jpg_3                                                NaN\n",
       "3  0002cc93b.jpg_4                                                NaN\n",
       "4  00031f466.jpg_1                                                NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/root/.fastai/data/severstal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/root/.fastai/data/severstal/train_images.zip'),\n",
       " PosixPath('/root/.fastai/data/severstal/sample_submission.csv'),\n",
       " PosixPath('/root/.fastai/data/severstal/test_images.zip'),\n",
       " PosixPath('/root/.fastai/data/severstal/train.csv'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images'),\n",
       " PosixPath('/root/.fastai/data/severstal/test_images')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/root/.fastai/data/severstal/train_images/5e581254c.jpg'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images/fd2f7b4f4.jpg'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images/82f4c0b69.jpg')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = get_image_files(path/'train_images')\n",
    "train_images[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check maximum size of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_img_max_size(folder):\n",
    "    max_height = 0\n",
    "    max_width = 0\n",
    "    for train_image in train_images:\n",
    "        img = open_image(train_image)\n",
    "        if max_height < img.shape[1]:\n",
    "            max_height = img.shape[1]\n",
    "        if max_width < img.shape[2]:\n",
    "            max_width = img.shape[2]\n",
    "    return max_height, max_width\n",
    "\n",
    "def show_image(images, index):\n",
    "    img_f = images[index]\n",
    "    print(type(img_f))\n",
    "    img = open_image(img_f)\n",
    "    print(img)\n",
    "    img.show(figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_path = Path('/kaggle/mask')\n",
    "if not os.path.exists(mask_path):\n",
    "    os.makedirs(str(mask_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_encoded_to_array(encoded_pixels):\n",
    "    pos_array = []\n",
    "    len_array = []\n",
    "    splits = encoded_pixels.split()\n",
    "    pos_array = [int(n) - 1 for i, n in enumerate(splits) if i % 2 == 0]\n",
    "    len_array = [int(n) for i, n in enumerate(splits) if i % 2 == 1]\n",
    "    return pos_array, len_array\n",
    "        \n",
    "def convert_to_pair(pos_array, rows):\n",
    "    return [(p % rows, p // rows) for p in pos_array]\n",
    "\n",
    "def create_positions(single_pos, size):\n",
    "    return [i for i in range(single_pos, single_pos + size)]\n",
    "\n",
    "def create_positions_pairs(single_pos, size, row_size):\n",
    "    return convert_to_pair(create_positions(single_pos, size), row_size)\n",
    "\n",
    "def convert_to_mask(encoded_pixels, row_size, col_size, category):\n",
    "    pos_array, len_array = convert_encoded_to_array(encoded_pixels)\n",
    "    mask = np.zeros([row_size, col_size])\n",
    "    for(p, l) in zip(pos_array, len_array):\n",
    "        for row, col in create_positions_pairs(p, l, row_size):\n",
    "            mask[row][col] = category\n",
    "    return mask\n",
    "\n",
    "def save_to_image(masked, image_name):\n",
    "    im = PIL.Image.fromarray(masked)\n",
    "    im = im.convert(\"L\")\n",
    "    image_name = re.sub(r'(.+)\\.jpg', r'\\1', image_name) + \".png\"\n",
    "    real_path = mask_path/image_name\n",
    "    im.save(real_path)\n",
    "    return real_path\n",
    "\n",
    "def open_single_image(path):\n",
    "    img = open_image(path)\n",
    "    img.show(figsize=(20,20))\n",
    "    \n",
    "def get_y_fn(x):\n",
    "    return mask_path/(x.stem + '.png')\n",
    "\n",
    "def group_by(train_images, train_pd):\n",
    "    tran_dict = {image.name:[] for image in train_images}\n",
    "    pattern = re.compile('(.+)_(\\d+)')\n",
    "    for index, image_path in train_pd.iterrows():\n",
    "        m = pattern.match(image_path['ImageId_ClassId'])\n",
    "        file_name = m.group(1)\n",
    "        category = m.group(2)\n",
    "        tran_dict[file_name].append((int(category), image_path['EncodedPixels']))\n",
    "    return tran_dict\n",
    "\n",
    "def display_image_with_mask(img_name):\n",
    "    full_image = path/'train_images'/img_name\n",
    "    print(full_image)\n",
    "    open_single_image(full_image)\n",
    "    mask_image = get_y_fn(full_image)\n",
    "    mask = open_mask(mask_image)\n",
    "    print(full_image)\n",
    "    mask.show(figsize=(20, 20), alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_categories_mask = group_by(train_images, train_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create mask files and save these to kaggle/mask/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 256\n",
    "image_width = 1600\n",
    "if not os.path.exists(mask_path/'0002cc93b.png'):\n",
    "    for image_name, cat_list in grouped_categories_mask.items():\n",
    "        masked = np.zeros([image_height, image_width])\n",
    "        for cat_mask in cat_list:\n",
    "            encoded_pixels = cat_mask[1]\n",
    "            if pd.notna(cat_mask[1]):\n",
    "                masked += convert_to_mask(encoded_pixels, image_height, image_width, cat_mask[0])\n",
    "        if np.amax(masked) > 4:\n",
    "            print(f'Check {image_name} for max category {np.amax(masked)}')\n",
    "        save_to_image(masked, image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limited_dihedral_affine(k:partial(uniform_int,0,3)):\n",
    "    \"Randomly flip `x` image based on `k`.\"\n",
    "    x = -1 if k&1 else 1\n",
    "    y = -1 if k&2 else 1\n",
    "    if k&4: return [[0, x, 0.],\n",
    "                    [y, 0, 0],\n",
    "                    [0, 0, 1.]]\n",
    "    return [[x, 0, 0.],\n",
    "            [0, y, 0],\n",
    "            [0, 0, 1.]]\n",
    "\n",
    "dihedral_affine = TfmAffine(limited_dihedral_affine)\n",
    "\n",
    "def get_extra_transforms(max_rotate:float=3., max_zoom:float=1.1,\n",
    "                   max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75,\n",
    "                   p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None)->Collection[Transform]:\n",
    "    \"Utility func to easily create a list of flip, rotate, `zoom`, warp, lighting transforms.\"\n",
    "    p_lightings = [p_lighting, p_lighting + 0.2, p_lighting + 0.4, p_lighting + 0.6, p_lighting + 0.7]\n",
    "    max_lightings = [max_lighting, max_lighting + 0.2, max_lighting + 0.4, max_lighting + 0.6, max_lighting + 0.7]\n",
    "    res = [rand_crop(), dihedral_affine(), \n",
    "           symmetric_warp(magnitude=(-max_warp,max_warp), p=p_affine),\n",
    "           rotate(degrees=(-max_rotate,max_rotate), p=p_affine),\n",
    "           rand_zoom(scale=(1., max_zoom), p=p_affine)]\n",
    "    res.extend([brightness(change=(0.5*(1-mp[0]), 0.5*(1+mp[0])), p=mp[1]) for mp in zip(max_lightings, p_lightings)])\n",
    "    res.extend([contrast(scale=(1-mp[0], 1/(1-mp[0])), p=mp[1]) for mp in zip(max_lightings, p_lightings)])\n",
    "    #       train                   , valid\n",
    "    return (res, [crop_pad()])\n",
    "\n",
    "def get_simple_transforms(max_rotate:float=3., max_zoom:float=1.1,\n",
    "                   max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75,\n",
    "                   p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None)->Collection[Transform]:\n",
    "    \"Utility func to easily create a list of flip, rotate, `zoom`, warp, lighting transforms.\"\n",
    "    res = [\n",
    "        rand_crop(),\n",
    "        symmetric_warp(magnitude=(-max_warp,max_warp), p=p_affine),\n",
    "        rotate(degrees=(-max_rotate,max_rotate), p=p_affine),\n",
    "        rand_zoom(scale=(1., max_zoom), p=p_affine)\n",
    "          ]\n",
    "    #       train                   , valid\n",
    "    return (res, [crop_pad()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = (path/'train_images').ls()\n",
    "src_size = np.array(open_image(str(train_images[0])).shape[1:])\n",
    "valid_pct = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = array(['0', '1', '2', '3', '4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_bunch(bs, size):\n",
    "    src = (SegmentationItemList.from_folder(path/'train_images')\n",
    "       .split_by_rand_pct(valid_pct=valid_pct)\n",
    "       .label_from_func(get_y_fn, classes=codes))\n",
    "    data = (src.transform(get_simple_transforms(), size=size, tfm_y=True)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))\n",
    "    return src, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "size = src_size//2\n",
    "src, data = create_data_bunch(bs, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create learner and training\n",
    "Starting with low resolution training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "name2id = {v:k for k,v in enumerate(codes)}\n",
    "void_code = name2id['0']\n",
    "\n",
    "def acc_camvid(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    mask = target != void_code\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    comparison = argmax[mask]==target[mask]\n",
    "    return torch.tensor(0.) if comparison.numel() == 0 else comparison.float().mean()\n",
    "\n",
    "def acc_camvid_with_zero_check(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        if(torch.sum(argmax[b]).item() == 0.0 and torch.sum(target[b]).item() == 0.0):\n",
    "            total[b] = 1\n",
    "        else:\n",
    "            mask = target[b] != void_code\n",
    "            comparison = argmax[b][mask]==target[b][mask]\n",
    "            total[b] = torch.tensor(0.) if comparison.numel() == 0 else comparison.float().mean()\n",
    "    return total.mean()\n",
    "\n",
    "\n",
    "def calc_dice_coefficients(argmax, target, cats):\n",
    "    def calc_dice_coefficient(seg, gt, cat: int):\n",
    "        mask_seg = seg == cat\n",
    "        mask_gt = gt == cat\n",
    "        sum_seg = torch.sum(mask_seg.float())\n",
    "        sum_gt = torch.sum(mask_gt.float())\n",
    "        if sum_seg + sum_gt == 0:\n",
    "            return torch.tensor(1.0)\n",
    "        return (torch.sum((seg[gt == cat] / cat).float()) * 2.0) / (sum_seg + sum_gt)\n",
    "\n",
    "    total_avg = torch.empty([len(cats)])\n",
    "    for i, c in enumerate(cats):\n",
    "        total_avg[i] = calc_dice_coefficient(argmax, target, c)\n",
    "    return total_avg.mean()\n",
    "\n",
    "\n",
    "def dice_coefficient(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    cats = [1, 2, 3, 4]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        total[b] = calc_dice_coefficients(argmax[b], target[b], cats)\n",
    "    return total.mean()\n",
    "\n",
    "def calc_dice_coefficients_2(argmax, target, cats):\n",
    "    def calc_dice_coefficient(seg, gt, cat: int):\n",
    "        mask_seg = seg == cat\n",
    "        mask_gt = gt == cat\n",
    "        sum_seg = torch.sum(mask_seg.float())\n",
    "        sum_gt = torch.sum(mask_gt.float())\n",
    "        return (torch.sum((seg[gt == cat] / cat).float())), (sum_seg + sum_gt)\n",
    "\n",
    "    total_avg = torch.empty([len(cats), 2])\n",
    "    for i, c in enumerate(cats):\n",
    "        total_avg[i][0], total_avg[i][1] = calc_dice_coefficient(argmax, target, c)\n",
    "    total_sum = total_avg.sum(axis=0)\n",
    "    if (total_sum[1] == 0.0):\n",
    "        return torch.tensor(1.0)\n",
    "    return total_sum[0] * 2.0 / total_sum[1]\n",
    "\n",
    "\n",
    "def dice_coefficient_2(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    cats = [1, 2, 3, 4]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        total[b] = calc_dice_coefficients_2(argmax[b], target[b], cats)\n",
    "    return total.mean()\n",
    "\n",
    "\n",
    "def accuracy_simple(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    return (input.argmax(dim=1)==target).float().mean()\n",
    "\n",
    "\n",
    "def dice_coeff(pred, target):\n",
    "    smooth = 1.\n",
    "    num = pred.size(0)\n",
    "    m1 = pred.view(num, -1)  # Flatten\n",
    "    m2 = target.view(num, -1)  # Flatten\n",
    "    intersection = (m1 * m2).sum()\n",
    "    return (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDiceLoss(nn.Module):\n",
    "    def __init__(self, zero_cat_factor=0.1):\n",
    "        super().__init__()\n",
    "        self.zero_cat_factor = zero_cat_factor\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return self.dice_loss(target, input, self.zero_cat_factor)\n",
    "\n",
    "    def dice_loss(self, target, output, eps=1e-7, zero_cat_factor=0.1):\n",
    "        '''\n",
    "        Soft dice loss calculation for arbitrary batch size, number of classes, and number of spatial dimensions.\n",
    "        Assumes the `channels_last` format.\n",
    "\n",
    "        # Arguments\n",
    "            target: b x 1 x X x Y( x Z...) ground truth\n",
    "            output: b x c x X x Y( x Z...) Network output, must sum to 1 over c channel (such as after softmax)\n",
    "            epsilon: Used for numerical stability to avoid divide by zero errors\n",
    "\n",
    "        # References\n",
    "            V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation\n",
    "            https://arxiv.org/abs/1606.04797\n",
    "            More details on Dice loss formulation\n",
    "            https://mediatum.ub.tum.de/doc/1395260/1395260.pdf (page 72)\n",
    "\n",
    "            Adapted from https://github.com/Lasagne/Recipes/issues/99#issuecomment-347775022\n",
    "        '''\n",
    "\n",
    "        # skip the batch and class axis for calculating Dice score\n",
    "        num_classes = output.shape[1]\n",
    "        y_true = F.one_hot(target.long().squeeze(), num_classes)\n",
    "        y_pred = F.softmax(output, dim=1).permute(0, 2, 3, 1)\n",
    "        y_true = y_true.type(y_pred.type())\n",
    "        y_true = y_true.permute(0, 3, 1, 2)\n",
    "        y_true[:,0,:] *= zero_cat_factor # Factor used to take power away from the zeroth category\n",
    "        y_true = y_true.permute(0, 2, 3, 1)\n",
    "        axes = tuple(range(1, len(y_pred.shape)-1))\n",
    "        numerator = 2. * torch.sum(y_pred * y_true, axes)\n",
    "        denominator = torch.sum(y_pred ** 2 + y_true ** 2, axes)\n",
    "        # When intersection and cardinality are all zero you have 100% score and not 0% score\n",
    "        # For this we use the eps parameter\n",
    "        loss_array = ((numerator + eps) / (denominator + eps))\n",
    "        loss_array = (loss_array).mean(dim=0)\n",
    "        return ((1 - torch.mean(loss_array)) + F.cross_entropy(output, target.squeeze())) / 2.\n",
    "\n",
    "    def __del__(self): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import callbacks\n",
    "\n",
    "def train_learner(learn, slice_lr, epochs=10, pct_start=0.8, best_model_name='best_model', \n",
    "                  patience_early_stop=4, patience_reduce_lr = 3):\n",
    "    learn.fit_one_cycle(epochs, slice_lr, pct_start=pct_start, \n",
    "                    callbacks=[callbacks.SaveModelCallback(learn, monitor='dice_coefficient',mode='max', name=best_model_name),\n",
    "                              callbacks.EarlyStoppingCallback(learn=learn, monitor='dice_coefficient', patience=patience_early_stop),\n",
    "                              callbacks.ReduceLROnPlateauCallback(learn=learn, monitor='dice_coefficient', patience=patience_reduce_lr),\n",
    "                              callbacks.TerminateOnNaNCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=accuracy_simple, acc_camvid_with_zero_check, dice_coefficient, dice_coefficient_2\n",
    "wd=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedDiceLoss()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd, bottle=True)\n",
    "# learn.loss_func = CrossEntropyFlat(axis=1, weight=torch.tensor([1.0, .5, .5, .5, .5]).cuda())\n",
    "learn.loss_func = CombinedDiceLoss(zero_cat_factor=0.5)\n",
    "learn.loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model_dir = Path('/kaggle/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn = to_fp16(learn, loss_scale=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_find(learn, num_it=400)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_simple</th>\n",
       "      <th>acc_camvid_with_zero_check</th>\n",
       "      <th>dice_coefficient</th>\n",
       "      <th>dice_coefficient_2</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.210704</td>\n",
       "      <td>0.203583</td>\n",
       "      <td>0.974005</td>\n",
       "      <td>0.556441</td>\n",
       "      <td>0.877445</td>\n",
       "      <td>0.564374</td>\n",
       "      <td>05:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.204501</td>\n",
       "      <td>0.201876</td>\n",
       "      <td>0.974453</td>\n",
       "      <td>0.540219</td>\n",
       "      <td>0.880200</td>\n",
       "      <td>0.571359</td>\n",
       "      <td>05:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.217851</td>\n",
       "      <td>0.180933</td>\n",
       "      <td>0.977930</td>\n",
       "      <td>0.567265</td>\n",
       "      <td>0.885530</td>\n",
       "      <td>0.608430</td>\n",
       "      <td>05:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.187363</td>\n",
       "      <td>0.181227</td>\n",
       "      <td>0.977313</td>\n",
       "      <td>0.566786</td>\n",
       "      <td>0.886843</td>\n",
       "      <td>0.615561</td>\n",
       "      <td>05:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.186578</td>\n",
       "      <td>0.169047</td>\n",
       "      <td>0.979720</td>\n",
       "      <td>0.630603</td>\n",
       "      <td>0.899377</td>\n",
       "      <td>0.649751</td>\n",
       "      <td>05:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.193060</td>\n",
       "      <td>0.158909</td>\n",
       "      <td>0.980248</td>\n",
       "      <td>0.675486</td>\n",
       "      <td>0.909994</td>\n",
       "      <td>0.689128</td>\n",
       "      <td>05:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.165434</td>\n",
       "      <td>0.161674</td>\n",
       "      <td>0.980629</td>\n",
       "      <td>0.688209</td>\n",
       "      <td>0.915096</td>\n",
       "      <td>0.705524</td>\n",
       "      <td>05:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.178435</td>\n",
       "      <td>0.159382</td>\n",
       "      <td>0.980107</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.903398</td>\n",
       "      <td>0.686789</td>\n",
       "      <td>05:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.163246</td>\n",
       "      <td>0.155871</td>\n",
       "      <td>0.980304</td>\n",
       "      <td>0.688581</td>\n",
       "      <td>0.911034</td>\n",
       "      <td>0.694085</td>\n",
       "      <td>05:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.156271</td>\n",
       "      <td>0.148171</td>\n",
       "      <td>0.981584</td>\n",
       "      <td>0.715938</td>\n",
       "      <td>0.921352</td>\n",
       "      <td>0.725638</td>\n",
       "      <td>05:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with dice_coefficient value: 0.8774449229240417.\n",
      "Better model found at epoch 1 with dice_coefficient value: 0.8802003264427185.\n",
      "Better model found at epoch 2 with dice_coefficient value: 0.8855295181274414.\n",
      "Better model found at epoch 3 with dice_coefficient value: 0.8868431448936462.\n",
      "Better model found at epoch 4 with dice_coefficient value: 0.8993771076202393.\n",
      "Better model found at epoch 5 with dice_coefficient value: 0.9099943041801453.\n",
      "Better model found at epoch 6 with dice_coefficient value: 0.91509610414505.\n",
      "Better model found at epoch 9 with dice_coefficient value: 0.9213515520095825.\n"
     ]
    }
   ],
   "source": [
    "train_learner(learn, slice(lr), epochs=10, pct_start=0.8, best_model_name='bestmodel-frozen-1', \n",
    "              patience_early_stop=4, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('bestmodel-frozen-1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type CombinedDiceLoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "learn.export(file='/kaggle/model/export-1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = slice(lr/100,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/10 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_simple</th>\n",
       "      <th>acc_camvid_with_zero_check</th>\n",
       "      <th>dice_coefficient</th>\n",
       "      <th>dice_coefficient_2</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1146' class='' max='2828', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      40.52% [1146/2828 02:15<03:18 0.1526]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_learner(learn, lrs, epochs=10, pct_start=0.8, best_model_name='bestmodel-unfrozen-1', \n",
    "              patience_early_stop=4, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'DynamicUnet.__init__.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2fd3fff21061>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/kaggle/model/export-2.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, file, destroy)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mxtra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cls'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mtry_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdestroy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/fastai/torch_core.py\u001b[0m in \u001b[0;36mtry_save\u001b[0;34m(state, path, file)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtry_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mPathLikeOrBinaryStream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_pathlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{e}\\n Can't write {path/file}. Pass an absolute writable pathlib obj `fname`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \"\"\"\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \"\"\"\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0mserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_storages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'DynamicUnet.__init__.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "learn.export(file='/kaggle/model/export-2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn=None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_large_learner(bs=4, size=size, transform_func=get_simple_transforms, model_to_load='bestmodel-unfrozen-1'):\n",
    "    data = (src.transform(transform_func(), size=size, tfm_y=True)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))\n",
    "    learn = unet_learner(data, models.cadene_models.se_resnext101_32x4d, metrics=metrics, wd=wd, bottle=True)\n",
    "    learn.model_dir = Path('/kaggle/model')\n",
    "    learn.loss_func = CrossEntropyFlat(axis=1, weight=torch.tensor([1.5, .5, .5, .5, .5]).cuda())\n",
    "    learn = to_fp16(learn, loss_scale=8.0)\n",
    "    if model_to_load is not None:\n",
    "        learn.load(model_to_load)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_large_learner(bs=bs, size=src_size, transform_func=get_simple_transforms, model_to_load='bestmodel-unfrozen-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_find(learn, num_it=400)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=6e-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      60.00% [3/5 4:40:00<3:06:40]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_simple</th>\n",
       "      <th>acc_camvid_with_zero_check</th>\n",
       "      <th>dice_coefficient</th>\n",
       "      <th>dice_coefficient_2</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.051314</td>\n",
       "      <td>0.055195</td>\n",
       "      <td>0.968907</td>\n",
       "      <td>0.339916</td>\n",
       "      <td>0.810896</td>\n",
       "      <td>0.372016</td>\n",
       "      <td>1:33:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.058454</td>\n",
       "      <td>0.054237</td>\n",
       "      <td>0.969428</td>\n",
       "      <td>0.421480</td>\n",
       "      <td>0.846516</td>\n",
       "      <td>0.454154</td>\n",
       "      <td>1:33:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.068239</td>\n",
       "      <td>0.053321</td>\n",
       "      <td>0.970236</td>\n",
       "      <td>0.431206</td>\n",
       "      <td>0.850170</td>\n",
       "      <td>0.461081</td>\n",
       "      <td>1:32:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='9760' class='' max='11312', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      86.28% [9760/11312 1:17:23<12:18 0.0661]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with dice_coefficient value: 0.8108955025672913.\n",
      "Better model found at epoch 1 with dice_coefficient value: 0.8465156555175781.\n",
      "Better model found at epoch 2 with dice_coefficient value: 0.8501698970794678.\n"
     ]
    }
   ],
   "source": [
    "train_learner(learn, slice(lr), epochs=5, pct_start=0.8, best_model_name='bestmodel-frozen-3', \n",
    "              patience_early_stop=4, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-3');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('bestmodel-frozen-3');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(file='/kaggle/model/export-3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_large_learner(bs=bs, transform_func=get_extra_transforms, model_to_load='bestmodel-frozen-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = slice(lr/1000,lr/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_learner(learn, lrs, epochs=10, pct_start=0.8, best_model_name='bestmodel-4', \n",
    "              patience_early_stop=5, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-4');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('bestmodel-4');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(file='/kaggle/model/export-4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!cp /kaggle/model/export.pkl /opt/fastai/fastai-exercises/nbs_gil\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'export-4.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn=None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = (path/'test_images').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_learn = load_learner('/kaggle/model/', file='export-2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_learn = to_fp16(inference_learn, loss_scale=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_path):\n",
    "    pred_class, pred_idx, outputs = inference_learn.predict(open_image(str(img_path)))\n",
    "    return pred_class, pred_idx, outputs\n",
    "\n",
    "def encode_classes(pred_class_data):\n",
    "    pixels = np.concatenate([[0], torch.transpose(pred_class_data.squeeze(), 0, 1).flatten(), [0]])\n",
    "    classes_dict = {1: [], 2: [], 3: [], 4: []}\n",
    "    count = 0\n",
    "    previous = pixels[0]\n",
    "    for i, val in enumerate(pixels):\n",
    "        if val != previous:\n",
    "            if previous in classes_dict:\n",
    "                classes_dict[previous].append((i - count, count))\n",
    "            count = 0\n",
    "        previous = val\n",
    "        count += 1\n",
    "    return classes_dict\n",
    "\n",
    "\n",
    "def convert_classes_to_text(classes_dict, clazz):\n",
    "    return ' '.join([f'{v[0]} {v[1]}' for v in classes_dict[clazz]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_to_predict = train_images[16].name\n",
    "display_image_with_mask(image_to_predict)\n",
    "pred_class, pred_idx, outputs = predict(path/f'train_images/{image_to_predict}')\n",
    "pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(pred_class.data.squeeze(), 0, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking encoding methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_all = encode_classes(pred_class.data)\n",
    "print(convert_classes_to_text(encoded_all, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = train_images[16]\n",
    "print(get_y_fn(image_name))\n",
    "img = open_mask(get_y_fn(image_name))\n",
    "img_data = img.data\n",
    "print(convert_classes_to_text(encode_classes(img_data), 3))\n",
    "img_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through the test images and create submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "defect_classes = [1, 2, 3, 4]\n",
    "with open('submission.csv', 'w') as submission_file:\n",
    "    submission_file.write('ImageId_ClassId,EncodedPixels\\n')\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        pred_class, pred_idx, outputs = predict(test_image)\n",
    "        encoded_all = encode_classes(pred_class.data)\n",
    "        for defect_class in defect_classes:\n",
    "            submission_file.write(f'{test_image.name}_{defect_class},{convert_classes_to_text(encoded_all, defect_class)}\\n')\n",
    "        if i % 5 == 0:\n",
    "            print(f'Processed {i} images\\r', end='')\n",
    "            \n",
    "print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative prediction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,y = learn.get_preds(ds_type=DatasetType.Test, with_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class_data = preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((path/'test_images').ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test_ds.x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
