{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from fastai.vision import *\n",
    "from fastai import *\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from fastai.vision.models.cadene_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = pd.read_csv('/root/.fastai/data/severstal/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId_ClassId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002cc93b.jpg_1</td>\n",
       "      <td>29102 12 29346 24 29602 24 29858 24 30114 24 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002cc93b.jpg_2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002cc93b.jpg_3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002cc93b.jpg_4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00031f466.jpg_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId_ClassId                                      EncodedPixels\n",
       "0  0002cc93b.jpg_1  29102 12 29346 24 29602 24 29858 24 30114 24 3...\n",
       "1  0002cc93b.jpg_2                                                NaN\n",
       "2  0002cc93b.jpg_3                                                NaN\n",
       "3  0002cc93b.jpg_4                                                NaN\n",
       "4  00031f466.jpg_1                                                NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/root/.fastai/data/severstal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/root/.fastai/data/severstal/train_images.zip'),\n",
       " PosixPath('/root/.fastai/data/severstal/sample_submission.csv'),\n",
       " PosixPath('/root/.fastai/data/severstal/test_images.zip'),\n",
       " PosixPath('/root/.fastai/data/severstal/train.csv'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images'),\n",
       " PosixPath('/root/.fastai/data/severstal/test_images')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/root/.fastai/data/severstal/train_images/5e581254c.jpg'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images/fd2f7b4f4.jpg'),\n",
       " PosixPath('/root/.fastai/data/severstal/train_images/82f4c0b69.jpg')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = get_image_files(path/'train_images')\n",
    "train_images[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check maximum size of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_img_max_size(folder):\n",
    "    max_height = 0\n",
    "    max_width = 0\n",
    "    for train_image in train_images:\n",
    "        img = open_image(train_image)\n",
    "        if max_height < img.shape[1]:\n",
    "            max_height = img.shape[1]\n",
    "        if max_width < img.shape[2]:\n",
    "            max_width = img.shape[2]\n",
    "    return max_height, max_width\n",
    "\n",
    "def show_image(images, index):\n",
    "    img_f = images[index]\n",
    "    print(type(img_f))\n",
    "    img = open_image(img_f)\n",
    "    print(img)\n",
    "    img.show(figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_path = Path('/kaggle/mask')\n",
    "if not os.path.exists(mask_path):\n",
    "    os.makedirs(str(mask_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_encoded_to_array(encoded_pixels):\n",
    "    pos_array = []\n",
    "    len_array = []\n",
    "    splits = encoded_pixels.split()\n",
    "    pos_array = [int(n) - 1 for i, n in enumerate(splits) if i % 2 == 0]\n",
    "    len_array = [int(n) for i, n in enumerate(splits) if i % 2 == 1]\n",
    "    return pos_array, len_array\n",
    "        \n",
    "def convert_to_pair(pos_array, rows):\n",
    "    return [(p % rows, p // rows) for p in pos_array]\n",
    "\n",
    "def create_positions(single_pos, size):\n",
    "    return [i for i in range(single_pos, single_pos + size)]\n",
    "\n",
    "def create_positions_pairs(single_pos, size, row_size):\n",
    "    return convert_to_pair(create_positions(single_pos, size), row_size)\n",
    "\n",
    "def convert_to_mask(encoded_pixels, row_size, col_size, category):\n",
    "    pos_array, len_array = convert_encoded_to_array(encoded_pixels)\n",
    "    mask = np.zeros([row_size, col_size])\n",
    "    for(p, l) in zip(pos_array, len_array):\n",
    "        for row, col in create_positions_pairs(p, l, row_size):\n",
    "            mask[row][col] = category\n",
    "    return mask\n",
    "\n",
    "def save_to_image(masked, image_name):\n",
    "    im = PIL.Image.fromarray(masked)\n",
    "    im = im.convert(\"L\")\n",
    "    image_name = re.sub(r'(.+)\\.jpg', r'\\1', image_name) + \".png\"\n",
    "    real_path = mask_path/image_name\n",
    "    im.save(real_path)\n",
    "    return real_path\n",
    "\n",
    "def open_single_image(path):\n",
    "    img = open_image(path)\n",
    "    img.show(figsize=(20,20))\n",
    "    \n",
    "def get_y_fn(x):\n",
    "    return mask_path/(x.stem + '.png')\n",
    "\n",
    "def group_by(train_images, train_pd):\n",
    "    tran_dict = {image.name:[] for image in train_images}\n",
    "    pattern = re.compile('(.+)_(\\d+)')\n",
    "    for index, image_path in train_pd.iterrows():\n",
    "        m = pattern.match(image_path['ImageId_ClassId'])\n",
    "        file_name = m.group(1)\n",
    "        category = m.group(2)\n",
    "        tran_dict[file_name].append((int(category), image_path['EncodedPixels']))\n",
    "    return tran_dict\n",
    "\n",
    "def display_image_with_mask(img_name):\n",
    "    full_image = path/'train_images'/img_name\n",
    "    print(full_image)\n",
    "    open_single_image(full_image)\n",
    "    mask_image = get_y_fn(full_image)\n",
    "    mask = open_mask(mask_image)\n",
    "    print(full_image)\n",
    "    mask.show(figsize=(20, 20), alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_categories_mask = group_by(train_images, train_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create mask files and save these to kaggle/mask/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 256\n",
    "image_width = 1600\n",
    "if not os.path.exists(mask_path/'0002cc93b.png'):\n",
    "    for image_name, cat_list in grouped_categories_mask.items():\n",
    "        masked = np.zeros([image_height, image_width])\n",
    "        for cat_mask in cat_list:\n",
    "            encoded_pixels = cat_mask[1]\n",
    "            if pd.notna(cat_mask[1]):\n",
    "                masked += convert_to_mask(encoded_pixels, image_height, image_width, cat_mask[0])\n",
    "        if np.amax(masked) > 4:\n",
    "            print(f'Check {image_name} for max category {np.amax(masked)}')\n",
    "        save_to_image(masked, image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limited_dihedral_affine(k:partial(uniform_int,0,3)):\n",
    "    \"Randomly flip `x` image based on `k`.\"\n",
    "    x = -1 if k&1 else 1\n",
    "    y = -1 if k&2 else 1\n",
    "    if k&4: return [[0, x, 0.],\n",
    "                    [y, 0, 0],\n",
    "                    [0, 0, 1.]]\n",
    "    return [[x, 0, 0.],\n",
    "            [0, y, 0],\n",
    "            [0, 0, 1.]]\n",
    "\n",
    "dihedral_affine = TfmAffine(limited_dihedral_affine)\n",
    "\n",
    "def get_extra_transforms(max_rotate:float=3., max_zoom:float=1.1,\n",
    "                   max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75,\n",
    "                   p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None)->Collection[Transform]:\n",
    "    \"Utility func to easily create a list of flip, rotate, `zoom`, warp, lighting transforms.\"\n",
    "    p_lightings = [p_lighting, p_lighting + 0.2, p_lighting + 0.4, p_lighting + 0.6, p_lighting + 0.7]\n",
    "    max_lightings = [max_lighting, max_lighting + 0.2, max_lighting + 0.4, max_lighting + 0.6, max_lighting + 0.7]\n",
    "    res = [rand_crop(), dihedral_affine(), \n",
    "           symmetric_warp(magnitude=(-max_warp,max_warp), p=p_affine),\n",
    "           rotate(degrees=(-max_rotate,max_rotate), p=p_affine),\n",
    "           rand_zoom(scale=(1., max_zoom), p=p_affine)]\n",
    "    res.extend([brightness(change=(0.5*(1-mp[0]), 0.5*(1+mp[0])), p=mp[1]) for mp in zip(max_lightings, p_lightings)])\n",
    "    res.extend([contrast(scale=(1-mp[0], 1/(1-mp[0])), p=mp[1]) for mp in zip(max_lightings, p_lightings)])\n",
    "    #       train                   , valid\n",
    "    return (res, [crop_pad()])\n",
    "\n",
    "def get_simple_transforms(max_rotate:float=3., max_zoom:float=1.1,\n",
    "                   max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75,\n",
    "                   p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None)->Collection[Transform]:\n",
    "    \"Utility func to easily create a list of flip, rotate, `zoom`, warp, lighting transforms.\"\n",
    "    res = [\n",
    "        rand_crop(),\n",
    "        symmetric_warp(magnitude=(-max_warp,max_warp), p=p_affine),\n",
    "        rotate(degrees=(-max_rotate,max_rotate), p=p_affine),\n",
    "        rand_zoom(scale=(1., max_zoom), p=p_affine)\n",
    "          ]\n",
    "    #       train                   , valid\n",
    "    return (res, [crop_pad()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = (path/'train_images').ls()\n",
    "src_size = np.array(open_image(str(train_images[0])).shape[1:])\n",
    "valid_pct = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = array(['0', '1', '2', '3', '4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_bunch(bs, size):\n",
    "    src = (SegmentationItemList.from_folder(path/'train_images')\n",
    "       .split_by_rand_pct(valid_pct=valid_pct)\n",
    "       .label_from_func(get_y_fn, classes=codes))\n",
    "    data = (src.transform(get_simple_transforms(), size=size, tfm_y=True)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))\n",
    "    return src, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "size = src_size//2\n",
    "src, data = create_data_bunch(bs, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create learner and training\n",
    "Starting with low resolution training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "name2id = {v:k for k,v in enumerate(codes)}\n",
    "void_code = name2id['0']\n",
    "\n",
    "def acc_camvid(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    mask = target != void_code\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    comparison = argmax[mask]==target[mask]\n",
    "    return torch.tensor(0.) if comparison.numel() == 0 else comparison.float().mean()\n",
    "\n",
    "def acc_camvid_with_zero_check(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        if(torch.sum(argmax[b]).item() == 0.0 and torch.sum(target[b]).item() == 0.0):\n",
    "            total[b] = 1\n",
    "        else:\n",
    "            mask = target[b] != void_code\n",
    "            comparison = argmax[b][mask]==target[b][mask]\n",
    "            total[b] = torch.tensor(0.) if comparison.numel() == 0 else comparison.float().mean()\n",
    "    return total.mean()\n",
    "\n",
    "\n",
    "def calc_dice_coefficients(argmax, target, cats):\n",
    "    def calc_dice_coefficient(seg, gt, cat: int):\n",
    "        mask_seg = seg == cat\n",
    "        mask_gt = gt == cat\n",
    "        sum_seg = torch.sum(mask_seg.float())\n",
    "        sum_gt = torch.sum(mask_gt.float())\n",
    "        if sum_seg + sum_gt == 0:\n",
    "            return torch.tensor(1.0)\n",
    "        return (torch.sum((seg[gt == cat] / cat).float()) * 2.0) / (sum_seg + sum_gt)\n",
    "\n",
    "    total_avg = torch.empty([len(cats)])\n",
    "    for i, c in enumerate(cats):\n",
    "        total_avg[i] = calc_dice_coefficient(argmax, target, c)\n",
    "    return total_avg.mean()\n",
    "\n",
    "\n",
    "def dice_coefficient(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    cats = [1, 2, 3, 4]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        total[b] = calc_dice_coefficients(argmax[b], target[b], cats)\n",
    "    return total.mean()\n",
    "\n",
    "def calc_dice_coefficients_2(argmax, target, cats):\n",
    "    def calc_dice_coefficient(seg, gt, cat: int):\n",
    "        mask_seg = seg == cat\n",
    "        mask_gt = gt == cat\n",
    "        sum_seg = torch.sum(mask_seg.float())\n",
    "        sum_gt = torch.sum(mask_gt.float())\n",
    "        return (torch.sum((seg[gt == cat] / cat).float())), (sum_seg + sum_gt)\n",
    "\n",
    "    total_avg = torch.empty([len(cats), 2])\n",
    "    for i, c in enumerate(cats):\n",
    "        total_avg[i][0], total_avg[i][1] = calc_dice_coefficient(argmax, target, c)\n",
    "    total_sum = total_avg.sum(axis=0)\n",
    "    if (total_sum[1] == 0.0):\n",
    "        return torch.tensor(1.0)\n",
    "    return total_sum[0] * 2.0 / total_sum[1]\n",
    "\n",
    "\n",
    "def dice_coefficient_2(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    argmax = (input.argmax(dim=1))\n",
    "    batch_size = input.shape[0]\n",
    "    cats = [1, 2, 3, 4]\n",
    "    total = torch.empty([batch_size])\n",
    "    for b in range(batch_size):\n",
    "        total[b] = calc_dice_coefficients_2(argmax[b], target[b], cats)\n",
    "    return total.mean()\n",
    "\n",
    "\n",
    "def accuracy_simple(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    return (input.argmax(dim=1)==target).float().mean()\n",
    "\n",
    "\n",
    "def dice_coeff(pred, target):\n",
    "    smooth = 1.\n",
    "    num = pred.size(0)\n",
    "    m1 = pred.view(num, -1)  # Flatten\n",
    "    m2 = target.view(num, -1)  # Flatten\n",
    "    intersection = (m1 * m2).sum()\n",
    "    return (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDiceLoss(nn.Module):\n",
    "    def __init__(self, zero_cat_factor=0.1):\n",
    "        super().__init__()\n",
    "        self.zero_cat_factor = zero_cat_factor\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return self.dice_loss(target, input, self.zero_cat_factor)\n",
    "\n",
    "    def dice_loss(self, target, output, eps=1e-7, zero_cat_factor=0.1):\n",
    "        '''\n",
    "        Soft dice loss calculation for arbitrary batch size, number of classes, and number of spatial dimensions.\n",
    "        Assumes the `channels_last` format.\n",
    "\n",
    "        # Arguments\n",
    "            target: b x 1 x X x Y( x Z...) ground truth\n",
    "            output: b x c x X x Y( x Z...) Network output, must sum to 1 over c channel (such as after softmax)\n",
    "            epsilon: Used for numerical stability to avoid divide by zero errors\n",
    "\n",
    "        # References\n",
    "            V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation\n",
    "            https://arxiv.org/abs/1606.04797\n",
    "            More details on Dice loss formulation\n",
    "            https://mediatum.ub.tum.de/doc/1395260/1395260.pdf (page 72)\n",
    "\n",
    "            Adapted from https://github.com/Lasagne/Recipes/issues/99#issuecomment-347775022\n",
    "        '''\n",
    "\n",
    "        # skip the batch and class axis for calculating Dice score\n",
    "        num_classes = output.shape[1]\n",
    "        y_true = F.one_hot(target.long().squeeze(), num_classes)\n",
    "        y_pred = F.softmax(output, dim=1).permute(0, 2, 3, 1)\n",
    "        y_true = y_true.type(y_pred.type())\n",
    "        y_true = y_true.permute(0, 3, 1, 2)\n",
    "        y_true[:,0,:] *= zero_cat_factor # Factor used to take power away from the zeroth category\n",
    "        y_true = y_true.permute(0, 2, 3, 1)\n",
    "        axes = tuple(range(1, len(y_pred.shape)-1))\n",
    "        numerator = 2. * torch.sum(y_pred * y_true, axes)\n",
    "        denominator = torch.sum(y_pred ** 2 + y_true ** 2, axes)\n",
    "        # When intersection and cardinality are all zero you have 100% score and not 0% score\n",
    "        # For this we use the eps parameter\n",
    "        loss_array = ((numerator + eps) / (denominator + eps))\n",
    "        loss_array = (loss_array).mean(dim=0)\n",
    "        return ((1 - torch.mean(loss_array)) + F.cross_entropy(output, target.squeeze())) / 2.\n",
    "\n",
    "    def __del__(self): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import callbacks\n",
    "\n",
    "def train_learner(learn, slice_lr, epochs=10, pct_start=0.8, best_model_name='best_model', \n",
    "                  patience_early_stop=4, patience_reduce_lr = 3):\n",
    "    learn.fit_one_cycle(epochs, slice_lr, pct_start=pct_start, \n",
    "                    callbacks=[callbacks.SaveModelCallback(learn, monitor='dice_coefficient',mode='max', name=best_model_name),\n",
    "                              callbacks.EarlyStoppingCallback(learn=learn, monitor='dice_coefficient', patience=patience_early_stop),\n",
    "                              callbacks.ReduceLROnPlateauCallback(learn=learn, monitor='dice_coefficient', patience=patience_reduce_lr),\n",
    "                              callbacks.TerminateOnNaNCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=accuracy_simple, acc_camvid_with_zero_check, dice_coefficient, dice_coefficient_2\n",
    "wd=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedDiceLoss()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd, bottle=True)\n",
    "# learn.loss_func = CrossEntropyFlat(axis=1, weight=torch.tensor([1.0, .5, .5, .5, .5]).cuda())\n",
    "learn.loss_func = CombinedDiceLoss(zero_cat_factor=0.5)\n",
    "learn.loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model_dir = Path('/kaggle/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn = to_fp16(learn, loss_scale=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEJCAYAAACzPdE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd5hU5dnH8e+9FXZZlrb0jhQRKbIURRETRTQqGiv2SqxRk+hrjIlvLHlTjbHFoCJqAhoVjCV2UQRE2KU3YUMvsov0vuV+/5jRjLgLCzuzZ2b397muuZh5znNm7nmuZX972nPM3REREamqpKALEBGRmkGBIiIiUaFAERGRqFCgiIhIVChQREQkKhQoIiISFTELFDNrY2YTzWyRmS0ws1vL6XOJmc0NP6aaWa+IZSvMbJ6ZzTazvFjVKSIi0ZESw/cuAX7q7jPNLAvIN7P33X1hRJ/lwInuvtnMTgNGAQMilp/k7htjWKOIiERJzALF3dcD68PPt5vZIqAVsDCiz9SIVaYBravymU2aNPH27dtX5S1ERGqV/Pz8je6eE433iuUWyjfMrD3QB/j8AN2uAd6OeO3Ae2bmwN/cfVQF7z0SGAnQtm1b8vK0d0xEpLLMbGW03ivmgWJm9YBXgdvcfVsFfU4iFCjHRzQPcvd1ZtYUeN/MFrv7pP3XDQfNKIDc3FzNIyMiEpCYnuVlZqmEwuQf7j6+gj49gaeB4e7+1dft7r4u/G8hMAHoH8taRUSkamJ5lpcBzwCL3P2hCvq0BcYDl7n7koj2zPCBfMwsExgKzI9VrSIiUnWx3OU1CLgMmGdms8NtdwNtAdz9SeBXQGPgiVD+UOLuuUAzYEK4LQUY6+7vxLBWERGpolie5TUZsIP0uRa4tpz2ZUCv764hIiLxSlfKi4hIVChQREQkKqrlOpR45+5MWrqReWu2kJ2RRscmmXRuWo+crHTCx3FEROQgan2gbNtTzLVj8pi+YtN3ljWvX4fBXZrQuF465/RpRZdmWQFUKCKSGGp9oGSlp5CTlc79w4/i3L6t2ba7hGVFO1iyYTufL9/EB4sK2ba7mNGTl3NGz5bsKy2jTcO6tG+SSb/2jejQJDPoryAiEhfMveZcXJ6bm+uxmHqlaPtefj5+LrNWbSEzPYV1W3ZTUuaYwXGdGpORlsKZvVpyZs8W2kUmIgnFzPLDl2tU/b0UKIeupLSM1Zt3M37mGt5fuIHte0pYu2U3GWnJ1EtP4erjOzC8d0taZNeNeS0iIlWhQKlAdQXK/krLnFfz17Doy20UFO7g06WhGffTkpNolJnGhf3akNu+IT1bNyC7bmq11yciUpFoBkqtP4YSDclJxgX92nzzevGX25ha8BWF2/ey+Mtt/OXDpQBkpCVzQW4bzuvbmh6tsoMqV0QkJhQoMdCteX26Na//zesN2/ZQULiDV/LX8I/PVzJm6gpO6NyEG07sRL8OjUhN1uVAIpL4tMurmm3ZtY+X89bw2MQCtu4uJqtOCid2yeHs3q0Y0jWHFIWLiFQjHUOpQCIEytd27i1hcsFGPly0gQ8XFfLVzn00qZfOhf1a86MTO1G/jo61iEjsKVAqkEiBEqm4tIyJiwv5Z94aPly8gYYZaQzv3ZIL+7X51q4zEZFoU6BUIFEDJdL8tVt55MOlfLykiH0lZZx8ZDNu/t4R9G7TIOjSRKQGUqBUoCYEyte27NrHmKkreHbKCrbuLuaYtg247eQuDO6SE3RpIlKDKFAqUJMC5Ws79pbw0ozVPDd1Bas27WJE/zbcffqRZOkYi4hEQTQDRacUxbl66Slcc3wH3rt9MNef2ImXZqzm1D9P4uW81ZSUlgVdnojINxQoCaJOajJ3ndaNV284jkb10rjjlbmc8ehkZq7aHHRpIiKAAiXh9GnbkDduPp4nLz2GbbuLuWjUNKb+Z2PQZYmIxC5QzKyNmU00s0VmtsDMbi2nj5nZI2ZWYGZzzeyYiGVXmNnS8OOKWNWZiMyMYT1a8NaPT6B94wyufS6PFz5bQWlZzTkeJiKJJ5ZbKCXAT939SGAgcJOZdd+vz2lA5/BjJPBXADNrBNwLDAD6A/eaWcMY1pqQGmam8fdrBtCnbQN++a8F3D1+XtAliUgtFrNAcff17j4z/Hw7sAhotV+34cDzHjINaGBmLYBTgffdfZO7bwbeB4bFqtZE1rR+Hf5+zQBuGNKJl/JWM3rycmrSmXsikjiq5RiKmbUH+gCf77eoFbA64vWacFtF7eW990gzyzOzvKKiomiVnFDMjJ8N7cqJXXK4782FnP34FCYtKVKwiEi1inmgmFk94FXgNnfftv/iclbxA7R/t9F9lLvnuntuTk7tvegvOcl45opcfn9uTzbu2Mflo6dz+ejpLCvaEXRpIlJLxDRQzCyVUJj8w93Hl9NlDdAm4nVrYN0B2uUAUpKTuKBfGz762Yn86ozuzF69heGPT2H26i1BlyYitUAsz/Iy4Blgkbs/VEG314HLw2d7DQS2uvt64F1gqJk1DB+MHxpuk0pIT0nm6uM78PatJ9AwI43Lnv5cWyoiEnOx3EIZBFwGfM/MZocfp5vZ9WZ2fbjPv4FlQAHwFHAjgLtvAu4HZoQf94Xb5BC0bpjBuJEDSUk2bho7iz3FpUGXJCI1mObyqgUmLi7kqjEz6NGqPn86vzddm2cFXZKIxAnN5SWH5KRuTXny0r58uXUPwx+fzBtzdDhKRKJPgVJLDOvRnLdvHUyPltncMm4Wv3tnsa6sF5GoUqDUIjlZ6Yy9biAXD2jLXz/+D9c8N4Otu4uDLktEaggFSi2TlpLEb845mgfO7sHkpRs5+/EpFBRuD7osEakBFCi11KUD2zH2uoFs31PM8Mem8MK0lZRpF5iIVIECpRbr36ERr998PH3aNuSXr83nZy/P0U27ROSwKVBquZYN6vLCNf35ySldGD9rLT9+cRb7ShQqInLoUoIuQIJnZvz4+53JSEvmgbcWsac4n0dH9CEzXT8eIlJ52kKRb1x7QkceOLsHH39RyHlPfsbaLbuDLklEEogCRb7l0oHtGH1lP9Zs2sXwxyaTv1Iz3ohI5ShQ5DuGdG3KhJuOo156CiNGfc6r+WuCLklEEoACRcp1RNMsXrtpELntG/LTl+fwzxmrD76SiNRqChSpUIOMNJ69qh8ndG7CXePn8tbc9UGXJCJxTIEiB5SekszfLutL33YNue2lWUz8ojDokkQkTilQ5KAy0lJ45sp+dG2exfUv5DNt2VdBlyQicUiBIpVSv04qz13VnzaNMrhmzAymFGwMuiQRiTMKFKm0xvXSGXvtANo0yuCqZ2fw73k6piIi/6VAkUPStH4dXhp5LD1bZ3PT2JmM/XxV0CWJSJxQoMghy85I5YVrBnBS16bcPWEej08sCLokEYkDMQsUMxttZoVmNr+C5XeY2ezwY76ZlZpZo/CyFWY2L7xMN4mPQ3XTQmd/ndOnFX949wuembw86JJEJGCxnP1vDPAY8Hx5C939D8AfAMzsTOB2d4+c5+Mkd9eR3ziWmpzEH8/vxZ7iUu5/cyEN6qZybt/WQZclIgGJ2RaKu08CKjsR1AhgXKxqkdhJTjIevqg3xx/RhDtfncsHCzcEXZKIBCTwYyhmlgEMA16NaHbgPTPLN7ORB1l/pJnlmVleUVFRLEuVCnx98WOPlvW5aexMTSgpUksFHijAmcCU/XZ3DXL3Y4DTgJvMbHBFK7v7KHfPdffcnJycWNcqFchMT2H0lf1o2aAu1zyXR0HhjqBLEpFqFg+BchH77e5y93XhfwuBCUD/AOqSQ9S4XjrPXdWflKQkrhg9nQ3b9gRdkohUo0ADxcyygROBf0W0ZZpZ1tfPgaFAuWeKSfxp2ziDMVf1Y8uufVwxejrb9hQHXZKIVJNYnjY8DvgM6Gpma8zsGjO73syuj+h2DvCeu++MaGsGTDazOcB04C13fydWdUr09WiVzZOX9aWgcAcjn89jb0lp0CWJSDUwdw+6hqjJzc31vDxdthIvXpu1lttems0ZPVvw6Ig+mFnQJYnIfsws391zo/FesbwORWq5s/u04stte/jt24vp3DSLW0/uHHRJIhJDChSJqR8N7siSDdv58wdL6NKsHqcd3SLokkQkRuLhLC+pwcyM35xzNH3aNuAn/5zDgnVbgy5JRGJEgSIxVyc1dOFjg4xURj6fz8Yde4MuSURiQIEi1aJpVh2eujyXr3bu5foX8nXml0gNpECRatOjVTZ/PL8XeSs3c8+E+dSkMwxFRAflpZqd0bMlS77cziMfFdC1eRbXntAx6JJEJEq0hSLV7raTuzDsqOb85t+L+GSJJvQUqSkUKFLtkpKMhy7sRdfm9bl57Ez+U6SJJEVqAgWKBCIjLYWnLu9LWnIS1z6Xx9ZdmvNLJNEpUCQwrRtm8LfL+rJm8y5uHjeTktKyoEsSkSpQoEigcts34sGzj+bTpRt54K1FQZcjIlWgs7wkcBf0a8PiL7czespyjmpZn/Nz2wRdkogcBm2hSFy4+/RuHNuxMb/61wLd7VEkQSlQJC6kJCfx8EW9qZOaxC3jZrGnWFfSiyQaBYrEjWb16/DH83uxaP02fvv24qDLEZFDpECRuPL9I5tx1aD2jJm6gg8Wbgi6HBE5BAoUiTt3ndaN7i3qc8crc/hy656gyxGRSlKgSNxJT0nm0Yv7sLekjNtemkVpmSaRFEkEChSJS51y6vHrs45i2rJNPDGxIOhyRKQSYhYoZjbazArNbH4Fy4eY2VYzmx1+/Cpi2TAz+8LMCszsrljVKPHtvL6tGd67JQ9/uJS8FZuCLkdEDiKWWyhjgGEH6fOpu/cOP+4DMLNk4HHgNKA7MMLMusewTolTZsYDZ/egVYO63PribM33JRLnYhYo7j4JOJw/K/sDBe6+zN33AS8Cw6NanCSMrDqpPDKiDxu27eGu8XN1Uy6ROBb0MZRjzWyOmb1tZkeF21oBqyP6rAm3lcvMRppZnpnlFRXp3ho1Ue82Dbjj1K68Pf9L/pm3+uAriEggggyUmUA7d+8FPAq8Fm63cvpW+Gepu49y91x3z83JyYlBmRIPrjuhIwM7NuL+NxexdsvuoMsRkXIEFijuvs3dd4Sf/xtINbMmhLZIImcHbA2sC6BEiSNJScbvz+1FmTs/Hz9Pu75E4lBggWJmzc3Mws/7h2v5CpgBdDazDmaWBlwEvB5UnRI/2jbO4K7TujFpSREv560JuhwR2U/Mpq83s3HAEKCJma0B7gVSAdz9SeA84AYzKwF2Axd56M/OEjO7GXgXSAZGu/uCWNUpieXSAe14a+567n9zISd0aUKL7LpBlyQiYVaTdh3k5uZ6Xl5e0GVIjK38aifDHv6U/h0aMeaqfoQ3dEXkMJhZvrvnRuO9gj7LS+SQtWucyf8M68onS4p4OV+7vkTihQJFEtLlx7anf4dG3P/mQtZv1VlfIvFAgSIJKSnJ+MN5PSkuLeNunfUlEhcUKJKwQru+ujHxiyJenbk26HJEaj0FiiS0K45tT//2jfj1Gwt07xSRgClQJKElJRm//3rX1wTt+hIJkgJFEl77JpnceWo3PlpcyHjt+hIJjAJFaoQrj2tPv/YN+fUbC9iwTbu+RIKgQJEaIbTrqxf7dNaXSGAqFShm1snM0sPPh5jZj82sQWxLEzk0HZpkcsep3fhwcSETZmnXl0h1q+wWyqtAqZkdATwDdADGxqwqkcN05XHtyW3XkP99fQGF2vUlUq0qGyhl7l4CnAM87O63Ay1iV5bI4UkOn/W1t0RnfYlUt8oGSrGZjQCuAN4Mt6XGpiSRqumYU487Tu3KB4sK+dN7SxQqItWkstPXXwVcDzzo7svNrAPw99iVJVI1Vw3qwNINO3hsYgF7iku554zuQZckUuNVKlDcfSHwYwAzawhkuftvY1mYSFUkJxm/Pfdo0lOTeHrycnLbN2JYj+ZBlyVSo1X2LK+Pzay+mTUC5gDPmtlDsS1NpGrMjHt+0J2erbO585U5rN60K+iSRGq0yh5DyXb3bcAPgWfdvS9wcuzKEomOtJQkHh3RB3e4Zdws9pWUBV2SSI1V2UBJMbMWwAX896C8SEJo1ziT357bk9mrt3DB3z5j3RbdP0UkFiobKPcRusf7f9x9hpl1BJbGriyR6PpBzxY8cckxFBTu4OoxM9hTXBp0SSI1TqUCxd1fdvee7n5D+PUydz/3QOuY2WgzKzSz+RUsv8TM5oYfU82sV8SyFWY2z8xmm5luEi9RcfrRLXj04j4s/nI7v/n3oqDLEalxKntQvrWZTQgHxAYze9XMWh9ktTHAsAMsXw6c6O49gfuBUfstP8nde7t7bmVqFKmMk7o25drjO/D8Zyt5b8GXQZcjUqNUdpfXs8DrQEugFfBGuK1C7j4J2HSA5VPdfXP45TTgYAElEhV3DOtKj1b1ufPVuazV8RSRqKlsoOS4+7PuXhJ+jAFyoljHNcDbEa8deM/M8s1s5IFWNLORZpZnZnlFRUVRLElqqvSUZB4dcQylpc71L+TreIpIlFQ2UDaa2aVmlhx+XAp8FY0CzOwkQoHyPxHNg9z9GOA04CYzG1zR+u4+yt1z3T03JyeaGSc1WYcmmfz5wt7MW7uVW1+cRXGpTicWqarKBsrVhE4Z/hJYD5xHaDqWKjGznsDTwHB3/yag3H1d+N9CYALQv6qfJbK/k7s3494zu/Pugg3c9tJszfklUkWVPctrlbuf5e457t7U3c8mdJHjYTOztsB44DJ3XxLRnmlmWV8/B4YC5Z4pJlJVVw3qwF2ndeOtuet5ZvLyoMsRSWiVnRyyPD8BHq5ooZmNA4YATcxsDXAv4RmK3f1J4FdAY+AJMwMoCZ/R1QyYEG5LAca6+ztVqFPkgH40uCP5Kzfzu3cWc1TLbI7t1DjokkQSkh3uZr6ZrXb3NlGup0pyc3M9L0+Xrcih27qrmHOfnMqXW/fw4siB9GiVHXRJItXCzPKjdXlGVe4prx3OUmNkZ6TywjX9qV8nhZHP5/HVjr1BlySScA4YKGa23cy2lfPYTuiaFJEao0V2Xf52WS4bd+7juufzWL9V16iIHIoDBoq7Z7l7/XIeWe5eleMvInHp6NbZPHxhbxat386whz9lwbqtQZckkjCqsstLpEY6/egW/PvWE8hIS+a65/Io3L4n6JJEEoICRaQcHZpk8tTluWzeVczI53U1vUhlKFBEKtCjVTZ/vrAXs1dv4acvz6FEV9OLHJACReQAhvVowd2nhy58vGWcpmgRORAFishBjBzciV+e0Z2353/Jna/MpaxMZ8yLlEdnaolUwjXHd2BPcSl/ePcL0lOSePCco0lOsqDLEokrChSRSrpxSCf2FJfy6EcFbN9TwkMX9iI9JTnoskTihgJFpJLMjJ8O7Up23VQeeGsR2/YU87fL+pKRpv9GIqBjKCKH7NoTOvL783oypWAjlzz9OVt27Qu6JJG4oEAROQwX5LbhiUv6smDtNoWKSJgCReQwDevRnFGX92Vp4Q6FiggKFJEqGdK1KaMuU6iIgAJFpMqGdG3KU5fnsrRwBxc/9TmbdipUpHZSoIhEwYldcnjq8lwKinZw7l+nsvKrnUGXJFLtFCgiUXJilxzGXjuALbv2cc4TU8lfuTnokkSqlQJFJIpy2zdi/I2DqF8nhYufmsbb89YHXZJItYlpoJjZaDMrNLP5FSw3M3vEzArMbK6ZHROx7AozWxp+XBHLOkWiqUOTTMbfOIgerbK5cexMnpq0DHfN/yU1X6y3UMYAww6w/DSgc/gxEvgrgJk1Au4FBgD9gXvNrGFMKxWJokaZafzj2gGc3qMFD/57Eb/813xNfy81XkwDxd0nAZsO0GU48LyHTAMamFkL4FTgfXff5O6bgfc5cDCJxJ06qck8OqIPPxrckb9PW8XIF/LZsbck6LJEYiboYyitgNURr9eE2ypq/w4zG2lmeWaWV1RUFLNCRQ5HUpLx89OP5IGze/DJkiLOf/Iz1m3ZHXRZIjERdKCUN/+3H6D9u43uo9w9191zc3JyolqcSLRcOrAdz1yRy+pNuzj78SlMWlKk+6pIjRN0oKwB2kS8bg2sO0C7SMIa0rUpr95wHKnJSVw+ejon/elj5qzeEnRZIlETdKC8DlwePttrILDV3dcD7wJDzaxh+GD80HCbSELr2jyL938ymL9c1JuSUuf8Jz9j7OerdBaY1AgxvZGDmY0DhgBNzGwNoTO3UgHc/Ung38DpQAGwC7gqvGyTmd0PzAi/1X3ufqCD+yIJIyMtheG9WzG4cw63vjSbuyfMY+aqzTxwdg/qpOqGXZK4rCb9ZZSbm+t5eXlBlyFSaaVlziMfLuWRj5ZyZPP6PHlpX9o2zgi6LKlFzCzf3XOj8V5B7/ISqdWSk4zbT+nC6Cv6sXbLbs549FOembycPcWlQZcmcsgUKCJx4KRuTXnzluPp0Sqb+99cyFmPTWbFRk0wKYlFgSISJ9o0ymDsdQN59sp+FG7fy1mPTWbiF4VBlyVSaQoUkThzUremvHHz8bRqmMHVY2bwiwnz2Kx7rEgCUKCIxKE2jTIYf8NxXHlce16csZqT/vQxL07X6cUS3xQoInGqbloy9555FG/9+Hi6NMvirvHzuP7v+UxaUsTeEh20l/ijQBGJc92a1+fF6wZy9+ndmLi4iMtHT+eHT0xl/VbNCSbxRYEikgCSkoyRgzsx81en8JeLerPyq12c9dgUZq3SXSElfihQRBJIvfTQVfbjbzyOuqnJXDhqGs9MXq6JJiUuKFBEElCXZlm8dtMgBnduwv1vLuTa5/PYvqc46LKkllOgiCSoRplpPHV5LvcNP4pPlhRx7l+nsuqrXUGXJbWYAkUkgZkZlx/bnheu7s+GbXsZ/vhkpi37KuiypJZSoIjUAMcd0YTXbhpEw8w0Ln36cx75cCm79ul2w1K9FCgiNUSHJplMuHEQp3RvxkPvL+GUhyYxd41u4CXVR4EiUoNk103lr5f25aWRAwE476+f8fdpK3WFvVQLBYpIDTSgY2PevOV4ju3UmHtem88Dby1SqEjMxfSOjSISnIaZaTx7ZT/ue3Mhz0xezsYdezmjZ0u+360pSUkWdHlSAylQRGqwpCTj3jO7k5xk/OPzlfxr9jqO69SY353bkzaNdGdIiS7t8hKp4cyMX57Rnbn3nsqD5/RgzuotDHt4Ev/MWx10aVLDxDRQzGyYmX1hZgVmdlc5y/9sZrPDjyVmtiViWWnEstdjWadIbZCWksQlA9rx7u2D6dWmAXe+MpeH3vtCx1YkamK2y8vMkoHHgVOANcAMM3vd3Rd+3cfdb4/ofwvQJ+Itdrt771jVJ1JbtW6YwfNX9+fuCfN45KMCNmzby4Pn9CAlWTsspGpieQylP1Dg7ssAzOxFYDiwsIL+I4B7Y1iPiISlJCfxu3N70rx+HR75qICiHXt57OI+ZKTpsKocvlj+SdIKiNxJuybc9h1m1g7oAHwU0VzHzPLMbJqZnV3Rh5jZyHC/vKKiomjULVIrmBk/GdqVB87uwcdfFDL8sSl8skT/h+TwxTJQyjsvsaKdtRcBr7h75G3o2rp7LnAx8LCZdSpvRXcf5e657p6bk5NTtYpFaqFLB7Zj9JX92FdaxhWjp3Pls9NZu0U375JDF8tAWQO0iXjdGlhXQd+LgHGRDe6+LvzvMuBjvn18RUSiaEjXprx3+2Du+cGR5K3YzA+fmKJpW+SQxTJQZgCdzayDmaURCo3vnK1lZl2BhsBnEW0NzSw9/LwJMIiKj72ISBSkpyRz7QkdeeWGYwE467EpXDTqM/JWbAq4MkkUMQsUdy8BbgbeBRYB/3T3BWZ2n5mdFdF1BPCif/vcxSOBPDObA0wEfht5dpiIxE635vV559bB3HVaN5YV7eS8Jz/j5+PnsXtf6cFXllrNatI56Lm5uZ6Xlxd0GSI1xq59Jfzlg6X8bdIyWmbX4fZTunBe39aYaeqWmsLM8sPHq6tMJ56LSIUy0lL4+elH8uLIgTStX4c7XpnL3RPmU1xaFnRpEocUKCJyUAM7Nmb8Dcdx45BOjJu+iqvHzKBw+56gy5I4o6uYRKRSkpKMO4d1o32TTO4eP4/+D35I20YZXHtCB0b0b0uqrrSv9RQoInJILshtQ4+W2Uwp2Mjb89fzq38t4KUZq/nTBb3o1rx+0OVJgHRQXkQOm7vz7oIN3PPaPLbuLubW73dm5OBOpKVoayVR6KC8iMQFM2NYj+a8e9tghnZvzh/fW8Kwhyfx7JTlbN1VHHR5Us0UKCJSZY3rpfP4Jcfw7FX9qJuWzK/fWMj3H/qEt+au1/T4tYgCRUSi5qSuTXnrxyfw+s2DaNmgDjeNncl1z+ezYZvOCKsNFCgiEnU9Wzdg/A3H8YvTj2RyQRGnPPQJf5+2Utev1HAKFBGJiZTkJK4b3JG3bx1Mtxb1uee1+Qx7eBLTl2/SbrAaSmd5iUjMuTsfLirk3tcXsHbLbuqmJjPoiMacn9uG73VrqmtYAhTNs7x0HYqIxJyZcXL3Zgzs1JjXZq1lyYbtvDP/Sz5YVEiTemn8aHAnLju2HXVSk4MuVapAWygiEoiS0jImLS3i2Skr+HTpRto2yuDXZx3FMW0b8tSny0gy6Nwsiy7NsujQJFPXtsRINLdQFCgiErgpBRu557X5LN+4k/SUpG8O3peFfz2lJBln9GzB7ad0oV3jzG/Wc3eKduylaVadIMquERQoFVCgiCSufSVlvDpzDdOXb+Ka4ztwRNN6LCvaydLC7cxatYUXZ6zCHX42tCtXH9+B5CTjj+9+wWMTC+jfvhEXD2jLsB7NtdvsEClQKqBAEam5Nmzbwy8mzOeDRRvo07YBI/q35e7x8+jdpgFFO/ay8qtdNMhI5Yd9WnPDkE7kZKUHXXJCUKBUQIEiUrO5O6/PWcev31jIpp37aJCRykc/HUKDuqlM/c9XvDhjFe/M/5K0lCSuPaEj1xzfgey6qUGXHdcUKBVQoIjUDnuKS/n4iyJaNqhDz9YNvrVs+cad/PG9L3hr7npSkoxjOzXm7tOP5MgWmgm5PAqUCihQRORrC9Zt5a2563lxxmq27i7mB0e34KaTjqBr86ygS4srCTPbsJkNM7MvzIjQWjoAAAxPSURBVKzAzO4qZ/mVZlZkZrPDj2sjll1hZkvDjytiWaeI1DxHtczmzmHd+OinJ3L1oPZ8tLiQ0/4yibtencuc1Vt0tX4MxGwLxcySgSXAKcAaYAYwwt0XRvS5Esh195v3W7cRkAfkAg7kA33dffOBPlNbKCJSkS279vHn95cwbsZq9pWU0atNA648rh1DuzcnM732XuOdKFfK9wcK3H0ZgJm9CAwHFh5wrZBTgffdfVN43feBYcC4GNUqIjVcg4w0fj28Bz89tSv/mrWWpycv5/aX5pBVZwE/7NOKdVv3cGSL+vxocEcy01PYuruYOqlJpKfoNOTKimWgtAJWR7xeAwwop9+5ZjaY0NbM7e6+uoJ1W5X3IWY2EhgJ0LZt2yiULSI1Wf06qVx2bHsuGdCOGSs2MWbqCp6ftpKW2XV5f+EGnv9sBb1aN2BywUay6qRwZs+WnNy9Gccf0YTkJAu6/LgWy0Apb+T337/2BjDO3fea2fXAc8D3KrluqNF9FDAKQru8Dr9cEalNkpKMAR0bM6BjY/aVlJGWksTMVZsZM2UF+Ss3c9nAdhTt2MvL+at5YdpK2jSqyxXHtuf83DbfnIpcUlpGqbu2YsJiGShrgDYRr1sD6yI7uPtXES+fAn4Xse6Q/db9OOoViojAN/OEHdO2Ice0bfitZXuKS/locSFjpqzggbcW8dD7Szj3mNac0bMF9725kFWbdnF+3za0yK7DEU3r0adtAxpkpAXxNQIXy4PyKYR2Y30fWEvooPzF7r4gok8Ld18ffn4O8D/uPjB8UD4fOCbcdSahg/KbDvSZOigvIrE0f+1Wnp2ygjfmrGNfaRmZackM7NiYiV8UfjPvGEC35ln86MSOnNmzJSnJSbg7ZvG5uyxhrkMxs9OBh4FkYLS7P2hm9wF57v66mf0fcBZQAmwCbnD3xeF1rwbuDr/Vg+7+7ME+T4EiItVh4469vDZrLSd0zqFr8yxKSsvYua+UReu3kb9yM2/OXc+i9dtoXr8O2XVTWblpJ2f1akmnnHp0yqnHkK45pMTJPWASJlCqmwJFROJBWZnz4eJCxk1fxZ7iUppn1+GtuevZWxKaRbl5/ToM69Gciwe0pUuzYC+0VKBUQIEiIvGquLSMvSVlTF66kVdnrmHSkiL2lpRxRs8W3POD7jTPDmYK/kS5DkVERMJSk5NITU5iWI/mDOvRnM079zF6ynJGTVrGxMWF3HZyFwZ2bMzW3cUs27iDtVt2075xJk2z0slIS+GoVvWpXye+J7rUFoqISIBWfbWLe1+fz8Qvir7VnpxklJZ9+/dzxyaZdMypR6ecTE7skkNu+0ZVvpOldnlVQIEiIonI3Zm/dhvrt+6mXnoKHXPq0TQrndWbd7F1dzGbdu5j/tqtzF2zlVWbdrGsaCf7Ssuol55Cdt1U6qWn8O7tgw/rs7XLS0SkBjEzjm6dzdGts7/VHnm74yFdm37zfOfeEqYUbOSTJUXsLi4lp1583ExMgSIikmAy01MYelRzhh7VPOhSviU+ToQWEZGEp0AREZGoUKCIiEhUKFBERCQqFCgiIhIVChQREYkKBYqIiESFAkVERKKiRk29YmZFwEqgCbAxSm+bDWyNYv8DLS9v2f5th/I6muNQUX1V6VtRn8qMw/5tGofyX8fLOFSm/6GMQ3ntGoeDvy5vHNq5e87By64Ed69xD0I38IrWe42KZv8DLS9v2f5th/I6muNwqGNRmb4V9anMOJTzXTUOBxmXIMehMv0PZRwO9r01DtU/Du6uXV6V8EaU+x9oeXnL9m871NfRdCjvXZm+FfWpzDjs36ZxKP91vIxDZfofyjiU165xOPjrWI5Dzdrl9TUzy/MozZ6ZyDQOIRqHEI1DiMYhJBbjUFO3UEYFXUCc0DiEaBxCNA4hGoeQqI9DjdxCERGR6ldTt1BERKSaKVBERCQq4jpQzGy0mRWa2fzDWLevmc0zswIze8TMLNz+kpnNDj9WmNns6FceXbEYh/CyW8zsCzNbYGa/j27VsRGjn4n/NbO1ET8Xp0e/8uiK1c9EePnPzMzNrEn0Ko6NGP083G9mc8M/C++ZWcvoVx5dMRqHP5jZ4vBYTDCzBgd9s2ifhxzl87sHA8cA8w9j3enAsYABbwOnldPnT8Cvgv6eQYwDcBLwAZAeft006O8Z4Fj8L/CzoL9b0OMQXtYGeJfwBcJBf8+Afh7qR/T5MfBk0N8zoHEYCqSEn/8O+N3B3iuut1DcfRKwKbLNzDqZ2Ttmlm9mn5pZt/3XM7MWhH4oPvPQaDwPnL1fHwMuAMbF7htER4zG4Qbgt+6+N/wZhbH9FtERy5+JRBLDcfgzcCeQEGfrxGIc3H1bRNdMEmAsYjQO77l7SbjrNKD1weqI60CpwCjgFnfvC/wMeKKcPq2ANRGv14TbIp0AbHD3pTGpMvaqOg5dgBPM7HMz+8TM+sW02tiKxs/EzeFN+9Fm1jB2pcZUlcbBzM4C1rr7nFgXGmNV/nkwswfNbDVwCfCrGNYaS9H6XQlwNaGtlwNKOYwiA2Nm9YDjgJcjdvuml9e1nLb9/8oYQQJsnZQnSuOQAjQEBgL9gH+aWcfwXykJI0pj8Vfg/vDr+wntCr06upXGVlXHwcwygF8Q2s2RsKL1O8LdfwH8wsx+DtwM3BvlUmMqmr8rzewXQAnwj4N9bkIFCqEtqi3u3juy0cySgfzwy9cJ/YKI3DxrDayL6J8C/BDoG9NqYyca47AGGB8OkOlmVkZosriiWBYeA1UeC3ffELHeU8CbsSw4Rqo6Dp2ADsCc8C+g1sBMM+vv7l/GuPZoisrviAhjgbdIsEAher8rrwDOAL5fqT82gz6YVIkDRu2JONAETAXODz83oFcF680g9Nf31weaTo9YNgz4JOjvFuQ4ANcD94WfdwFWE77QNd4fMRiLFhF9bgdeDPo7BjEO+/VZQQIclI/Rz0PniD63AK8E/R0DGodhwEIgp9I1BD0IBxmgccB6oJjQX9TXEPor6h1gTvjLlnuWFpALzAf+AzwW+csSGANcH/T3C3IcgDTg7+FlM4HvBf09AxyLF4B5wFxCf7W1qK7vE0/jsF+fFSRAoMTo5+HVcPtcQpMptgr6ewY0DgWE/tCcHX4c9Gw3Tb0iIiJRkYhneYmISBxSoIiISFQoUEREJCoUKCIiEhUKFBERiQoFitRoZrajmj/vaTPrHqX3Kg3PeDvfzN442GyvZtbAzG6MxmeLHA6dNiw1mpntcPd6UXy/FP/vhHkxFVm7mT0HLHH3Bw/Qvz3wprv3qI76RPanLRSpdcwsx8xeNbMZ4cegcHt/M5tqZrPC/3YNt19pZi+b2RvAe2Y2xMw+NrNXwveL+EfEPSQ+NrPc8PMd4UkG55jZNDNrFm7vFH49w8zuq+RW1Gf8dxLHemb2oZnNtNB9LIaH+/wW6BTeqvlDuO8d4c+Za2a/juIwinyHAkVqo78Af3b3fsC5wNPh9sXAYHfvQ2iG2d9ErHMscIW7fy/8ug9wG9Ad6AgMKudzMoFp7t4LmARcF/H5fwl/fnnzR31LeP6l7xO6ih9gD3COux9D6L42fwoH2l3Af9y9t7vfYWZDgc5Af6A30NfMBh/s80QOV6JNDikSDScD3SNmYa1vZllANvCcmXUmNONqasQ677t75P0mprv7GgAL3fWzPTB5v8/Zx38nmswHTgk/P5b/3oNkLPDHCuqsG/He+cD74XYDfhMOhzJCWy7Nyll/aPgxK/y6HqGAmVTB54lUiQJFaqMk4Fh33x3ZaGaPAhPd/Zzw8YiPIxbv3O899kY8L6X8/0vF/t+DlBX1OZDd7t7bzLIJBdNNwCOE7tGRA/R192IzWwHUKWd9A/7P3f92iJ8rcli0y0tqo/cI3eMCADP7eorvbGBt+PmVMfz8aYR2tQFcdLDO7r6V0K1of2ZmqYTqLAyHyUlAu3DX7UBWxKrvAleH742BmbUys6ZR+g4i36FAkZouw8zWRDx+QuiXc274QPVCQlP5A/we+D8zmwIkx7Cm24CfmNl0oAWw9WAruPssQrPGXkToRke5ZpZHaGtlcbjPV8CU8GnGf3D39wjtUvvMzOYBr/DtwBGJKp02LFLNwndH3O3ubmYXASPcffjB1hOJdzqGIlL9+gKPhc/M2kKC3W5YpCLaQhERkajQMRQREYkKBYqIiESFAkVERKJCgSIiIlGhQBERkaj4f75GXmo+ZfPRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_find(learn, num_it=400)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_simple</th>\n",
       "      <th>acc_camvid_with_zero_check</th>\n",
       "      <th>dice_coefficient</th>\n",
       "      <th>dice_coefficient_2</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.210704</td>\n",
       "      <td>0.203583</td>\n",
       "      <td>0.974005</td>\n",
       "      <td>0.556441</td>\n",
       "      <td>0.877445</td>\n",
       "      <td>0.564374</td>\n",
       "      <td>05:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.204501</td>\n",
       "      <td>0.201876</td>\n",
       "      <td>0.974453</td>\n",
       "      <td>0.540219</td>\n",
       "      <td>0.880200</td>\n",
       "      <td>0.571359</td>\n",
       "      <td>05:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.217851</td>\n",
       "      <td>0.180933</td>\n",
       "      <td>0.977930</td>\n",
       "      <td>0.567265</td>\n",
       "      <td>0.885530</td>\n",
       "      <td>0.608430</td>\n",
       "      <td>05:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.187363</td>\n",
       "      <td>0.181227</td>\n",
       "      <td>0.977313</td>\n",
       "      <td>0.566786</td>\n",
       "      <td>0.886843</td>\n",
       "      <td>0.615561</td>\n",
       "      <td>05:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.186578</td>\n",
       "      <td>0.169047</td>\n",
       "      <td>0.979720</td>\n",
       "      <td>0.630603</td>\n",
       "      <td>0.899377</td>\n",
       "      <td>0.649751</td>\n",
       "      <td>05:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.193060</td>\n",
       "      <td>0.158909</td>\n",
       "      <td>0.980248</td>\n",
       "      <td>0.675486</td>\n",
       "      <td>0.909994</td>\n",
       "      <td>0.689128</td>\n",
       "      <td>05:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.165434</td>\n",
       "      <td>0.161674</td>\n",
       "      <td>0.980629</td>\n",
       "      <td>0.688209</td>\n",
       "      <td>0.915096</td>\n",
       "      <td>0.705524</td>\n",
       "      <td>05:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.178435</td>\n",
       "      <td>0.159382</td>\n",
       "      <td>0.980107</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.903398</td>\n",
       "      <td>0.686789</td>\n",
       "      <td>05:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.163246</td>\n",
       "      <td>0.155871</td>\n",
       "      <td>0.980304</td>\n",
       "      <td>0.688581</td>\n",
       "      <td>0.911034</td>\n",
       "      <td>0.694085</td>\n",
       "      <td>05:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.156271</td>\n",
       "      <td>0.148171</td>\n",
       "      <td>0.981584</td>\n",
       "      <td>0.715938</td>\n",
       "      <td>0.921352</td>\n",
       "      <td>0.725638</td>\n",
       "      <td>05:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with dice_coefficient value: 0.8774449229240417.\n",
      "Better model found at epoch 1 with dice_coefficient value: 0.8802003264427185.\n",
      "Better model found at epoch 2 with dice_coefficient value: 0.8855295181274414.\n",
      "Better model found at epoch 3 with dice_coefficient value: 0.8868431448936462.\n",
      "Better model found at epoch 4 with dice_coefficient value: 0.8993771076202393.\n",
      "Better model found at epoch 5 with dice_coefficient value: 0.9099943041801453.\n",
      "Better model found at epoch 6 with dice_coefficient value: 0.91509610414505.\n",
      "Better model found at epoch 9 with dice_coefficient value: 0.9213515520095825.\n"
     ]
    }
   ],
   "source": [
    "train_learner(learn, slice(lr), epochs=10, pct_start=0.8, best_model_name='bestmodel-frozen-1', \n",
    "              patience_early_stop=4, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('bestmodel-frozen-1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type CombinedDiceLoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "learn.export(file='/kaggle/model/export-1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = slice(lr/100,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_simple</th>\n",
       "      <th>acc_camvid_with_zero_check</th>\n",
       "      <th>dice_coefficient</th>\n",
       "      <th>dice_coefficient_2</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.163928</td>\n",
       "      <td>0.148662</td>\n",
       "      <td>0.981743</td>\n",
       "      <td>0.720051</td>\n",
       "      <td>0.919637</td>\n",
       "      <td>0.724050</td>\n",
       "      <td>05:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.158752</td>\n",
       "      <td>0.146669</td>\n",
       "      <td>0.981914</td>\n",
       "      <td>0.716869</td>\n",
       "      <td>0.922488</td>\n",
       "      <td>0.732964</td>\n",
       "      <td>05:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.155523</td>\n",
       "      <td>0.148330</td>\n",
       "      <td>0.980853</td>\n",
       "      <td>0.741496</td>\n",
       "      <td>0.921275</td>\n",
       "      <td>0.725375</td>\n",
       "      <td>05:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.165156</td>\n",
       "      <td>0.146465</td>\n",
       "      <td>0.981966</td>\n",
       "      <td>0.705554</td>\n",
       "      <td>0.915786</td>\n",
       "      <td>0.717604</td>\n",
       "      <td>05:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.149147</td>\n",
       "      <td>0.150821</td>\n",
       "      <td>0.980260</td>\n",
       "      <td>0.723218</td>\n",
       "      <td>0.918364</td>\n",
       "      <td>0.712595</td>\n",
       "      <td>05:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.154772</td>\n",
       "      <td>0.148231</td>\n",
       "      <td>0.980182</td>\n",
       "      <td>0.751208</td>\n",
       "      <td>0.923175</td>\n",
       "      <td>0.730273</td>\n",
       "      <td>05:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.158132</td>\n",
       "      <td>0.149820</td>\n",
       "      <td>0.981884</td>\n",
       "      <td>0.706415</td>\n",
       "      <td>0.917149</td>\n",
       "      <td>0.713693</td>\n",
       "      <td>05:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.151295</td>\n",
       "      <td>0.144460</td>\n",
       "      <td>0.982041</td>\n",
       "      <td>0.736426</td>\n",
       "      <td>0.927686</td>\n",
       "      <td>0.744796</td>\n",
       "      <td>05:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.140953</td>\n",
       "      <td>0.141040</td>\n",
       "      <td>0.982625</td>\n",
       "      <td>0.743231</td>\n",
       "      <td>0.922104</td>\n",
       "      <td>0.733332</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.141663</td>\n",
       "      <td>0.137530</td>\n",
       "      <td>0.983165</td>\n",
       "      <td>0.766649</td>\n",
       "      <td>0.928940</td>\n",
       "      <td>0.758719</td>\n",
       "      <td>05:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with dice_coefficient value: 0.9196370244026184.\n",
      "Better model found at epoch 1 with dice_coefficient value: 0.9224876165390015.\n",
      "Better model found at epoch 5 with dice_coefficient value: 0.923175036907196.\n",
      "Better model found at epoch 7 with dice_coefficient value: 0.9276857972145081.\n",
      "Better model found at epoch 9 with dice_coefficient value: 0.9289398193359375.\n"
     ]
    }
   ],
   "source": [
    "train_learner(learn, lrs, epochs=10, pct_start=0.8, best_model_name='bestmodel-unfrozen-1', \n",
    "              patience_early_stop=4, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(file='/kaggle/model/export-2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn=None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_large_learner(bs=4, size=size, transform_func=get_simple_transforms, model_to_load='bestmodel-unfrozen-1'):\n",
    "    data = (src.transform(transform_func(), size=size, tfm_y=True)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))\n",
    "    learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd, bottle=True)\n",
    "    learn.model_dir = Path('/kaggle/model')\n",
    "    learn.loss_func = CombinedDiceLoss(zero_cat_factor=0.5)\n",
    "#     learn.loss_func = CrossEntropyFlat(axis=1, weight=torch.tensor([1.5, .5, .5, .5, .5]).cuda())\n",
    "#     learn = to_fp16(learn, loss_scale=8.0)\n",
    "    if model_to_load is not None:\n",
    "        learn.load(model_to_load)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_large_learner(bs=bs, size=src_size, transform_func=get_simple_transforms, model_to_load='bestmodel-unfrozen-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_find(learn, num_it=400)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/10 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_simple</th>\n",
       "      <th>acc_camvid_with_zero_check</th>\n",
       "      <th>dice_coefficient</th>\n",
       "      <th>dice_coefficient_2</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='73' class='' max='2828', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      2.58% [73/2828 00:22<14:23 0.1921]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_learner(learn, slice(lr), epochs=10, pct_start=0.8, best_model_name='bestmodel-frozen-3', \n",
    "              patience_early_stop=4, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-3');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('bestmodel-frozen-3');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type CombinedDiceLoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "learn.export(file='/kaggle/model/export-3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = slice(lr/1000,lr/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      30.00% [3/10 17:03<39:47]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_simple</th>\n",
       "      <th>acc_camvid_with_zero_check</th>\n",
       "      <th>dice_coefficient</th>\n",
       "      <th>dice_coefficient_2</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.155961</td>\n",
       "      <td>0.145340</td>\n",
       "      <td>0.980400</td>\n",
       "      <td>0.734930</td>\n",
       "      <td>0.921745</td>\n",
       "      <td>0.721175</td>\n",
       "      <td>05:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.149046</td>\n",
       "      <td>0.140909</td>\n",
       "      <td>0.982022</td>\n",
       "      <td>0.738009</td>\n",
       "      <td>0.925075</td>\n",
       "      <td>0.740396</td>\n",
       "      <td>05:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.143460</td>\n",
       "      <td>0.137951</td>\n",
       "      <td>0.982658</td>\n",
       "      <td>0.747147</td>\n",
       "      <td>0.929874</td>\n",
       "      <td>0.755609</td>\n",
       "      <td>05:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='2828', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with dice_coefficient value: 0.9217453598976135.\n",
      "Better model found at epoch 1 with dice_coefficient value: 0.925074577331543.\n",
      "Better model found at epoch 2 with dice_coefficient value: 0.929873526096344.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-f4925f3b69b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train_learner(learn, lrs, epochs=10, pct_start=0.8, best_model_name='bestmodel-4', \n\u001b[0;32m----> 2\u001b[0;31m               patience_early_stop=5, patience_reduce_lr = 3)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-f266707c06c5>\u001b[0m in \u001b[0;36mtrain_learner\u001b[0;34m(learn, slice_lr, epochs, pct_start, best_model_name, patience_early_stop, patience_reduce_lr)\u001b[0m\n\u001b[1;32m      7\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStoppingCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dice_coefficient'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatience_early_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateauCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dice_coefficient'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatience_reduce_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                               callbacks.TerminateOnNaNCallback()])\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     21\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb_fns_registered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/fastai/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mnres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0;31m# We have to remove res.orig to avoid hanging refs and therefore memory leaks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deeplearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_learner(learn, lrs, epochs=10, pct_start=0.8, best_model_name='bestmodel-4', \n",
    "              patience_early_stop=5, patience_reduce_lr = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-4');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('bestmodel-4');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(file='/kaggle/model/export-4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!cp /kaggle/model/export.pkl /opt/fastai/fastai-exercises/nbs_gil\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'export-4.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn=None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = (path/'test_images').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_learn = load_learner('/kaggle/model/', file='export-2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_learn = to_fp16(inference_learn, loss_scale=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_path):\n",
    "    pred_class, pred_idx, outputs = inference_learn.predict(open_image(str(img_path)))\n",
    "    return pred_class, pred_idx, outputs\n",
    "\n",
    "def encode_classes(pred_class_data):\n",
    "    pixels = np.concatenate([[0], torch.transpose(pred_class_data.squeeze(), 0, 1).flatten(), [0]])\n",
    "    classes_dict = {1: [], 2: [], 3: [], 4: []}\n",
    "    count = 0\n",
    "    previous = pixels[0]\n",
    "    for i, val in enumerate(pixels):\n",
    "        if val != previous:\n",
    "            if previous in classes_dict:\n",
    "                classes_dict[previous].append((i - count, count))\n",
    "            count = 0\n",
    "        previous = val\n",
    "        count += 1\n",
    "    return classes_dict\n",
    "\n",
    "\n",
    "def convert_classes_to_text(classes_dict, clazz):\n",
    "    return ' '.join([f'{v[0]} {v[1]}' for v in classes_dict[clazz]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_to_predict = train_images[16].name\n",
    "display_image_with_mask(image_to_predict)\n",
    "pred_class, pred_idx, outputs = predict(path/f'train_images/{image_to_predict}')\n",
    "pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(pred_class.data.squeeze(), 0, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking encoding methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_all = encode_classes(pred_class.data)\n",
    "print(convert_classes_to_text(encoded_all, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = train_images[16]\n",
    "print(get_y_fn(image_name))\n",
    "img = open_mask(get_y_fn(image_name))\n",
    "img_data = img.data\n",
    "print(convert_classes_to_text(encode_classes(img_data), 3))\n",
    "img_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through the test images and create submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "defect_classes = [1, 2, 3, 4]\n",
    "with open('submission.csv', 'w') as submission_file:\n",
    "    submission_file.write('ImageId_ClassId,EncodedPixels\\n')\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        pred_class, pred_idx, outputs = predict(test_image)\n",
    "        encoded_all = encode_classes(pred_class.data)\n",
    "        for defect_class in defect_classes:\n",
    "            submission_file.write(f'{test_image.name}_{defect_class},{convert_classes_to_text(encoded_all, defect_class)}\\n')\n",
    "        if i % 5 == 0:\n",
    "            print(f'Processed {i} images\\r', end='')\n",
    "            \n",
    "print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative prediction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,y = learn.get_preds(ds_type=DatasetType.Test, with_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class_data = preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((path/'test_images').ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test_ds.x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
